{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64806f2f",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click the markdown to see the code instead of this </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c69e7",
   "metadata": {},
   "source": [
    "# 3.3 Dimentionality Reduction-based Techniques\n",
    "\n",
    "-- Feature projections and transformation-based importance\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- t-SNE (for visualization, not selection)\n",
    "- UMAP\n",
    "- Autoencoder Feature Compression\n",
    "- Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ada08",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73979b",
   "metadata": {},
   "source": [
    "## 3.3.1 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8492a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Principal Component Analysis (PCA) } </h2>\n",
    "</summary>\n",
    "<h3> What is PCA? </h3>\n",
    "<p> PCA is a statistical technique that transforms high-dimensional data into a lower-dimensional space by finding new axes (principal components) that capture the most variance in the data.</p>\n",
    "<h3> It's role in Feature Importance / Compression: </h3>\n",
    "<ul>\n",
    "    <li> PCA does not use the target variable—it's unsupervised.</li>\n",
    "    <li> Reduces dimensionality while retaining as much variance as possible.</li>\n",
    "    <li> Helps in noise reduction, visualization, and speeding up ML algorithms.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=FgakZw6K1QQ\" target=\"_blank\">PCA Explained Visually (StatQuest)</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de100ae7",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- model: A fitted model (optional, to align PCA-transformed features with model-specific importance)\n",
    "- n_components: Number of principal components to compute (default: keep all)\n",
    "- scale_data: Whether to standardize features before PCA (important for PCA accuracy)\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features and their overall absolute contribution scores (based on PCA loadings)\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2b32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def pca_feature_importance(X,\n",
    "                            model=None,\n",
    "                            n_components=None,\n",
    "                            scale_data=True,\n",
    "                            top_n_features=20,\n",
    "                            show_plot=True,\n",
    "                            plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_processed)\n",
    "\n",
    "    # Feature importance based on PCA loadings\n",
    "    loadings = pca.components_.T\n",
    "\n",
    "    # Sum of absolute loadings across all principal components\n",
    "    feature_contributions = np.sum(np.abs(loadings), axis=1)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'PCA_Contribution': feature_contributions\n",
    "    }).sort_values('PCA_Contribution', ascending=False)\n",
    "\n",
    "    # Optionally add model feature importances if provided\n",
    "    if model is not None and hasattr(model, 'feature_importances_'):\n",
    "        importance_df['Model_Importance'] = model.feature_importances_\n",
    "    elif model is not None and hasattr(model, 'coef_'):\n",
    "        importance_df['Model_Importance'] = np.abs(model.coef_).flatten()\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('PCA_Contribution', ascending=True)\n",
    "\n",
    "        colors = plt.cm.cool(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['PCA_Contribution'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Overall Absolute Contribution (PCA)', fontsize=12)\n",
    "        plt.title('PCA Feature Importance (via Loadings)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: PCA Loadings\\n\"\n",
    "            f\"Scale Data: {scale_data}\\n\"\n",
    "            f\"Model-Specific: {'Yes' if model else 'No'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc866dc",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0182c591",
   "metadata": {},
   "source": [
    "## 3.3.2 t-SNE (for visualization, not selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476a425d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding t-SNE for Visualization } </h2>\n",
    "</summary>\n",
    "<h3> What is t-SNE? </h3>\n",
    "<p> t-SNE is a non-linear dimensionality reduction technique that projects high-dimensional data into 2D or 3D for visualization while preserving local neighborhood structure.</p>\n",
    "<h3> It's role in Feature Importance / Visualization: </h3>\n",
    "<ul>\n",
    "    <li> Primarily used for exploring and visualizing data—not for feature selection or compression.</li>\n",
    "    <li> Useful for understanding clusters and patterns in the dataset.</li>\n",
    "    <li> Not ideal for training downstream ML models due to its stochastic nature.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=NEaUSP4YerM\" target=\"_blank\">t-SNE Intuition and Application</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f96c5",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "    \n",
    "- X: Features (DataFrame)\n",
    "- y: Target (Series or 1D array)\n",
    "- model: A fitted model (optional, to color points by predicted classes)\n",
    "- perplexity: t-SNE perplexity parameter (related to number of nearest neighbors)\n",
    "- n_iter: Number of optimization iterations\n",
    "- learning_rate: Learning rate for t-SNE updates\n",
    "- scale_data: Whether to standardize features before t-SNE\n",
    "- figsize: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "    \n",
    "- DataFrame containing 2D t-SNE coordinates with true labels (and optionally predicted labels if model is given)\n",
    "- Displays a 2D scatter plot colored by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f6f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def tsne_feature_visualization(X,\n",
    "                                y,\n",
    "                                model=None,\n",
    "                                perplexity=30,\n",
    "                                n_iter=1000,\n",
    "                                learning_rate=200,\n",
    "                                scale_data=True,\n",
    "                                random_state=42,\n",
    "                                figsize=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    tsne = TSNE(n_components=2,\n",
    "                perplexity=perplexity,\n",
    "                n_iter=n_iter,\n",
    "                learning_rate=learning_rate,\n",
    "                random_state=random_state)\n",
    "\n",
    "    X_embedded = tsne.fit_transform(X_processed)\n",
    "\n",
    "    tsne_df = pd.DataFrame({\n",
    "        'TSNE_1': X_embedded[:, 0],\n",
    "        'TSNE_2': X_embedded[:, 1],\n",
    "        'True_Label': y\n",
    "    })\n",
    "\n",
    "    if model is not None:\n",
    "        preds = model.predict(X)\n",
    "        tsne_df['Predicted_Label'] = preds\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    if model is None:\n",
    "        groups = tsne_df.groupby('True_Label')\n",
    "        title = 't-SNE Visualization by True Labels'\n",
    "    else:\n",
    "        groups = tsne_df.groupby('Predicted_Label')\n",
    "        title = 't-SNE Visualization by Model Predictions'\n",
    "\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(groups)))\n",
    "\n",
    "    for (label, group), color in zip(groups, colors):\n",
    "        plt.scatter(group['TSNE_1'],\n",
    "                    group['TSNE_2'],\n",
    "                    label=label,\n",
    "                    alpha=0.7,\n",
    "                    s=60,\n",
    "                    edgecolors='k',\n",
    "                    linewidth=0.5,\n",
    "                    color=color)\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('TSNE 1', fontsize=12)\n",
    "    plt.ylabel('TSNE 2', fontsize=12)\n",
    "    plt.legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return tsne_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3d0f4",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457175d",
   "metadata": {},
   "source": [
    "## 3.3.3 UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fa39a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding UMAP for Dimensionality Reduction } </h2>\n",
    "</summary>\n",
    "<h3> What is UMAP? </h3>\n",
    "<p> UMAP is a non-linear dimensionality reduction technique that preserves both global and local data structures. It's faster and often more effective than t-SNE for many tasks.</p>\n",
    "<h3> It's role in Feature Importance / Visualization: </h3>\n",
    "<ul>\n",
    "    <li> Suitable for both visualization and downstream ML applications.</li>\n",
    "    <li> Retains more of the global structure compared to t-SNE.</li>\n",
    "    <li> Can be used for clustering, anomaly detection, and data exploration.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://umap-learn.readthedocs.io/en/latest/\" target=\"_blank\">UMAP Official Documentation</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d073974",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- model: A fitted model (optional, for aligning compressed features with model performance)\n",
    "- n_components: Number of compressed dimensions (default=2)\n",
    "- n_neighbors: Size of local neighborhood (default=15)\n",
    "- min_dist: Minimum distance between points in low-dimensional space (default=0.1)\n",
    "- metric: Distance metric for UMAP (default='euclidean')\n",
    "- scale_data: Whether to standardize features before compression\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- UMAP transformer (to transform features)\n",
    "- DataFrame of compressed feature representations\n",
    "- Displays a plot of feature contributions (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea3a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap\n",
      "  Downloading umap-0.1.1.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: umap\n",
      "  Building wheel for umap (setup.py): started\n",
      "  Building wheel for umap (setup.py): finished with status 'done'\n",
      "  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3550 sha256=836c652fd923d66d8392472788f8169536a6de5c19955190bc46bab3e9bc49a1\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\6e\\d3\\96\\de108ab40279ac7d1ba174a5c40b25dc758a6b3d371617cdea\n",
      "Successfully built umap\n",
      "Installing collected packages: umap\n",
      "Successfully installed umap-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc30a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def umap_feature_compression(X,\n",
    "                              model=None,\n",
    "                              n_components=2,\n",
    "                              n_neighbors=15,\n",
    "                              min_dist=0.1,\n",
    "                              metric='euclidean',\n",
    "                              random_state=42,\n",
    "                              scale_data=True,\n",
    "                              show_plot=True,\n",
    "                              plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    # Build and Fit UMAP\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=n_components,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        metric=metric,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    embedding = reducer.fit_transform(X_processed)\n",
    "    compressed_df = pd.DataFrame(embedding, columns=[f'UMAP_{i+1}' for i in range(n_components)])\n",
    "\n",
    "    if show_plot:\n",
    "        # Feature Importance Approximation: Permute each feature and measure embedding distortion\n",
    "        baseline_embedding = reducer.transform(X_processed)\n",
    "        distortions = []\n",
    "\n",
    "        for i, col in enumerate(feature_names):\n",
    "            X_permuted = X_processed.copy()\n",
    "            np.random.shuffle(X_permuted[:, i])\n",
    "            permuted_embedding = reducer.transform(X_permuted)\n",
    "\n",
    "            distortion = np.mean(np.linalg.norm(baseline_embedding - permuted_embedding, axis=1))\n",
    "            distortions.append(distortion)\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': distortions\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Distortion after Feature Permutation', fontsize=12)\n",
    "        plt.title('UMAP Feature Importance (Embedding Sensitivity)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.001,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: UMAP Compression\\n\"\n",
    "            f\"Components: {n_components}\\n\"\n",
    "            f\"Metric: {metric}\\n\"\n",
    "            f\"Scale Data: {scale_data}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return reducer, compressed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a981c",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedb9c7f",
   "metadata": {},
   "source": [
    "## 3.3.4 Autoencoder Feature Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cc12f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Autoencoders for Feature Compression } </h2>\n",
    "</summary>\n",
    "<h3> What are Autoencoders? </h3>\n",
    "<p> Autoencoders are neural networks trained to reconstruct their input. The compressed hidden layer representation is a lower-dimensional encoding of the input data.</p>\n",
    "<h3> It's role in Feature Importance / Compression: </h3>\n",
    "<ul>\n",
    "    <li> Learns compressed latent representations of high-dimensional input data.</li>\n",
    "    <li> Can capture non-linear relationships and interactions between features.</li>\n",
    "    <li> Often used for denoising, anomaly detection, and representation learning.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=9zKuYvjFFS8\" target=\"_blank\">Autoencoders Explained (StatQuest)</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e01b44e",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- model: A fitted model (optional, for aligning compressed features with model performance)\n",
    "- encoding_dim: Size of the compressed (latent) space\n",
    "- hidden_layers: List defining hidden layers size (optional, for deeper architectures)\n",
    "- epochs: Number of training epochs\n",
    "- batch_size: Batch size during training\n",
    "- learning_rate: Learning rate for the optimizer\n",
    "- scale_data: Whether to standardize features before training\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot feature contributions to latent space (optional)\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- Encoder model (to transform features)\n",
    "- DataFrame of compressed feature representations\n",
    "- Displays a plot of feature contributions (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8382f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def autoencoder_feature_compression(X,\n",
    "                                     model=None,\n",
    "                                     encoding_dim=10,\n",
    "                                     hidden_layers=None,\n",
    "                                     epochs=50,\n",
    "                                     batch_size=32,\n",
    "                                     learning_rate=0.001,\n",
    "                                     scale_data=True,\n",
    "                                     random_state=42,\n",
    "                                     show_plot=True,\n",
    "                                     plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    input_dim = X_processed.shape[1]\n",
    "\n",
    "    # Build Autoencoder\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder\n",
    "    x = input_layer\n",
    "    if hidden_layers:\n",
    "        for units in hidden_layers:\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(1e-5))(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = encoded\n",
    "    if hidden_layers:\n",
    "        for units in reversed(hidden_layers):\n",
    "            x = Dense(units, activation='relu')(x)\n",
    "    decoded = Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    # Train Autoencoder\n",
    "    autoencoder.fit(X_processed, X_processed,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    verbose=0)\n",
    "\n",
    "    compressed_features = encoder.predict(X_processed)\n",
    "    compressed_df = pd.DataFrame(compressed_features, columns=[f'Latent_{i+1}' for i in range(encoding_dim)])\n",
    "\n",
    "    if show_plot:\n",
    "        feature_importance = np.abs(encoder.get_weights()[0])  # Encoder's first layer weights\n",
    "        importance_scores = np.sum(feature_importance, axis=1)\n",
    "\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance_scores\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Contribution to Latent Space', fontsize=12)\n",
    "        plt.title('Autoencoder Feature Importance (Latent Contributions)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Autoencoder Compression\\n\"\n",
    "            f\"Encoding Dim: {encoding_dim}\\n\"\n",
    "            f\"Scale Data: {scale_data}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return encoder, compressed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b7dfe",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464d14c",
   "metadata": {},
   "source": [
    "## 3.3.5 Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6a7d5",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Factor Analysis for Feature Extraction } </h2>\n",
    "</summary>\n",
    "<h3> What is Factor Analysis? </h3>\n",
    "<p> Factor Analysis is a statistical method used to describe variability among observed, correlated variables in terms of fewer unobserved variables called factors.</p>\n",
    "<h3> It's role in Feature Importance / Dimensionality Reduction: </h3>\n",
    "<ul>\n",
    "    <li> Assumes that the observed features are influenced by a smaller number of latent variables (factors).</li>\n",
    "    <li> Especially useful in psychology, social sciences, and market research.</li>\n",
    "    <li> Focuses more on modeling the structure rather than just variance like PCA.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=Jkf-pGDdy7k\" target=\"_blank\">Factor Analysis Explained</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332284c",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "    \n",
    "- n_factors: Number of factors to extract (default=5, or determined automatically if kaiser_criterion=True)\n",
    "- rotation: Type of rotation method ('varimax', 'quartimax', 'promax', 'oblimin', or None)\n",
    "- max_iter: Maximum number of iterations for FA algorithm convergence\n",
    "- scale_data: Whether to standardize features before factor analysis\n",
    "- min_correlation: Minimum correlation threshold for displaying factor loadings\n",
    "- kaiser_criterion: If True, automatically determine n_factors based on eigenvalues > 1\n",
    "- variance_explained_threshold: Minimum cumulative variance explained threshold for factor selection\n",
    "    \n",
    "##### Returns:\n",
    "    \n",
    "- factor_model: Fitted factor analysis model\n",
    "- factor_scores: DataFrame of factor scores for each observation\n",
    "- feature_importance: DataFrame of feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af5c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def factor_analysis_feature_importance(X,\n",
    "                                      n_factors=5,\n",
    "                                      rotation='varimax',\n",
    "                                      max_iter=1000,\n",
    "                                      scale_data=True,\n",
    "                                      min_correlation=0.3,\n",
    "                                      random_state=42,\n",
    "                                      show_plot=True,\n",
    "                                      plot_size=(12, 8),\n",
    "                                      feature_contribution_method='loadings_squared',\n",
    "                                      kaiser_criterion=True,\n",
    "                                      variance_explained_threshold=0.8):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Standardize data if requested\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "    \n",
    "    # Determine optimal number of factors if kaiser_criterion is True\n",
    "    if kaiser_criterion:\n",
    "        # Calculate correlation matrix and its eigenvalues\n",
    "        corr_matrix = np.corrcoef(X_processed, rowvar=False)\n",
    "        eigenvalues, _ = np.linalg.eig(corr_matrix)\n",
    "        eigenvalues = np.real(eigenvalues)  # Ensure real values\n",
    "        \n",
    "        # Apply Kaiser criterion (eigenvalues > 1)\n",
    "        k = sum(eigenvalues > 1)\n",
    "        \n",
    "        # Ensure we have at least 1 factor and at most n_features factors\n",
    "        n_factors = max(min(k, X_processed.shape[1]), 1)\n",
    "        \n",
    "        # Further adjust n_factors based on variance explained threshold\n",
    "        explained_variance_ratio = eigenvalues / sum(eigenvalues)\n",
    "        cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "        for i, var in enumerate(cumulative_variance):\n",
    "            if var >= variance_explained_threshold:\n",
    "                n_factors = min(n_factors, i + 1)\n",
    "                break\n",
    "    \n",
    "    # Create and fit factor analysis model\n",
    "    factor_model = FactorAnalysis(n_components=n_factors,\n",
    "                                 rotation=rotation,\n",
    "                                 max_iter=max_iter,\n",
    "                                 random_state=random_state)\n",
    "    \n",
    "    factor_model.fit(X_processed)\n",
    "    \n",
    "    # Get factor loadings\n",
    "    loadings = factor_model.components_.T\n",
    "    \n",
    "    # Calculate factor scores\n",
    "    factor_scores = factor_model.transform(X_processed)\n",
    "    factor_scores_df = pd.DataFrame(factor_scores, \n",
    "                                   columns=[f'Factor_{i+1}' for i in range(n_factors)])\n",
    "    \n",
    "    # Calculate variance explained by each factor\n",
    "    # Since FactorAnalysis doesn't provide explained_variance directly,\n",
    "    # we'll compute it from the loadings\n",
    "    communalities = np.sum(loadings**2, axis=1)\n",
    "    total_variance_explained = np.sum(communalities) / X_processed.shape[1]\n",
    "    factor_variance = np.sum(loadings**2, axis=0) / X_processed.shape[1]\n",
    "    variance_explained = factor_variance / sum(factor_variance) * total_variance_explained\n",
    "    \n",
    "    # Calculate feature importance based on the chosen method\n",
    "    if feature_contribution_method == 'loadings_squared':\n",
    "        # Sum of squared loadings across factors\n",
    "        importance_scores = np.sum(loadings**2, axis=1)\n",
    "    elif feature_contribution_method == 'communality':\n",
    "        # Communality (proportion of each variable's variance explained by the factors)\n",
    "        importance_scores = communalities\n",
    "    elif feature_contribution_method == 'variance_weighted':\n",
    "        # Weight loadings by variance explained by each factor\n",
    "        weighted_loadings = loadings**2 * variance_explained\n",
    "        importance_scores = np.sum(weighted_loadings, axis=1)\n",
    "    else:\n",
    "        # Default to squared loadings\n",
    "        importance_scores = np.sum(loadings**2, axis=1)\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Create factor loadings DataFrame for interpretation\n",
    "    loadings_df = pd.DataFrame(loadings, \n",
    "                             index=feature_names,\n",
    "                             columns=[f'Factor_{i+1}' for i in range(n_factors)])\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "        \n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                      importance_df_sorted['Importance'],\n",
    "                      color=colors,\n",
    "                      alpha=0.9)\n",
    "        \n",
    "        plt.xlabel('Feature Importance Score', fontsize=12)\n",
    "        plt.title('Factor Analysis Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005 * max(importance_scores),\n",
    "                   bar.get_y() + bar.get_height() / 2,\n",
    "                   f'{width:.4f}',\n",
    "                   va='center',\n",
    "                   fontsize=9)\n",
    "        \n",
    "        method_text = (\n",
    "            f\"Method: Factor Analysis\\n\"\n",
    "            f\"# Factors: {n_factors}\\n\"\n",
    "            f\"Rotation: {rotation}\\n\"\n",
    "            f\"Importance: {feature_contribution_method}\\n\"\n",
    "            f\"Total Var Explained: {total_variance_explained:.2f}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                   xy=(0.98, 0.02),\n",
    "                   xycoords='axes fraction',\n",
    "                   ha='right',\n",
    "                   va='bottom',\n",
    "                   fontsize=9,\n",
    "                   bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Additional plot: Factor loading heatmap\n",
    "        plt.figure(figsize=plot_size)\n",
    "        \n",
    "        # Mask small loadings for clarity\n",
    "        masked_loadings = loadings.copy()\n",
    "        abs_loadings = np.abs(masked_loadings)\n",
    "        mask = abs_loadings < min_correlation\n",
    "        masked_loadings[mask] = 0\n",
    "        \n",
    "        # Plot heatmap\n",
    "        im = plt.imshow(masked_loadings, aspect='auto', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.colorbar(im, label='Factor Loading')\n",
    "        \n",
    "        plt.xlabel('Factors', fontsize=12)\n",
    "        plt.ylabel('Features', fontsize=12)\n",
    "        plt.title('Factor Loadings Heatmap', fontsize=14)\n",
    "        \n",
    "        plt.xticks(np.arange(n_factors), [f'Factor {i+1}\\n({variance_explained[i]:.2f})' for i in range(n_factors)])\n",
    "        plt.yticks(np.arange(len(feature_names)), feature_names)\n",
    "        \n",
    "        # Annotate loadings\n",
    "        for i in range(len(feature_names)):\n",
    "            for j in range(n_factors):\n",
    "                if abs(loadings[i, j]) >= min_correlation:\n",
    "                    plt.text(j, i, f'{loadings[i, j]:.2f}',\n",
    "                           ha='center', va='center',\n",
    "                           color='white' if abs(loadings[i, j]) > 0.6 else 'black',\n",
    "                           fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create a dictionary to return multiple results\n",
    "    return factor_model, factor_scores_df, importance_df, loadings_df\n",
    "\n",
    "\n",
    "def interpret_factors(loadings_df, min_loading=0.5, max_features=3):\n",
    "   \n",
    "    factor_interpretations = {}\n",
    "    \n",
    "    for col in loadings_df.columns:\n",
    "        # Get absolute loadings and sort\n",
    "        abs_loadings = loadings_df[col].abs()\n",
    "        sorted_loadings = abs_loadings.sort_values(ascending=False)\n",
    "        \n",
    "        # Get top features that meet minimum loading threshold\n",
    "        top_features = sorted_loadings[sorted_loadings >= min_loading].index.tolist()[:max_features]\n",
    "        \n",
    "        # Create interpretation\n",
    "        if top_features:\n",
    "            feature_signs = ['+' if loadings_df.loc[feature, col] > 0 else '-' for feature in top_features]\n",
    "            feature_desc = [f\"{sign}{feature}\" for feature, sign in zip(top_features, feature_signs)]\n",
    "            factor_interpretations[col] = \", \".join(feature_desc)\n",
    "        else:\n",
    "            factor_interpretations[col] = \"No strong loadings\"\n",
    "    \n",
    "    return factor_interpretations\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e420a50",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click the markdown to see the code instead of this </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df3c73",
   "metadata": {},
   "source": [
    "# 3.7 Feature Importance via Surrogate Models\n",
    "-- Train interpretable models on complex ones\n",
    "\n",
    "- Surrogate Decision Trees (on ensemble or neural nets)\n",
    "- Linear Approximation of Non-linear Models\n",
    "- Local Surrogate Models (e.g., in LIME)\n",
    "- Model Distillation-based Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1baba9",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e48ad1e",
   "metadata": {},
   "source": [
    "## 3.7.1 Surrogate Decision Trees (on ensemble or neural nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64f7429",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Surrogate Decision Trees } </h2>\n",
    "</summary>\n",
    "<h3> What are Surrogate Decision Trees? </h3>\n",
    "<p> A surrogate decision tree is a simplified model trained to mimic the predictions of a complex model (like a neural network or ensemble). It approximates the decision boundaries to improve interpretability.</p>\n",
    "<h3> Role in Model Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Acts as a transparent proxy for an opaque model (e.g., XGBoost, deep nets).</li>\n",
    "    <li> Offers global interpretability by showing which features the original model implicitly uses most.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://christophm.github.io/interpretable-ml-book/surrogate.html\" target=\"_blank\">Interpretable ML Book â€” Surrogate Models</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\" target=\"_blank\">Scikit-learn DecisionTreeClassifier</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc337a6",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: Trained black-box model (ensemble, neural network, etc.)\n",
    "- X: Features (DataFrame or array-like)\n",
    "- feature_names: List of feature names (optional)\n",
    "- tree_max_depth: Maximum depth of surrogate decision tree (default=5)\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- Fitted surrogate decision tree\n",
    "- DataFrame of feature importances\n",
    "- Displays a plot of feature importances if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b34526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.base import is_classifier, is_regressor\n",
    "\n",
    "def surrogate_tree_feature_importance(X,\n",
    "                                       model,\n",
    "                                       feature_names=None,\n",
    "                                       tree_max_depth=5,\n",
    "                                       random_state=42,\n",
    "                                       show_plot=True,\n",
    "                                       plot_size=(12, 8)):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Predict on the dataset\n",
    "    if is_classifier(model):\n",
    "        y_pred = model.predict(X)\n",
    "    elif is_regressor(model):\n",
    "        y_pred = model.predict(X)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type for surrogate modeling.\")\n",
    "\n",
    "    # Choose type of surrogate tree\n",
    "    if is_classifier(model) and len(np.unique(y_pred)) <= 20:\n",
    "        surrogate = DecisionTreeClassifier(max_depth=tree_max_depth, random_state=random_state)\n",
    "    else:\n",
    "        surrogate = DecisionTreeRegressor(max_depth=tree_max_depth, random_state=random_state)\n",
    "\n",
    "    # Fit surrogate tree\n",
    "    surrogate.fit(X, y_pred)\n",
    "\n",
    "    # Feature importances\n",
    "    importances = surrogate.feature_importances_\n",
    "    feature_names = feature_names or X.columns.tolist()\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(importance_df['Feature'],\n",
    "                        importance_df['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Feature Importance', fontsize=12)\n",
    "        plt.title('Surrogate Decision Tree Feature Importances', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Surrogate Tree\\n\"\n",
    "            f\"Estimator: {type(model).__name__}\\n\"\n",
    "            f\"Tree Depth: {tree_max_depth}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return surrogate, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48418cc5",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb1565",
   "metadata": {},
   "source": [
    "## 3.7.2 Linear Approximation of Non-linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d8dc3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Linear Approximation of Non-linear Models } </h2>\n",
    "</summary>\n",
    "<h3> What is Linear Approximation? </h3>\n",
    "<p> Linear approximation involves fitting a linear model (like Linear Regression or Logistic Regression) to approximate the local behavior of a complex model around a given point or region.</p>\n",
    "<h3> Role in Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Offers insight into feature effects in a specific neighborhood of the input space.</li>\n",
    "    <li> Often used in conjunction with tools like LIME or SHAP (kernel-based versions).</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://arxiv.org/abs/1602.04938\" target=\"_blank\">LIME Paper (explains local linear approximations)</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/linear_model.html\" target=\"_blank\">Scikit-learn Linear Models</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7322631",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: Trained black-box model (neural network, ensemble, etc.)\n",
    "- X: Features (DataFrame or array-like)\n",
    "- feature_names: List of feature names (optional)\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- sample_size: Number of points to sample from X for fitting (optional, for speed)\n",
    "\n",
    "##### Returns:\n",
    "- Fitted linear approximation model\n",
    "- DataFrame of feature coefficients (importance)\n",
    "- Displays a plot of feature coefficients if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b36147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.base import is_classifier, is_regressor\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def linear_approximation_feature_importance(X,\n",
    "                                             model,\n",
    "                                             feature_names=None,\n",
    "                                             random_state=42,\n",
    "                                             show_plot=True,\n",
    "                                             plot_size=(12, 8),\n",
    "                                             sample_size=None):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Subsample if needed\n",
    "    if sample_size is not None and sample_size < len(X):\n",
    "        X = resample(X, n_samples=sample_size, random_state=random_state)\n",
    "\n",
    "    # Predict on X\n",
    "    if is_classifier(model):\n",
    "        y_pred = model.predict_proba(X)[:, 1]  # For binary classification, use probability of class 1\n",
    "    elif is_regressor(model):\n",
    "        y_pred = model.predict(X)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type for linear approximation.\")\n",
    "\n",
    "    # Choose linear model type\n",
    "    if is_classifier(model):\n",
    "        surrogate = LinearRegression()\n",
    "    else:\n",
    "        surrogate = LinearRegression()\n",
    "\n",
    "    # Fit surrogate linear model\n",
    "    surrogate.fit(X, y_pred)\n",
    "\n",
    "    coefficients = surrogate.coef_\n",
    "    feature_names = feature_names or X.columns.tolist()\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefficients\n",
    "    }).sort_values('Coefficient', key=lambda x: np.abs(x), ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.coolwarm(np.linspace(0.2, 1, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(importance_df['Feature'],\n",
    "                        importance_df['Coefficient'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Feature Coefficient (Linear Approximation)', fontsize=12)\n",
    "        plt.title('Linear Approximation of Non-linear Model (Feature Importance)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + (0.01 if width >= 0 else -0.05),\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Linear Approximation\\n\"\n",
    "            f\"Estimator: {type(model).__name__}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return surrogate, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa57120c",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4210326",
   "metadata": {},
   "source": [
    "## 3.7.3 Local Surrogate Models (e.g., in LIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a3c5b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Local Surrogate Models (LIME) } </h2>\n",
    "</summary>\n",
    "<h3> What is LIME? </h3>\n",
    "<p> LIME (Local Interpretable Model-agnostic Explanations) fits an interpretable model (like linear regression or decision trees) locally around a prediction point to understand feature influence.</p>\n",
    "<h3> Role in Explainability: </h3>\n",
    "<ul>\n",
    "    <li> Provides localized, interpretable explanations around individual predictions.</li>\n",
    "    <li> It perturbs the data and observes how predictions change.</li>\n",
    "    <li> The surrogate is trained on perturbed samples weighted by similarity to the instance.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://github.com/marcotcr/lime\" target=\"_blank\">LIME GitHub Repo</a></li>\n",
    "    <li><a href=\"https://arxiv.org/abs/1602.04938\" target=\"_blank\">Original LIME Paper</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df050d",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: Trained black-box model (neural network, ensemble, etc.)\n",
    "- X: Features (DataFrame or array-like)\n",
    "- instance: Single instance (row) to explain (array-like or Series)\n",
    "- num_features: Number of top features to explain locally\n",
    "- kernel_width: Controls locality around the instance (higher = more global)\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot local feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- sample_size: Number of perturbed samples to generate around instance\n",
    "\n",
    "##### Returns:\n",
    "- Fitted local surrogate model\n",
    "- DataFrame of local feature importance (weights)\n",
    "- Displays a plot of local feature contributions if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fb499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def local_surrogate_lime_feature_importance(model,\n",
    "                                             X,\n",
    "                                             instance,\n",
    "                                             num_features=10,\n",
    "                                             kernel_width=0.75,\n",
    "                                             random_state=42,\n",
    "                                             show_plot=True,\n",
    "                                             plot_size=(10, 6),\n",
    "                                             sample_size=5000):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    rng = check_random_state(random_state)\n",
    "\n",
    "    # Ensure instance is array-like\n",
    "    instance = np.array(instance).reshape(1, -1)\n",
    "\n",
    "    # Create perturbations around instance\n",
    "    perturbations = rng.normal(loc=instance, scale=0.1, size=(sample_size, X.shape[1]))\n",
    "    perturbations = np.clip(perturbations, X.min().values, X.max().values)\n",
    "\n",
    "    # Predict model outputs on perturbations\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        preds = model.predict_proba(perturbations)[:, 1]  # For binary\n",
    "    else:\n",
    "        preds = model.predict(perturbations)\n",
    "\n",
    "    # Compute distances to original instance\n",
    "    distances = euclidean_distances(perturbations, instance).flatten()\n",
    "\n",
    "    # Kernel weights for proximity\n",
    "    kernel_weights = np.exp(-(distances ** 2) / (kernel_width ** 2))\n",
    "\n",
    "    # Fit weighted local surrogate model (Ridge for stability)\n",
    "    surrogate = Ridge(alpha=1.0, fit_intercept=True)\n",
    "    surrogate.fit(perturbations, preds, sample_weight=kernel_weights)\n",
    "\n",
    "    coefficients = surrogate.coef_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Local_Weight': coefficients\n",
    "    }).sort_values('Local_Weight', key=lambda x: np.abs(x), ascending=False)\n",
    "\n",
    "    importance_df = importance_df.head(num_features).reset_index(drop=True)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.PuOr(np.linspace(0.2, 1, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(importance_df['Feature'],\n",
    "                        importance_df['Local_Weight'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Local Feature Weight', fontsize=12)\n",
    "        plt.title('LIME-style Local Surrogate Model (Feature Importance)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + (0.01 if width >= 0 else -0.05),\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: LIME-style Local Surrogate\\n\"\n",
    "            f\"Instance explained\\n\"\n",
    "            f\"Estimator: {type(model).__name__}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return surrogate, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cec5ea",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63a390",
   "metadata": {},
   "source": [
    "## 3.7.4 Model Distillation-based Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6daf7ba",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Model Distillation-based Attribution } </h2>\n",
    "</summary>\n",
    "<h3> What is Model Distillation? </h3>\n",
    "<p> Model distillation involves training a simpler (interpretable) model to imitate a more complex one by learning from its soft predictions (logits/probabilities) rather than the hard labels.</p>\n",
    "<h3> Role in Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Transfers knowledge from a \"teacher\" model (e.g., a deep net) to a \"student\" model (e.g., a decision tree or logistic reg).</li>\n",
    "    <li> Attribution can then be analyzed using the simpler model.</li>\n",
    "    <li> This gives insight into what the complex model has learned and how it weighs features.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://arxiv.org/abs/1503.02531\" target=\"_blank\">Distilling the Knowledge in a Neural Network (Hinton et al.)</a></li>\n",
    "    <li><a href=\"https://christophm.github.io/interpretable-ml-book/distillation.html\" target=\"_blank\">Interpretable ML Book â€” Model Distillation</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1bbf84",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- teacher_model: Trained complex model (e.g., ensemble, neural net) to explain\n",
    "- X: Input features (DataFrame or array-like)\n",
    "- y: True targets (array-like) or teacher_model predictions (preferred for distillation)\n",
    "- student_model: Simpler model for distillation (e.g., DecisionTree, LinearRegression)\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to visualize feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- fit_on_predictions: Whether to train student model on teacher's outputs or true labels\n",
    "\n",
    "##### Returns:\n",
    "- Fitted student model (interpretable surrogate)\n",
    "- DataFrame of global feature importances\n",
    "- Displays a plot of distilled feature importance if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8bb40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def model_distillation_feature_importance(teacher_model,\n",
    "                                           X,\n",
    "                                           y,\n",
    "                                           student_model,\n",
    "                                           random_state=42,\n",
    "                                           show_plot=True,\n",
    "                                           plot_size=(10, 6),\n",
    "                                           fit_on_predictions=True):\n",
    " \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Distillation: Fit on teacher's outputs if preferred\n",
    "    if fit_on_predictions:\n",
    "        if hasattr(teacher_model, 'predict_proba'):\n",
    "            distilled_y = teacher_model.predict_proba(X)[:, 1]\n",
    "        else:\n",
    "            distilled_y = teacher_model.predict(X)\n",
    "    else:\n",
    "        distilled_y = y\n",
    "\n",
    "    # Train the student model\n",
    "    student_model.fit(X, distilled_y)\n",
    "\n",
    "    # Extract feature importances\n",
    "    if hasattr(student_model, \"feature_importances_\"):\n",
    "        importances = student_model.feature_importances_\n",
    "    elif hasattr(student_model, \"coef_\"):\n",
    "        importances = np.abs(student_model.coef_)\n",
    "    else:\n",
    "        raise ValueError(\"Student model must have either feature_importances_ or coef_ attribute.\")\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.magma(np.linspace(0.2, 1, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(importance_df['Feature'],\n",
    "                        importance_df['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Distilled Feature Importance', fontsize=12)\n",
    "        plt.title('Model Distillation (Feature Attribution)', fontsize=14, pad=20)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.3f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Teacher: {type(teacher_model).__name__}\\n\"\n",
    "            f\"Student: {type(student_model).__name__}\\n\"\n",
    "            f\"Fit on: {'Teacher predictions' if fit_on_predictions else 'True labels'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return student_model, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74da7b37",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

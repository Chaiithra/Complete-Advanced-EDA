{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61f0fc2",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click the markdown to see the code instead of this </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41850b0",
   "metadata": {},
   "source": [
    "# 3.8 Meta / Consensus Techniques\n",
    "-- Aggregate rankings across multiple methods\n",
    "\n",
    "- Rank Aggregation Across Methods\n",
    "- Weighted Voting of Feature Ranks\n",
    "- Clustering Importance Scores\n",
    "- Consensus Stability Score across folds/methods\n",
    "- SHAP + RFE + Correlation Overlay (hybrid consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b0c61e",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f7f79",
   "metadata": {},
   "source": [
    "## 3.8.1 Rank Aggregation Across Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1def6e7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Rank Aggregation Across Methods } </h2>\n",
    "</summary>\n",
    "<h3> What is Rank Aggregation? </h3>\n",
    "<p> Rank aggregation refers to the process of combining feature importance rankings from multiple methods (e.g., SHAP, Decision Trees, Lasso Regression) to obtain a final consensus ranking.</p>\n",
    "<h3> Role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Improves robustness by combining different perspectives on feature importance.</li>\n",
    "    <li> Reduces bias that may arise from a single method.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.arxiv.org/abs/2204.12563\" target=\"_blank\">Combining multiple feature selection algorithms</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26b3544",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- feature_importances_list: List of DataFrames, each containing feature names and importance scores (or rankings) from different methods\n",
    "- aggregation_method: Strategy to combine ranks (choices: 'mean', 'median', 'geometric_mean', 'borda_count')\n",
    "- normalize: Whether to normalize individual importance scores before ranking\n",
    "- show_plot: Whether to plot the final aggregated feature importance\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- Aggregated feature importance DataFrame\n",
    "- Displays a plot of aggregated feature rankings if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b90587c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gmean\n",
    "\n",
    "def rank_aggregation_feature_importance(feature_importances_list,\n",
    "                                         aggregation_method='mean',\n",
    "                                         normalize=True,\n",
    "                                         show_plot=True,\n",
    "                                         plot_size=(12, 8)):\n",
    "\n",
    "    if not feature_importances_list:\n",
    "        raise ValueError(\"The feature_importances_list cannot be empty.\")\n",
    "\n",
    "    all_features = feature_importances_list[0]['Feature'].tolist()\n",
    "\n",
    "    importance_matrix = pd.DataFrame(index=all_features)\n",
    "\n",
    "    for idx, df in enumerate(feature_importances_list):\n",
    "        imp = df.set_index('Feature')['Importance']\n",
    "        if normalize:\n",
    "            imp = (imp - imp.min()) / (imp.max() - imp.min() + 1e-9)\n",
    "        importance_matrix[f'Method_{idx+1}'] = imp\n",
    "\n",
    "    # Replace missing values with 0 (if any feature missing in a method)\n",
    "    importance_matrix = importance_matrix.fillna(0)\n",
    "\n",
    "    # Ranking\n",
    "    ranks = importance_matrix.rank(ascending=False, method='average')\n",
    "\n",
    "    # Aggregation\n",
    "    if aggregation_method == 'mean':\n",
    "        aggregated_scores = ranks.mean(axis=1)\n",
    "    elif aggregation_method == 'median':\n",
    "        aggregated_scores = ranks.median(axis=1)\n",
    "    elif aggregation_method == 'geometric_mean':\n",
    "        aggregated_scores = gmean(ranks + 1e-9, axis=1)  # to avoid zero\n",
    "    elif aggregation_method == 'borda_count':\n",
    "        aggregated_scores = ranks.sum(axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported aggregation_method. Choose from 'mean', 'median', 'geometric_mean', 'borda_count'.\")\n",
    "\n",
    "    final_importance_df = pd.DataFrame({\n",
    "        'Feature': aggregated_scores.index,\n",
    "        'Aggregated_Rank': aggregated_scores\n",
    "    }).sort_values('Aggregated_Rank', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    if show_plot and not final_importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.cividis(np.linspace(0.2, 1, len(final_importance_df)))\n",
    "\n",
    "        bars = plt.barh(final_importance_df['Feature'],\n",
    "                        -final_importance_df['Aggregated_Rank'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('-(Aggregated Rank)', fontsize=12)\n",
    "        plt.title('Rank Aggregation (Feature Importance Across Methods)', fontsize=14, pad=20)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width - 0.2,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{-int(width)}',\n",
    "                     va='center',\n",
    "                     fontsize=9,\n",
    "                     color='white')\n",
    "\n",
    "        method_text = (\n",
    "            f\"Aggregation: {aggregation_method.capitalize()}\\n\"\n",
    "            f\"Normalization: {'Yes' if normalize else 'No'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return final_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e73160",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c50dd1",
   "metadata": {},
   "source": [
    "## 3.8.2 Weighted Voting of Feature Ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8d53d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Weighted Voting of Feature Ranks } </h2>\n",
    "</summary>\n",
    "<h3> What is Weighted Voting of Feature Ranks? </h3>\n",
    "<p> Weighted voting of feature ranks combines rankings from various feature selection techniques by giving different weights to each method based on their performance or reliability.</p>\n",
    "<h3> Role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Assigns greater importance to methods that have shown higher predictive performance.</li>\n",
    "    <li> Provides a more balanced and context-sensitive final ranking of features.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Weighted_voting\" target=\"_blank\">Feature Selection Techniques: Weighted Voting</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047ffe7",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- feature_importances_list: List of DataFrames, each containing 'Feature' and 'Importance' columns\n",
    "- method_weights: List of floats assigning relative importance to each method (must match feature_importances_list length)\n",
    "- normalize: Whether to normalize individual importance scores before ranking\n",
    "- aggregation_rule: How to combine (choices: 'weighted_mean', 'weighted_median')\n",
    "- show_plot: Whether to plot the final weighted voting feature ranking\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- Final weighted aggregated feature importance DataFrame\n",
    "- Displays a plot of weighted feature rankings if show_plot=Truem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1603d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def weighted_voting_feature_importance(feature_importances_list,\n",
    "                                        method_weights,\n",
    "                                        normalize=True,\n",
    "                                        aggregation_rule='weighted_mean',\n",
    "                                        show_plot=True,\n",
    "                                        plot_size=(12, 8)):\n",
    "\n",
    "    if len(feature_importances_list) != len(method_weights):\n",
    "        raise ValueError(\"Length of method_weights must match number of feature importance lists.\")\n",
    "\n",
    "    all_features = feature_importances_list[0]['Feature'].tolist()\n",
    "    importance_matrix = pd.DataFrame(index=all_features)\n",
    "\n",
    "    for idx, df in enumerate(feature_importances_list):\n",
    "        imp = df.set_index('Feature')['Importance']\n",
    "        if normalize:\n",
    "            imp = (imp - imp.min()) / (imp.max() - imp.min() + 1e-9)\n",
    "        importance_matrix[f'Method_{idx+1}'] = imp\n",
    "\n",
    "    importance_matrix = importance_matrix.fillna(0)\n",
    "\n",
    "    # Rank each method\n",
    "    ranks = importance_matrix.rank(ascending=False, method='average')\n",
    "\n",
    "    # Apply weights\n",
    "    weighted_ranks = ranks.multiply(method_weights, axis=1)\n",
    "\n",
    "    if aggregation_rule == 'weighted_mean':\n",
    "        aggregated_scores = weighted_ranks.sum(axis=1) / np.sum(method_weights)\n",
    "    elif aggregation_rule == 'weighted_median':\n",
    "        aggregated_scores = weighted_ranks.median(axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"aggregation_rule must be either 'weighted_mean' or 'weighted_median'.\")\n",
    "\n",
    "    final_importance_df = pd.DataFrame({\n",
    "        'Feature': aggregated_scores.index,\n",
    "        'Weighted_Aggregated_Rank': aggregated_scores\n",
    "    }).sort_values('Weighted_Aggregated_Rank', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    if show_plot and not final_importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(final_importance_df)))\n",
    "\n",
    "        bars = plt.barh(final_importance_df['Feature'],\n",
    "                        -final_importance_df['Weighted_Aggregated_Rank'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('-(Weighted Aggregated Rank)', fontsize=12)\n",
    "        plt.title('Weighted Voting (Feature Importance by Aggregated Ranks)', fontsize=14, pad=20)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width - 0.2,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{-int(width)}',\n",
    "                     va='center',\n",
    "                     fontsize=9,\n",
    "                     color='white')\n",
    "\n",
    "        method_text = (\n",
    "            f\"Aggregation: {aggregation_rule.capitalize()}\\n\"\n",
    "            f\"Normalization: {'Yes' if normalize else 'No'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return final_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7175c53",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a166d",
   "metadata": {},
   "source": [
    "## 3.8.3 Clustering Importance Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae4cc2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Clustering Importance Scores } </h2>\n",
    "</summary>\n",
    "<h3> What is Clustering Importance? </h3>\n",
    "<p> Clustering importance scores quantify the contribution of each feature by measuring how well a feature helps in distinguishing or clustering the data points into different groups.</p>\n",
    "<h3> Role in Feature Ranking: </h3>\n",
    "<ul>\n",
    "    <li> Helps in identifying features that significantly impact clustering quality.</li>\n",
    "    <li> Can be derived from clustering algorithms like K-means or DBSCAN, by evaluating how changes in features affect cluster separation.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6519994/\" target=\"_blank\">Clustering Algorithms for Feature Importance</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c2935",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- feature_importances_list: List of DataFrames, each containing 'Feature' and 'Importance' columns\n",
    "- clustering_method: Clustering algorithm to use (e.g., 'kmeans', 'agglomerative')\n",
    "- n_clusters: Number of clusters to form\n",
    "- cluster_selection_criteria: How to select important clusters (e.g., highest average importance)\n",
    "- normalize: Whether to normalize importances before clustering\n",
    "- show_plot: Whether to plot clustered feature importance\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of clustered feature importances\n",
    "- Displays cluster membership and feature ranking if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e6a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def clustering_feature_importance(feature_importances_list,\n",
    "                                   clustering_method='kmeans',\n",
    "                                   n_clusters=3,\n",
    "                                   cluster_selection_criteria='highest_mean',\n",
    "                                   normalize=True,\n",
    "                                   show_plot=True,\n",
    "                                   plot_size=(12, 8),\n",
    "                                   random_state=42):\n",
    "\n",
    "    if len(feature_importances_list) == 0:\n",
    "        raise ValueError(\"feature_importances_list must not be empty.\")\n",
    "\n",
    "    all_features = feature_importances_list[0]['Feature'].tolist()\n",
    "    importance_matrix = pd.DataFrame(index=all_features)\n",
    "\n",
    "    for idx, df in enumerate(feature_importances_list):\n",
    "        imp = df.set_index('Feature')['Importance']\n",
    "        if normalize:\n",
    "            imp = (imp - imp.min()) / (imp.max() - imp.min() + 1e-9)\n",
    "        importance_matrix[f'Method_{idx+1}'] = imp\n",
    "\n",
    "    importance_matrix = importance_matrix.fillna(0)\n",
    "\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        data_for_clustering = scaler.fit_transform(importance_matrix)\n",
    "    else:\n",
    "        data_for_clustering = importance_matrix.values\n",
    "\n",
    "    if clustering_method == 'kmeans':\n",
    "        clustering_model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    elif clustering_method == 'agglomerative':\n",
    "        clustering_model = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    else:\n",
    "        raise ValueError(\"clustering_method must be either 'kmeans' or 'agglomerative'.\")\n",
    "\n",
    "    cluster_labels = clustering_model.fit_predict(data_for_clustering)\n",
    "\n",
    "    importance_matrix['Cluster'] = cluster_labels\n",
    "\n",
    "    # Compute cluster importance means\n",
    "    cluster_stats = importance_matrix.groupby('Cluster').mean().mean(axis=1)\n",
    "\n",
    "    if cluster_selection_criteria == 'highest_mean':\n",
    "        selected_cluster = cluster_stats.idxmax()\n",
    "    elif cluster_selection_criteria == 'lowest_mean':\n",
    "        selected_cluster = cluster_stats.idxmin()\n",
    "    else:\n",
    "        raise ValueError(\"cluster_selection_criteria must be 'highest_mean' or 'lowest_mean'.\")\n",
    "\n",
    "    selected_features = importance_matrix[importance_matrix['Cluster'] == selected_cluster]\n",
    "\n",
    "    final_importance_df = selected_features.reset_index()[['Feature', 'Cluster']]\n",
    "\n",
    "    if show_plot and not final_importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "\n",
    "        plt.barh(final_importance_df['Feature'],\n",
    "                 -final_importance_df['Cluster'],\n",
    "                 color=[colors[label % 10] for label in final_importance_df['Cluster']],\n",
    "                 alpha=0.9)\n",
    "\n",
    "        plt.xlabel('-(Cluster Label)', fontsize=12)\n",
    "        plt.title(f'Feature Importance by Clustering ({clustering_method.capitalize()})', fontsize=14, pad=20)\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return final_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56688734",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bd7f3",
   "metadata": {},
   "source": [
    "## 3.8.4 Consensus Stability Score across folds/methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3dc26",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Consensus Stability Score Across Folds/Methods } </h2>\n",
    "</summary>\n",
    "<h3> What is Consensus Stability Score? </h3>\n",
    "<p> The Consensus Stability Score measures the consistency of feature importance rankings across different data splits (cross-validation folds) or feature selection methods. A high stability score indicates a reliable ranking.</p>\n",
    "<h3> Role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Helps validate feature importance rankings by testing them across different settings.</li>\n",
    "    <li> Enhances confidence in the final feature set chosen for model training.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC5872818/\" target=\"_blank\">Consensus Stability Score</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779cc765",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- feature_importances_list: List of DataFrames, each with 'Feature' and 'Importance' columns\n",
    "- threshold: Minimum importance to consider a feature \"selected\" (optional)\n",
    "- stability_metric: Stability computation method ('jaccard', 'spearman')\n",
    "- normalize: Whether to normalize importance scores before thresholding\n",
    "- plot_stability: Whether to show a plot of stability scores\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame with feature stability scores\n",
    "- Displays a stability plot if plot_stability=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51bd31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from itertools import combinations\n",
    "\n",
    "def consensus_stability_score(feature_importances_list,\n",
    "                               threshold=None,\n",
    "                               stability_metric='jaccard',\n",
    "                               normalize=True,\n",
    "                               plot_stability=True,\n",
    "                               plot_size=(12, 8),\n",
    "                               random_state=42):\n",
    "\n",
    "    if len(feature_importances_list) < 2:\n",
    "        raise ValueError(\"Need at least two importance lists for consensus stability.\")\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    features = feature_importances_list[0]['Feature'].tolist()\n",
    "\n",
    "    # Build matrix\n",
    "    importance_matrix = pd.DataFrame(index=features)\n",
    "\n",
    "    for idx, df in enumerate(feature_importances_list):\n",
    "        imp = df.set_index('Feature')['Importance']\n",
    "        if normalize:\n",
    "            imp = (imp - imp.min()) / (imp.max() - imp.min() + 1e-9)\n",
    "        importance_matrix[f'Method_{idx+1}'] = imp\n",
    "\n",
    "    importance_matrix = importance_matrix.fillna(0)\n",
    "\n",
    "    stability_scores = []\n",
    "\n",
    "    # Compute pairwise stability\n",
    "    for f_idx, feature in enumerate(features):\n",
    "        values = importance_matrix.loc[feature].values\n",
    "        pairwise_stability = []\n",
    "\n",
    "        for i, j in combinations(range(len(values)), 2):\n",
    "            v1, v2 = values[i], values[j]\n",
    "\n",
    "            if stability_metric == 'jaccard':\n",
    "                selected1 = v1 >= (threshold if threshold is not None else 0.5)\n",
    "                selected2 = v2 >= (threshold if threshold is not None else 0.5)\n",
    "                intersection = int(selected1 and selected2)\n",
    "                union = int(selected1 or selected2)\n",
    "                score = intersection / union if union != 0 else 1.0\n",
    "            elif stability_metric == 'spearman':\n",
    "                score, _ = spearmanr([v1], [v2])\n",
    "                score = score if not np.isnan(score) else 0\n",
    "            else:\n",
    "                raise ValueError(\"stability_metric must be 'jaccard' or 'spearman'.\")\n",
    "\n",
    "            pairwise_stability.append(score)\n",
    "\n",
    "        stability_scores.append(np.mean(pairwise_stability))\n",
    "\n",
    "    stability_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Stability_Score': stability_scores\n",
    "    }).sort_values('Stability_Score', ascending=False)\n",
    "\n",
    "    if plot_stability and not stability_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(stability_df)))\n",
    "\n",
    "        bars = plt.barh(stability_df['Feature'],\n",
    "                        stability_df['Stability_Score'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Consensus Stability Score', fontsize=12)\n",
    "        plt.title(f'Consensus Feature Stability ({stability_metric.capitalize()})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: {stability_metric.capitalize()}\\n\"\n",
    "            f\"Threshold: {threshold if threshold is not None else '0.5'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return stability_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27f3759",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f23b1",
   "metadata": {},
   "source": [
    "## 3.8.5 SHAP + RFE + Correlation Overlay (hybrid consensus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98773f65",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding SHAP + RFE + Correlation Overlay (Hybrid Consensus) } </h2>\n",
    "</summary>\n",
    "<h3> What is Hybrid Consensus? </h3>\n",
    "<p> This method involves combining multiple feature ranking techniques (SHAP, Recursive Feature Elimination (RFE), and correlation-based methods) to obtain a final feature ranking that reflects both model-specific and statistical perspectives.</p>\n",
    "<h3> Role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Combines global model interpretability (SHAP), recursive feature selection (RFE), and feature redundancy assessment (Correlation) to provide a well-rounded ranking.</li>\n",
    "    <li> Offers both local (SHAP) and global (RFE, Correlation) insights into feature importance.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://shap.readthedocs.io/en/latest/\" target=\"_blank\">SHAP Documentation</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html\" target=\"_blank\">Scikit-learn RFE</a></li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Correlation\" target=\"_blank\">Correlation Methods Overview</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9a8a1",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: Trained model (must be compatible with SHAP explainer)\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- correlation_threshold: Maximum allowed feature inter-correlation (e.g., 0.9)\n",
    "- step: Number or fraction of features to remove at each RFE iteration\n",
    "- scoring: Scoring metric for RFE (optional)\n",
    "- cv: Number of cross-validation folds for RFE (optional)\n",
    "- shap_sample_size: Subsample size for SHAP value computation (optional)\n",
    "- random_state: Random seed for reproducibility\n",
    "- plot_result: Whether to plot final hybrid feature importance\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of hybrid consensus feature importance\n",
    "- Displays hybrid importance plot if plot_result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d3a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "def hybrid_shap_rfe_correlation(model,\n",
    "                                 X,\n",
    "                                 y,\n",
    "                                 correlation_threshold=0.9,\n",
    "                                 step=1,\n",
    "                                 scoring=None,\n",
    "                                 cv=5,\n",
    "                                 shap_sample_size=1000,\n",
    "                                 random_state=42,\n",
    "                                 plot_result=True,\n",
    "                                 plot_size=(14, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Step 1: Compute SHAP Importances\n",
    "    explainer = shap.Explainer(model, X)\n",
    "    shap_values = explainer(X.sample(n=min(shap_sample_size, len(X)), random_state=random_state))\n",
    "    shap_importance = np.abs(shap_values.values).mean(axis=0)\n",
    "    shap_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'SHAP_Importance': shap_importance\n",
    "    }).sort_values('SHAP_Importance', ascending=False)\n",
    "\n",
    "    # Step 2: Recursive Feature Elimination\n",
    "    estimator = clone(model)\n",
    "    rfe = RFE(estimator=estimator, step=step)\n",
    "    rfe.fit(X, y)\n",
    "    rfe_ranking = rfe.ranking_\n",
    "\n",
    "    rfe_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'RFE_Rank': rfe_ranking\n",
    "    })\n",
    "\n",
    "    # Step 3: Correlation Overlay\n",
    "    corr_matrix = X.corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    correlated_features = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "    correlation_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Highly_Correlated': X.columns.isin(correlated_features)\n",
    "    })\n",
    "\n",
    "    # Step 4: Combine All\n",
    "    hybrid_df = shap_df.merge(rfe_df, on='Feature').merge(correlation_df, on='Feature')\n",
    "    \n",
    "    # Final Scoring: SHAP Importance (higher) + 1/RFE Rank (lower rank = better) - Correlation Penalty\n",
    "    hybrid_df['Hybrid_Score'] = (\n",
    "        (hybrid_df['SHAP_Importance'].rank(ascending=False) + \n",
    "         (1 / hybrid_df['RFE_Rank']).rank(ascending=False))\n",
    "    )\n",
    "\n",
    "    # Apply penalty for correlated features\n",
    "    hybrid_df.loc[hybrid_df['Highly_Correlated'], 'Hybrid_Score'] *= 0.8\n",
    "\n",
    "    hybrid_df = hybrid_df.sort_values('Hybrid_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if plot_result and not hybrid_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.coolwarm(np.linspace(0.2, 1, len(hybrid_df)))\n",
    "\n",
    "        bars = plt.barh(hybrid_df['Feature'],\n",
    "                        hybrid_df['Hybrid_Score'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Hybrid Feature Score', fontsize=12)\n",
    "        plt.title('SHAP + RFE + Correlation Overlay (Hybrid Importance)', fontsize=15, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "        plt.gca().invert_yaxis()\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Hybrid Method: SHAP + RFE + Correlation\\n\"\n",
    "            f\"Correlation Threshold: {correlation_threshold}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return hybrid_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f5f2d",
   "metadata": {},
   "source": [
    "-------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

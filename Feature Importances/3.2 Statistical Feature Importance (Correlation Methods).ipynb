{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98bdc8d3",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click the markdown to see the code instead of this </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a9248",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5609e0c8",
   "metadata": {},
   "source": [
    "# 3.2 Statistical Feature Importance (Correlational Methods)\n",
    "\n",
    "-- Purely data-driven, pre-modeling filters\n",
    "\n",
    "- Correlation (Pearson, Spearman, Kendall)\n",
    "- Mutual Information\n",
    "- Chi-Square Test (for categorical target)\n",
    "- ANOVA F-test\n",
    "- Variance Threshold\n",
    "- Maximal Information Coefficient (MIC)\n",
    "- Kolmogorov-Smirnov Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505248a",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66e65c",
   "metadata": {},
   "source": [
    "## 3.2.1 Correlation (Pearson, Spearman, Kendall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0de18c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Correlation-based Feature Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is Correlation? </h3>\n",
    "<p> Correlation measures the strength and direction of the relationship between two variables. It helps to determine how changes in one feature reflect in another.</p>\n",
    "<h3> Types of Correlation Coefficients: </h3>\n",
    "<ul>\n",
    "    <li><strong>Pearson:</strong> Measures linear relationships between continuous variables.</li>\n",
    "    <li><strong>Spearman:</strong> Rank-based; captures monotonic relationships, useful for non-linear data.</li>\n",
    "    <li><strong>Kendall:</strong> Rank correlation coefficient that evaluates the ordinal association between two quantities.</li>\n",
    "</ul>\n",
    "<h3> It's role in Feature Importance:</h3>\n",
    "<ul>\n",
    "    <li> Helps identify multicollinearity and remove redundant features.</li>\n",
    "    <li> Correlation with the target variable indicates predictive power (for regression/classification).</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=Vfo5le26IhY\" target=\"_blank\">Pearson, Spearman, Kendall Explained</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8151965e",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e466c",
   "metadata": {},
   "source": [
    "### 3.2.1.1 Pearson Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80d44b",
   "metadata": {},
   "source": [
    "Pearson Correlation-based Feature Importance.\n",
    "\n",
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (Series or array-like)\n",
    "- task_type: 'classification' or 'regression'\n",
    "- top_n_features: Number of top features to display\n",
    "- handle_categorical: Encode categorical features automatically\n",
    "- show_plot: Whether to display the bar plot\n",
    "- plot_size: Size of the bar plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features and their absolute Pearson correlation\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1bc3570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def pearson_feature_importance(X, y,\n",
    "                                task_type='classification',\n",
    "                                top_n_features=20,\n",
    "                                handle_categorical=True,\n",
    "                                show_plot=True,\n",
    "                                plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    if handle_categorical:\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    if task_type == 'classification' and not np.issubdtype(np.array(y).dtype, np.number):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    importance_scores = []\n",
    "    for feature in X.columns:\n",
    "        try:\n",
    "            corr, _ = pearsonr(X[feature], y)\n",
    "            importance_scores.append(abs(corr))\n",
    "        except:\n",
    "            importance_scores.append(0.0)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.coolwarm(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Absolute Pearson Correlation', fontsize=12)\n",
    "        plt.title(f'Pearson Correlation Feature Importance\\n(Task: {task_type.capitalize()})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: |Pearson Correlation|\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51752248",
   "metadata": {},
   "source": [
    "### 3.2.1.1 Spearman Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de085b0",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> Understanding Spearman Correlation and it's role in Feature Importance </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f8771",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (Series or array-like)\n",
    "- task_type: 'classification' or 'regression'\n",
    "- top_n_features: Number of top features to display\n",
    "- handle_categorical: Automatically encode categorical features if necessary\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features and their absolute Spearman correlation\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eca9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def spearman_feature_importance(X, y,\n",
    "                                 task_type='classification',\n",
    "                                 top_n_features=20,\n",
    "                                 handle_categorical=True,\n",
    "                                 show_plot=True,\n",
    "                                 plot_size=(12, 8)):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    if handle_categorical:\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    if task_type == 'classification' and not np.issubdtype(np.array(y).dtype, np.number):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    importance_scores = []\n",
    "    for feature in X.columns:\n",
    "        try:\n",
    "            corr, _ = spearmanr(X[feature], y)\n",
    "            importance_scores.append(abs(corr))\n",
    "        except:\n",
    "            importance_scores.append(0.0)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Absolute Spearman Correlation', fontsize=12)\n",
    "        plt.title(f'Spearman Correlation Feature Importance\\n(Task: {task_type.capitalize()})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                   fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: |Spearman Correlation|\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdca443",
   "metadata": {},
   "source": [
    "### 3.2.1.1 Kendall Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57a4d4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> Understanding Kendall Correlation and it's role in Feature Importance </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12d8031",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (Series or array-like)\n",
    "- task_type: 'classification' or 'regression'\n",
    "- top_n_features: Number of top features to display\n",
    "- handle_categorical: Automatically encode categorical features if necessary\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "Returns:\n",
    "\n",
    "- DataFrame containing features and their absolute Kendall correlation\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6dc21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def kendall_feature_importance(X, y,\n",
    "                                task_type='classification',\n",
    "                                top_n_features=20,\n",
    "                                handle_categorical=True,\n",
    "                                show_plot=True,\n",
    "                                plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    if handle_categorical:\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    if task_type == 'classification' and not np.issubdtype(np.array(y).dtype, np.number):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    importance_scores = []\n",
    "    for feature in X.columns:\n",
    "        try:\n",
    "            corr, _ = kendalltau(X[feature], y)\n",
    "            importance_scores.append(abs(corr))\n",
    "        except:\n",
    "            importance_scores.append(0.0)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Absolute Kendall Correlation', fontsize=12)\n",
    "        plt.title(f'Kendall Correlation Feature Importance\\n(Task: {task_type.capitalize()})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: |Kendall Correlation|\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de446be5",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a0f535",
   "metadata": {},
   "source": [
    "## 3.2.2 Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e92b7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Mutual Information for Feature Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is Mutual Information? </h3>\n",
    "<p> Mutual Information (MI) measures the amount of information one feature provides about the target. It is non-linear and captures any kind of relationship (not just linear).</p>\n",
    "<h3> It's role in Feature Importance:</h3>\n",
    "<ul>\n",
    "    <li> High MI implies the feature provides a lot of useful information about the target variable.</li>\n",
    "    <li> Works for both classification and regression problems.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#mutual-information\" target=\"_blank\">Mutual Information - scikit-learn</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99d19b",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (Series or array-like)\n",
    "- task_type: 'classification' or 'regression'\n",
    "- top_n_features: Number of top features to display\n",
    "- handle_categorical: Automatically encode categorical features if necessary\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- discrete_features: Whether to treat features as discrete (default 'auto')\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features and their mutual information scores\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810ba3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def mutual_info_feature_importance(X, y,\n",
    "                                    task_type='classification',\n",
    "                                    top_n_features=20,\n",
    "                                    handle_categorical=True,\n",
    "                                    discrete_features='auto',\n",
    "                                    show_plot=True,\n",
    "                                    plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    if handle_categorical:\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    if task_type == 'classification' and not np.issubdtype(np.array(y).dtype, np.number):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    if task_type == 'classification':\n",
    "        mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=42)\n",
    "    elif task_type == 'regression':\n",
    "        mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be either 'classification' or 'regression'\")\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': mi_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.cividis(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Mutual Information Score', fontsize=12)\n",
    "        plt.title(f'Mutual Information Feature Importance\\n(Task: {task_type.capitalize()})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Mutual Information\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66280e18",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e22dc1f",
   "metadata": {},
   "source": [
    "## 3.2.3 Chi-Square Test (for categorical target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4e764",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Chi-Square Test for Feature Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is the Chi-Square Test? </h3>\n",
    "<p> The Chi-Square test evaluates if two categorical variables are independent. It compares observed vs. expected frequencies in a contingency table.</p>\n",
    "<h3> It's role in Feature Importance:</h3>\n",
    "<ul>\n",
    "    <li> Useful for categorical input and output variables.</li>\n",
    "    <li> A high chi-square statistic implies a strong relationship with the target.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=mjSnOaoqeGo&t=107s\" target=\"_blank\">Chi-Square Test Explained</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309e8fed",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (Series or array-like) — must be categorical\n",
    "- top_n_features: Number of top features to display\n",
    "- handle_categorical: Automatically encode categorical features if necessary\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features and their chi-square statistic scores\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66eb71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "def chi2_feature_importance(X, y,\n",
    "                             top_n_features=20,\n",
    "                             handle_categorical=True,\n",
    "                             show_plot=True,\n",
    "                             plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    if handle_categorical:\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    # Chi2 expects non-negative values, scale features between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Encode target if not numeric\n",
    "    if not np.issubdtype(np.array(y).dtype, np.number):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    chi2_scores, _ = chi2(X_scaled, y)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': chi2_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Chi-Square Statistic', fontsize=12)\n",
    "        plt.title('Chi-Square Feature Importance\\n(Classification Only)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Chi-Square Test\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da52691",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027c93c",
   "metadata": {},
   "source": [
    "## 3.2.4 ANOVA F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f83c0c7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding ANOVA F-Test for Feature Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is ANOVA F-Test? </h3>\n",
    "<p> The ANOVA (Analysis of Variance) F-test measures whether the means of two or more groups are significantly different from each other.</p>\n",
    "<h3> It's role in Feature Importance:</h3>\n",
    "<ul>\n",
    "    <li> Used for continuous input features and a categorical output.</li>\n",
    "    <li> A higher F-score means the feature is more discriminatory with respect to the target class.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#f-test-for-feature-selection\" target=\"_blank\">ANOVA F-Test - scikit-learn</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93b459",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (Series or array-like) — must be categorical\n",
    "- top_n_features: Number of top features to display\n",
    "- handle_categorical: Automatically encode categorical features if necessary\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features and their ANOVA F-statistic scores\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "072af647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def anova_f_feature_importance(X, y,\n",
    "                                top_n_features=20,\n",
    "                                handle_categorical=True,\n",
    "                                show_plot=True,\n",
    "                                plot_size=(12, 8)):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    if handle_categorical:\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Encode target if not numeric\n",
    "    if not np.issubdtype(np.array(y).dtype, np.number):\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "    f_scores, _ = f_classif(X_scaled, y)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': f_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('ANOVA F-Statistic', fontsize=12)\n",
    "        plt.title('ANOVA F-Test Feature Importance\\n(Classification Only)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.5,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.1f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: ANOVA F-Test\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd853582",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59851186",
   "metadata": {},
   "source": [
    "## 3.2.5 Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eae998",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Variance Threshold for Feature Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is Variance Threshold? </h3>\n",
    "<p> A simple baseline method that removes features with low variance. Low variance features don't contribute much to learning.</p>\n",
    "<h3> It's role in Feature Importance:</h3>\n",
    "<ul>\n",
    "    <li> Eliminates features that are mostly constant.</li>\n",
    "    <li> Especially useful in sparse datasets like text data (bag of words).</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#variance-threshold\" target=\"_blank\">Variance Threshold - scikit-learn</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1f8521",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- threshold: Variance threshold below which features will be considered low-importance\n",
    "- top_n_features: Number of top features to display (highest variance first)\n",
    "- handle_categorical: Automatically encode categorical features if necessary\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features and their variance scores\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "044a4d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def variance_threshold_feature_importance(X,\n",
    "                                           threshold=0.0,\n",
    "                                           top_n_features=20,\n",
    "                                           handle_categorical=True,\n",
    "                                           show_plot=True,\n",
    "                                           plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    if handle_categorical:\n",
    "        X = X.copy()\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    # Compute variance for each feature\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    selector.fit(X)\n",
    "\n",
    "    variance_scores = selector.variances_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': variance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.cividis(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Variance', fontsize=12)\n",
    "        plt.title('Variance Threshold Feature Importance\\n(Unsupervised)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Variance Threshold\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\\n\"\n",
    "            f\"Threshold: {threshold}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b4031",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9530c",
   "metadata": {},
   "source": [
    "## 3.2.6 Maximal Information Coefficient (MIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b550f4b1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Maximal Information Coefficient (MIC) } </h2>\n",
    "</summary>\n",
    "<h3> What is MIC? </h3>\n",
    "<p> MIC captures both linear and non-linear relationships between features and targets. It is part of the Maximal Information-based Nonparametric Exploration (MINE) statistics.</p>\n",
    "<h3> It's role in Feature Importance:</h3>\n",
    "<ul>\n",
    "    <li> Unlike correlation, MIC can detect a wide range of functional relationships.</li>\n",
    "    <li> Particularly useful when relationships are complex or unknown.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://minepy.readthedocs.io/en/latest/\" target=\"_blank\">MINE Documentation</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed7c060",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target (Series or 1D array)\n",
    "- model: A fitted model (for model-specific feature evaluation, optional but recommended)\n",
    "- top_n_features: Number of top features to display (highest MIC scores first)\n",
    "- normalize: Whether to normalize MIC scores between 0 and 1\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame containing features, MIC scores, and optionally model feature importances (if model provided)\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdc81431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting minepy\n",
      "  Downloading minepy-1.2.6.tar.gz (496 kB)\n",
      "     -------------------------------------- 497.0/497.0 kB 7.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from minepy) (1.23.5)\n",
      "Building wheels for collected packages: minepy\n",
      "  Building wheel for minepy (setup.py): started\n",
      "  Building wheel for minepy (setup.py): finished with status 'done'\n",
      "  Created wheel for minepy: filename=minepy-1.2.6-cp310-cp310-win_amd64.whl size=48032 sha256=bce2f6b68a8f34f0f6239553180d00465fa047527dbaed21c9cdf53a581ca9df\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\cd\\02\\e9\\6bd979a2348bb20625593ce81029d4b1730194c261077be128\n",
      "Successfully built minepy\n",
      "Installing collected packages: minepy\n",
      "Successfully installed minepy-1.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install minepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f93466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from minepy import MINE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def mic_feature_importance(X,\n",
    "                            y,\n",
    "                            model=None,\n",
    "                            top_n_features=20,\n",
    "                            normalize=True,\n",
    "                            show_plot=True,\n",
    "                            plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "        y = y.values.ravel()\n",
    "\n",
    "    X = X.copy()\n",
    "    # Label encode categorical features if necessary\n",
    "    for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "        X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    mic_scores = []\n",
    "    mic = MINE()\n",
    "\n",
    "    for col in X.columns:\n",
    "        mic.compute_score(X[col], y)\n",
    "        score = mic.mic()\n",
    "        mic_scores.append(score)\n",
    "\n",
    "    mic_scores = np.array(mic_scores)\n",
    "\n",
    "    if normalize:\n",
    "        mic_scores = mic_scores / mic_scores.max()\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'MIC_Score': mic_scores\n",
    "    }).sort_values('MIC_Score', ascending=False)\n",
    "\n",
    "    # Optionally add model feature importances if provided\n",
    "    if model is not None and hasattr(model, 'feature_importances_'):\n",
    "        importance_df['Model_Importance'] = model.feature_importances_\n",
    "    elif model is not None and hasattr(model, 'coef_'):\n",
    "        importance_df['Model_Importance'] = np.abs(model.coef_).flatten()\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('MIC_Score', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['MIC_Score'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('MIC Score', fontsize=12)\n",
    "        plt.title('Maximal Information Coefficient (MIC) Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: MIC\\n\"\n",
    "            f\"Model-Specific: {'Yes' if model else 'No'}\\n\"\n",
    "            f\"Normalize Scores: {normalize}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4189618",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8422635e",
   "metadata": {},
   "source": [
    "## 3.2.7 Kolmogorov-Smirnov Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df506de6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Kolmogorov-Smirnov (KS) Statistic } </h2>\n",
    "</summary>\n",
    "<h3> What is KS Statistic? </h3>\n",
    "<p> The KS statistic measures the maximum distance between the cumulative distributions of two samples. It evaluates how well a feature separates the classes.</p>\n",
    "<h3> It's role in Feature Importance:</h3>\n",
    "<ul>\n",
    "    <li> Common in binary classification, especially in credit scoring.</li>\n",
    "    <li> Higher KS value means better separability between classes based on that feature.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=ZO2RmSkXK3c\" target=\"_blank\">KS Statistic Explained</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5493ce",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "    \n",
    "- X: Features (DataFrame)\n",
    "- y: Target (Series or 1D array, binary classification only)\n",
    "- model: A fitted classification model (optional, for combining model-specific feature importances)\n",
    "- top_n_features: Number of top features to display (highest KS scores first)\n",
    "- handle_categorical: Automatically encode categorical features if necessary\n",
    "- show_plot: Whether to display the feature importance plot\n",
    "- plot_size: Tuple indicating the size of the plot (width, height)\n",
    "\n",
    "##### Returns:\n",
    "    \n",
    "- DataFrame containing features, KS statistic scores, and optionally model feature importances\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aed5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_feature_importance(X,\n",
    "                           y,\n",
    "                           model=None,\n",
    "                           top_n_features=20,\n",
    "                           handle_categorical=True,\n",
    "                           show_plot=True,\n",
    "                           plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    if isinstance(y, (pd.Series, pd.DataFrame)):\n",
    "        y = y.values.ravel()\n",
    "\n",
    "    X = X.copy()\n",
    "\n",
    "    if handle_categorical:\n",
    "        for col in X.select_dtypes(include=['object', 'category']).columns:\n",
    "            X[col] = LabelEncoder().fit_transform(X[col])\n",
    "\n",
    "    ks_scores = []\n",
    "\n",
    "    for col in X.columns:\n",
    "        class_0 = X[y == 0][col]\n",
    "        class_1 = X[y == 1][col]\n",
    "        ks_stat, _ = ks_2samp(class_0, class_1)\n",
    "        ks_scores.append(ks_stat)\n",
    "\n",
    "    ks_scores = np.array(ks_scores)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'KS_Score': ks_scores\n",
    "    }).sort_values('KS_Score', ascending=False)\n",
    "\n",
    "    # Optionally add model feature importances if provided\n",
    "    if model is not None and hasattr(model, 'feature_importances_'):\n",
    "        importance_df['Model_Importance'] = model.feature_importances_\n",
    "    elif model is not None and hasattr(model, 'coef_'):\n",
    "        importance_df['Model_Importance'] = np.abs(model.coef_).flatten()\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('KS_Score', ascending=True)\n",
    "\n",
    "        colors = plt.cm.magma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['KS_Score'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('KS Statistic', fontsize=12)\n",
    "        plt.title('Kolmogorov-Smirnov (KS) Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Kolmogorov-Smirnov (KS)\\n\"\n",
    "            f\"Model-Specific: {'Yes' if model else 'No'}\\n\"\n",
    "            f\"Handle Categorical: {handle_categorical}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520718c",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

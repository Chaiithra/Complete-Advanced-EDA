{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d4fba3b",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click the markdown to see the code instead of this </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec3917",
   "metadata": {},
   "source": [
    "# 3.5 Advanced & Research-based Techniques\n",
    "-- High-value academic & ensemble techniques\n",
    "\n",
    "- Boruta Algorithm\n",
    "- Recursive Feature Elimination (RFE)/ RFECV\n",
    "- Stability Selection\n",
    "- Drop-Column Importance\n",
    "- SHAP (all types)\n",
    "- LIME\n",
    "- Permutation Importance\n",
    "- PDP\n",
    "- ReliefF\n",
    "- MRMR (Minimum Redundancy Maximum Relevance)\n",
    "- Leave-One-Covariate-Out (LOCO)\n",
    "- Forward/Backward Sequential Feature Selection\n",
    "- OMP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45954aa",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d689a",
   "metadata": {},
   "source": [
    "## 3.5.1 Boruta Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d658a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Boruta for Feature Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is Boruta? </h3>\n",
    "<p> Boruta is a wrapper method built around Random Forest to perform all-relevant feature selection.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Compares real features with randomized \"shadow\" features to decide importance.</li>\n",
    "    <li> Very robust for high-dimensional data and noise-resistant.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://github.com/scikit-learn-contrib/boruta_py\" target=\"_blank\">Boruta Python GitHub</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02235e60",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- estimator: Base estimator for Boruta (must have feature_importances_ attribute)\n",
    "- max_iter: Maximum number of Boruta iterations\n",
    "- n_estimators: Number of trees for tree-based estimators (if estimator supports it)\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- Fitted Boruta selector\n",
    "- DataFrame of selected feature importances\n",
    "- Displays a plot of feature importances if show_plot=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ffe766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boruta\n",
      "  Downloading Boruta-0.4.3-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.9/57.9 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from boruta) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from boruta) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from boruta) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.17.1->boruta) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.17.1->boruta) (3.5.0)\n",
      "Installing collected packages: boruta\n",
      "Successfully installed boruta-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c28b0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from boruta import BorutaPy\n",
    "\n",
    "def boruta_feature_importance(X,\n",
    "                               y,\n",
    "                               estimator=None,\n",
    "                               max_iter=100,\n",
    "                               n_estimators=100,\n",
    "                               random_state=42,\n",
    "                               show_plot=True,\n",
    "                               plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if estimator is None:\n",
    "        # Default to RandomForest based on problem type\n",
    "        if len(np.unique(y)) > 10:\n",
    "            estimator = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "        else:\n",
    "            estimator = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
    "\n",
    "    boruta_selector = BorutaPy(estimator,\n",
    "                               n_estimators='auto',\n",
    "                               max_iter=max_iter,\n",
    "                               random_state=random_state,\n",
    "                               verbose=0)\n",
    "\n",
    "    boruta_selector.fit(X_processed.values, np.array(y))\n",
    "\n",
    "    selected_features = np.array(feature_names)[boruta_selector.support_].tolist()\n",
    "    weak_features = np.array(feature_names)[boruta_selector.support_weak_].tolist()\n",
    "    all_features = selected_features + weak_features\n",
    "\n",
    "    importances = boruta_selector.ranking_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance (Rank)': importances\n",
    "    }).sort_values('Importance (Rank)', ascending=True)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance (Rank)', ascending=False)\n",
    "\n",
    "        colors = plt.cm.coolwarm(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        -importance_df_sorted['Importance (Rank)'],  # Inverted because lower rank is better\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('-(Feature Rank)', fontsize=12)\n",
    "        plt.title('Boruta Feature Selection (Feature Importance via Ranks)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width - 0.2,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{-int(width)}',\n",
    "                     va='center',\n",
    "                     fontsize=9,\n",
    "                     color='white')\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Boruta Selection\\n\"\n",
    "            f\"Base Estimator: {type(estimator).__name__}\\n\"\n",
    "            f\"Max Iter: {max_iter}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return boruta_selector, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae076ce3",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f6a1dc",
   "metadata": {},
   "source": [
    "## 3.5.2 Recursive Feature Elimination (RFE)/ RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cb14b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Recursive Feature Elimination (RFE) / RFECV } </h2>\n",
    "</summary>\n",
    "<h3> What is RFE? </h3>\n",
    "<p> RFE works by recursively removing the least important features and building the model on the remaining attributes.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Model-agnostic, supports any estimator with `coef_` or `feature_importances_`.</li>\n",
    "    <li> RFECV extends RFE with cross-validation to automatically select the optimal number of features.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/feature_selection.html#rfe\" target=\"_blank\">scikit-learn RFE Docs</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e5754",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- estimator: Base model to use (must have coef_ or feature_importances_ attribute)\n",
    "- step: Number (or fraction) of features to remove at each iteration\n",
    "- cv: Number of cross-validation folds (only for RFECV; optional)\n",
    "- scoring: Scoring metric for cross-validation (optional)\n",
    "- n_features_to_select: Desired number of features to select (optional, for RFE only)\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot feature ranking\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- Fitted RFE or RFECV selector\n",
    "- DataFrame of selected feature rankings\n",
    "- Displays a plot of feature rankings if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a18a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "def rfe_feature_importance(X,\n",
    "                            y,\n",
    "                            estimator,\n",
    "                            step=1,\n",
    "                            cv=None,\n",
    "                            scoring=None,\n",
    "                            n_features_to_select=None,\n",
    "                            random_state=42,\n",
    "                            show_plot=True,\n",
    "                            plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if cv:\n",
    "        selector = RFECV(estimator=estimator,\n",
    "                         step=step,\n",
    "                         cv=cv,\n",
    "                         scoring=scoring,\n",
    "                         n_jobs=-1)\n",
    "    else:\n",
    "        selector = RFE(estimator=estimator,\n",
    "                       n_features_to_select=n_features_to_select,\n",
    "                       step=step)\n",
    "\n",
    "    selector.fit(X_processed, y)\n",
    "\n",
    "    ranking = selector.ranking_\n",
    "    support = selector.support_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Ranking': ranking,\n",
    "        'Selected': support\n",
    "    }).sort_values('Ranking', ascending=True)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Ranking', ascending=False)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        -importance_df_sorted['Ranking'],  # Lower rank = better\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('-(Feature Ranking)', fontsize=12)\n",
    "        plt.title('Recursive Feature Elimination (Feature Importance by Ranking)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width - 0.2,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{-int(width)}',\n",
    "                     va='center',\n",
    "                     fontsize=9,\n",
    "                     color='white')\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: {'RFECV' if cv else 'RFE'}\\n\"\n",
    "            f\"Estimator: {type(estimator).__name__}\\n\"\n",
    "            f\"Step: {step}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.02, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='left',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return selector, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa3b15",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01b632",
   "metadata": {},
   "source": [
    "## 3.5.3 Stability Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558189d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Stability Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is Stability Selection? </h3>\n",
    "<p> Stability selection is a method that combines bootstrapping with feature selection techniques like Lasso to identify features that are consistently selected across multiple subsamples.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Improves robustness and reduces overfitting by choosing stable features.</li>\n",
    "    <li> Helps avoid selecting features that only appear due to random variation.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/whats_new/v0.24.html#id13\" target=\"_blank\">sklearn StabilitySelection</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0b60b",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- base_estimator: Base model (must support feature selection via coef_ or feature_importances_)\n",
    "- n_bootstrap_iterations: Number of bootstrap resampling iterations\n",
    "- sample_fraction: Fraction of samples used in each bootstrap sample\n",
    "- selection_threshold: Minimum selection frequency for a feature to be considered important\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot feature stability scores\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of features with their selection frequencies\n",
    "- List of selected features (based on selection threshold)\n",
    "- Displays a plot of feature stability scores if show_plot=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925b351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "\n",
    "def stability_selection(X,\n",
    "                         y,\n",
    "                         base_estimator,\n",
    "                         n_bootstrap_iterations=100,\n",
    "                         sample_fraction=0.75,\n",
    "                         selection_threshold=0.5,\n",
    "                         random_state=42,\n",
    "                         show_plot=True,\n",
    "                         plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    feature_names = X.columns.tolist()\n",
    "    selection_counts = np.zeros(n_features)\n",
    "\n",
    "    for iteration in range(n_bootstrap_iterations):\n",
    "        bootstrap_idx = np.random.choice(n_samples,\n",
    "                                         size=int(sample_fraction * n_samples),\n",
    "                                         replace=True)\n",
    "        X_sample = X.iloc[bootstrap_idx]\n",
    "        y_sample = np.array(y)[bootstrap_idx]\n",
    "\n",
    "        estimator = clone(base_estimator)\n",
    "        estimator.fit(X_sample, y_sample)\n",
    "\n",
    "        if hasattr(estimator, \"coef_\"):\n",
    "            coefs = estimator.coef_\n",
    "            if coefs.ndim > 1:\n",
    "                coefs = coefs[0]  # For multi-output models\n",
    "        elif hasattr(estimator, \"feature_importances_\"):\n",
    "            coefs = estimator.feature_importances_\n",
    "        else:\n",
    "            raise ValueError(\"Estimator must have coef_ or feature_importances_ attribute.\")\n",
    "\n",
    "        selected = np.abs(coefs) > 1e-6  # Threshold for non-zero importance\n",
    "        selection_counts += selected\n",
    "\n",
    "    stability_scores = selection_counts / n_bootstrap_iterations\n",
    "\n",
    "    stability_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Stability_Score': stability_scores\n",
    "    }).sort_values('Stability_Score', ascending=False)\n",
    "\n",
    "    selected_features = stability_df[stability_df['Stability_Score'] >= selection_threshold]['Feature'].tolist()\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        stability_df_sorted = stability_df.sort_values('Stability_Score', ascending=True)\n",
    "\n",
    "        colors = plt.cm.cividis(np.linspace(0.2, 1, len(stability_df_sorted)))\n",
    "        bars = plt.barh(stability_df_sorted['Feature'],\n",
    "                        stability_df_sorted['Stability_Score'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.axvline(selection_threshold, color='red', linestyle='--', label=f'Threshold ({selection_threshold})')\n",
    "        plt.xlabel('Stability Score (Selection Frequency)', fontsize=12)\n",
    "        plt.title('Stability Selection Feature Importance', fontsize=14, pad=20)\n",
    "        plt.legend()\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Stability Selection\\n\"\n",
    "            f\"Bootstrap Iterations: {n_bootstrap_iterations}\\n\"\n",
    "            f\"Sample Fraction: {sample_fraction}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return stability_df, selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be2dee",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55314042",
   "metadata": {},
   "source": [
    "## 3.5.4 Drop-Column Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4254d8f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Drop-Column Feature Importance } </h2>\n",
    "</summary>\n",
    "<h3> What is Drop-Column Importance? </h3>\n",
    "<p> This technique assesses the importance of a feature by training a model with and without the feature and measuring the performance difference.</p>\n",
    "<h3> Its role in Feature Importance: </h3>\n",
    "<ul>\n",
    "    <li> Measures real impact of each feature on model performance.</li>\n",
    "    <li> Model-agnostic but computationally expensive.</li>\n",
    "</ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef3ead",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- model: A fitted model (must have fit and score methods)\n",
    "- scoring: Scoring metric function (e.g., accuracy_score, r2_score) or string for model.score\n",
    "- greater_is_better: Whether higher scores are better (True for accuracy, False for error metrics)\n",
    "- random_state: Random seed for reproducibility (used if model has stochasticity)\n",
    "- show_plot: Whether to plot drop-column importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame with feature importance scores (performance drop when feature removed)\n",
    "- List of most important features (descending order)\n",
    "- Displays a plot of drop-column importances if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b7712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def drop_column_importance(X,\n",
    "                            y,\n",
    "                            model,\n",
    "                            scoring=None,\n",
    "                            greater_is_better=True,\n",
    "                            random_state=42,\n",
    "                            show_plot=True,\n",
    "                            plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    base_model = clone(model)\n",
    "    base_model.fit(X, y)\n",
    "\n",
    "    if scoring is not None:\n",
    "        baseline_score = scoring(y, base_model.predict(X))\n",
    "    else:\n",
    "        baseline_score = base_model.score(X, y)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    importances = []\n",
    "\n",
    "    for feature in feature_names:\n",
    "        X_dropped = X.drop(columns=[feature])\n",
    "        model_clone = clone(model)\n",
    "\n",
    "        # Ensure model's randomness stays consistent if model has random_state\n",
    "        if hasattr(model_clone, 'random_state'):\n",
    "            setattr(model_clone, 'random_state', random_state.randint(0, 10000))\n",
    "\n",
    "        model_clone.fit(X_dropped, y)\n",
    "\n",
    "        if scoring is not None:\n",
    "            dropped_score = scoring(y, model_clone.predict(X_dropped))\n",
    "        else:\n",
    "            dropped_score = model_clone.score(X_dropped, y)\n",
    "\n",
    "        # Importance is how much the performance drops\n",
    "        if greater_is_better:\n",
    "            importance = baseline_score - dropped_score\n",
    "        else:\n",
    "            importance = dropped_score - baseline_score\n",
    "\n",
    "        importances.append(importance)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    important_features = importance_df['Feature'].tolist()\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Performance Drop After Feature Removal', fontsize=12)\n",
    "        plt.title('Drop-Column Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.001,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Drop-Column Importance\\n\"\n",
    "            f\"Greater is Better: {greater_is_better}\\n\"\n",
    "            f\"Scoring: {'Custom' if scoring is not None else 'model.score'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df, important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7113781",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed4c3b7",
   "metadata": {},
   "source": [
    "## 3.5.5 SHAP (all types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68aa21",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding SHAP (Tree, Kernel, Deep, Linear) } </h2>\n",
    "</summary>\n",
    "<h3> What is SHAP? </h3>\n",
    "<p> SHAP is a game-theoretic approach to explain the output of any machine learning model by assigning each feature an importance value for a particular prediction.</p>\n",
    "<h3> Its role in Feature Importance: </h3>\n",
    "<ul>\n",
    "    <li> Provides consistent, local + global interpretability.</li>\n",
    "    <li> Supports different models via SHAP variants:\n",
    "        <ul>\n",
    "            <li><b>TreeSHAP</b>: For tree-based models.</li>\n",
    "            <li><b>DeepSHAP</b>: For deep learning models.</li>\n",
    "            <li><b>KernelSHAP</b>: Model-agnostic.</li>\n",
    "            <li><b>LinearSHAP</b>: For linear models.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://shap.readthedocs.io/en/latest/index.html\" target=\"_blank\">SHAP Documentation</a></li>\n",
    "    <li><a href=\"https://christophm.github.io/interpretable-ml-book/shap.html\" target=\"_blank\">Interpretable ML Book - SHAP</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a09a44d",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- model: A fitted model (must be compatible with SHAP)\n",
    "- model_type: Type of model (\"tree\", \"linear\", \"deep\", \"kernel\", or \"auto\")\n",
    "- background_sample_size: Number of background samples to use for KernelExplainer (if needed)\n",
    "- show_plot: Whether to plot global feature importance\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- max_display: Max number of features to display in plot (optional)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of mean absolute SHAP values per feature (global importance)\n",
    "- SHAP explainer object (for further local explanations if needed)\n",
    "- Displays a SHAP summary bar plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce59f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.47.2-cp310-cp310-win_amd64.whl (544 kB)\n",
      "     -------------------------------------- 544.2/544.2 kB 4.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (2.0.0)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (0.56.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (4.12.2)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (1.6.1)\n",
      "Collecting slicer==0.0.8\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (1.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (1.23.5)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from shap) (22.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (65.6.3)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.39.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->shap) (2024.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->shap) (2022.7)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
      "Installing collected packages: slicer, shap\n",
      "Successfully installed shap-0.47.2 slicer-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24ee5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def shap_feature_importance(X,\n",
    "                             model,\n",
    "                             model_type=\"auto\",\n",
    "                             background_sample_size=100,\n",
    "                             show_plot=True,\n",
    "                             plot_size=(12, 8),\n",
    "                             max_display=None,\n",
    "                             random_state=42):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    X_sampled = X.copy()\n",
    "\n",
    "    # Determine appropriate SHAP Explainer\n",
    "    if model_type == \"tree\":\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "    elif model_type == \"linear\":\n",
    "        explainer = shap.LinearExplainer(model, X_sampled)\n",
    "    elif model_type == \"deep\":\n",
    "        explainer = shap.DeepExplainer(model, X_sampled)\n",
    "    elif model_type == \"kernel\":\n",
    "        background = X_sampled.sample(n=min(background_sample_size, len(X_sampled)),\n",
    "                                      random_state=random_state)\n",
    "        explainer = shap.KernelExplainer(model.predict, background)\n",
    "    elif model_type == \"auto\":\n",
    "        try:\n",
    "            explainer = shap.Explainer(model, X_sampled)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Auto Explainer could not initialize. Please specify model_type manually. Error: {e}\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model_type '{model_type}'. Choose from 'tree', 'linear', 'deep', 'kernel', 'auto'.\")\n",
    "\n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer(X_sampled)\n",
    "\n",
    "    # If model outputs multiple dimensions, take the first\n",
    "    if hasattr(shap_values, 'values') and isinstance(shap_values.values, list):\n",
    "        shap_array = np.abs(shap_values.values[0])\n",
    "    else:\n",
    "        shap_array = np.abs(shap_values.values)\n",
    "\n",
    "    mean_abs_shap = np.mean(shap_array, axis=0)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'MeanAbsSHAP': mean_abs_shap\n",
    "    }).sort_values('MeanAbsSHAP', ascending=False)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "\n",
    "        display_df = importance_df if max_display is None else importance_df.head(max_display)\n",
    "        importance_df_sorted = display_df.sort_values('MeanAbsSHAP', ascending=True)\n",
    "\n",
    "        colors = plt.cm.magma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['MeanAbsSHAP'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Mean |SHAP value|', fontsize=12)\n",
    "        plt.title('SHAP Feature Importance (Global)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.001,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: SHAP\\n\"\n",
    "            f\"Model Type: {model_type}\\n\"\n",
    "            f\"Samples: {len(X_sampled)}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df, explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980c28f",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037cac6",
   "metadata": {},
   "source": [
    "## 3.5.6 LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392eb32a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding LIME (Local Interpretable Model-Agnostic Explanations) } </h2>\n",
    "</summary>\n",
    "<h3> What is LIME? </h3>\n",
    "<p> LIME explains individual predictions by locally approximating the model with an interpretable one (like a linear model).</p>\n",
    "<h3> Its role in Feature Importance: </h3>\n",
    "<ul>\n",
    "    <li> Provides local feature importance for single predictions.</li>\n",
    "    <li> Useful for debugging or explaining black-box models.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://github.com/marcotcr/lime\" target=\"_blank\">LIME GitHub</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0a31b",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- model: A fitted model\n",
    "- num_samples: Number of synthetic samples for LIME explanation\n",
    "- instance_index: Row index of the instance to explain (single instance)\n",
    "- mode: \"classification\" or \"regression\" depending on the model type\n",
    "- show_plot: Whether to plot local feature contributions\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- feature_names: Custom feature names (optional)\n",
    "- class_names: Custom class names (for classification, optional)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of feature contributions for the explained instance\n",
    "- LIME explanation object\n",
    "- Displays a plot of local feature importance if show_plot=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86bfa2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lime\n",
      "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
      "     -------------------------------------- 275.7/275.7 kB 8.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lime) (3.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lime) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lime) (1.10.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lime) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lime) (1.6.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lime) (0.19.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2021.7.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (1.4.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.26.0)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (22.0)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.8.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->lime) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->lime) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->lime) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->lime) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->lime) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
      "Building wheels for collected packages: lime\n",
      "  Building wheel for lime (setup.py): started\n",
      "  Building wheel for lime (setup.py): finished with status 'done'\n",
      "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283846 sha256=819018af771353c10cff25bc32b045b2bf788600d1db1d9a734e1cc16fb64f3d\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\ac\\fc\\ba\\bc2e218408e730b7ad32dc45fbaa1ae6f0ab314e581101bdff\n",
      "Successfully built lime\n",
      "Installing collected packages: lime\n",
      "Successfully installed lime-0.2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7037268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def lime_feature_importance(X,\n",
    "                             model,\n",
    "                             num_samples=5000,\n",
    "                             instance_index=0,\n",
    "                             mode=\"regression\",\n",
    "                             show_plot=True,\n",
    "                             plot_size=(10, 6),\n",
    "                             feature_names=None,\n",
    "                             class_names=None,\n",
    "                             random_state=42):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    feature_names = feature_names if feature_names else X.columns.tolist()\n",
    "    X_values = X.values\n",
    "\n",
    "    # Define LIME explainer\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_values,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode=mode,\n",
    "        discretize_continuous=True,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Explain the specific instance\n",
    "    instance = X_values[instance_index].reshape(1, -1)\n",
    "    if mode == \"classification\":\n",
    "        explanation = explainer.explain_instance(instance.flatten(), model.predict_proba, num_samples=num_samples)\n",
    "    else:\n",
    "        explanation = explainer.explain_instance(instance.flatten(), model.predict, num_samples=num_samples)\n",
    "\n",
    "    # Extract feature contributions\n",
    "    contributions = dict(explanation.as_list())\n",
    "    contrib_df = pd.DataFrame({\n",
    "        'Feature': list(contributions.keys()),\n",
    "        'Contribution': list(contributions.values())\n",
    "    }).sort_values('Contribution', ascending=True)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.coolwarm(np.linspace(0.2, 0.8, len(contrib_df)))\n",
    "        bars = plt.barh(contrib_df['Feature'],\n",
    "                        contrib_df['Contribution'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "        plt.xlabel('Contribution to Prediction', fontsize=12)\n",
    "        plt.title(f'LIME Feature Contributions (Instance {instance_index})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + np.sign(width) * 0.01,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: LIME\\n\"\n",
    "            f\"Samples: {num_samples}\\n\"\n",
    "            f\"Mode: {mode}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return contrib_df, explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667a0ea9",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d817e20",
   "metadata": {},
   "source": [
    "## 3.5.7 Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a71dec",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Permutation Importance } </h2>\n",
    "</summary>\n",
    "<h3> What is Permutation Importance? </h3>\n",
    "<p> Measures the change in model performance after shuffling the values of a feature — breaking the relationship between the feature and the outcome.</p>\n",
    "<h3> Its role in Feature Importance: </h3>\n",
    "<ul>\n",
    "    <li> Model-agnostic and robust.</li>\n",
    "    <li> Measures a feature’s effect on actual performance metrics.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/permutation_importance.html\" target=\"_blank\">sklearn Permutation Importance</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c4a7b",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target values (Series or array)\n",
    "- model: A fitted model\n",
    "- scoring: Scoring function or string (e.g., 'r2', 'accuracy') for evaluation\n",
    "- n_repeats: Number of times to permute a feature\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot permutation feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of feature importances (mean and standard deviation of importance scores)\n",
    "- Displays a feature importance plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9b68007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def permutation_feature_importance(X,\n",
    "                                    y,\n",
    "                                    model,\n",
    "                                    scoring=None,\n",
    "                                    n_repeats=10,\n",
    "                                    random_state=42,\n",
    "                                    show_plot=True,\n",
    "                                    plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    # Compute permutation importance\n",
    "    result = permutation_importance(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        scoring=scoring,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance_Mean': result.importances_mean,\n",
    "        'Importance_Std': result.importances_std\n",
    "    }).sort_values('Importance_Mean', ascending=True)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 0.8, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(\n",
    "            importance_df['Feature'],\n",
    "            importance_df['Importance_Mean'],\n",
    "            xerr=importance_df['Importance_Std'],\n",
    "            color=colors,\n",
    "            alpha=0.9,\n",
    "            capsize=4\n",
    "        )\n",
    "\n",
    "        plt.xlabel('Mean Importance Score', fontsize=12)\n",
    "        plt.title('Permutation Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + np.sign(width) * 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Permutation Importance\\n\"\n",
    "            f\"Scoring: {scoring}\\n\"\n",
    "            f\"Repeats: {n_repeats}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf755cc",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bf5225",
   "metadata": {},
   "source": [
    "## 3.5.8 PDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad9286",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Partial Dependence Plots (PDP) } </h2>\n",
    "</summary>\n",
    "<h3> What is PDP? </h3>\n",
    "<p> PDPs show the marginal effect of a feature (or a pair) on the predicted outcome, averaging over the values of all other features.</p>\n",
    "<h3> Its role in Feature Interpretation: </h3>\n",
    "<ul>\n",
    "    <li> Not for selection, but helps visualize whether the relationship is linear, monotonic, etc.</li>\n",
    "    <li> Useful for understanding global feature effect.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://christophm.github.io/interpretable-ml-book/pdp.html\" target=\"_blank\">Interpretable ML Book - PDP</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c131d33",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- model: A fitted model\n",
    "- features: List of feature names (single feature or feature pairs for 2D PDP)\n",
    "- grid_resolution: Number of points to plot along feature axes\n",
    "- kind: 'average' (default) or 'individual' for ICE plots\n",
    "- show_plot: Whether to display the PDP plot\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- PDP results (as Bunch object)\n",
    "- Displays the PDP plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a667fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n",
    "\n",
    "def partial_dependence_plot(X,\n",
    "                             model,\n",
    "                             features,\n",
    "                             grid_resolution=100,\n",
    "                             kind='average',\n",
    "                             show_plot=True,\n",
    "                             plot_size=(12, 8)):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    # Calculate Partial Dependence\n",
    "    pdp_result = partial_dependence(\n",
    "        estimator=model,\n",
    "        X=X,\n",
    "        features=features,\n",
    "        grid_resolution=grid_resolution,\n",
    "        kind=kind\n",
    "    )\n",
    "\n",
    "    if show_plot:\n",
    "        fig, ax = plt.subplots(figsize=plot_size)\n",
    "        display = PartialDependenceDisplay.from_estimator(\n",
    "            estimator=model,\n",
    "            X=X,\n",
    "            features=features,\n",
    "            grid_resolution=grid_resolution,\n",
    "            kind=kind,\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        title_text = (\n",
    "            f\"Method: Partial Dependence Plot\\n\"\n",
    "            f\"Features: {features}\\n\"\n",
    "            f\"Kind: {kind}\"\n",
    "        )\n",
    "        plt.suptitle(title_text, fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return pdp_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a799fb",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c83e5",
   "metadata": {},
   "source": [
    "## 3.5.9 ReliefF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da3934",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding ReliefF Algorithm } </h2>\n",
    "</summary>\n",
    "<h3> What is ReliefF? </h3>\n",
    "<p> ReliefF evaluates the importance of features based on how well they differentiate between instances that are near each other.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Works well with noisy, multi-class, and non-linear data.</li>\n",
    "    <li> Ranks features by how well they separate similar/dissimilar instances.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://medium.datadriveninvestor.com/feature-selection-with-relieff-algorithm-96f8cd30c5e3\" target=\"_blank\">ReliefF Explained</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3a1532",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target values (Series or array)\n",
    "- n_neighbors: Number of neighbors to consider in ReliefF\n",
    "- n_features_to_select: Number of top features to keep (optional; if None, return all scores)\n",
    "- discrete_threshold: Threshold to treat features as discrete (optional)\n",
    "- show_plot: Whether to display feature importance plot\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of feature importances (sorted)\n",
    "- Displays a feature importance plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc29c6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skrebate\n",
      "  Downloading skrebate-0.62.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from skrebate) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from skrebate) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from skrebate) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->skrebate) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->skrebate) (3.5.0)\n",
      "Building wheels for collected packages: skrebate\n",
      "  Building wheel for skrebate (setup.py): started\n",
      "  Building wheel for skrebate (setup.py): finished with status 'done'\n",
      "  Created wheel for skrebate: filename=skrebate-0.62-py3-none-any.whl size=29266 sha256=53954fcb7d0e790d612cb4d7fa32d22560f1f3ada66e4b87787fa2bcf6e6185f\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\80\\cc\\df\\85c526cc1ab20b1421f3411ee8c2cdad3e6bb1320c846d6943\n",
      "Successfully built skrebate\n",
      "Installing collected packages: skrebate\n",
      "Successfully installed skrebate-0.62\n"
     ]
    }
   ],
   "source": [
    "!pip install skrebate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b996df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "from skrebate import ReliefF  # Requires `scikit-rebate` library\n",
    "\n",
    "def relieff_feature_importance(X,\n",
    "                                y,\n",
    "                                n_neighbors=100,\n",
    "                                n_features_to_select=None,\n",
    "                                discrete_threshold=None,\n",
    "                                show_plot=True,\n",
    "                                plot_size=(12, 8),\n",
    "                                random_state=42):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    relieff = ReliefF(\n",
    "        n_neighbors=n_neighbors,\n",
    "        discrete_threshold=discrete_threshold\n",
    "    )\n",
    "\n",
    "    relieff.fit(X.values, y)\n",
    "\n",
    "    importance_scores = relieff.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=True)\n",
    "\n",
    "    if n_features_to_select:\n",
    "        importance_df = importance_df.tail(n_features_to_select)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.cividis(np.linspace(0.2, 0.8, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(\n",
    "            importance_df['Feature'],\n",
    "            importance_df['Importance'],\n",
    "            color=colors,\n",
    "            alpha=0.9\n",
    "        )\n",
    "\n",
    "        plt.xlabel('ReliefF Importance Score', fontsize=12)\n",
    "        plt.title('ReliefF Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + np.sign(width) * 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: ReliefF\\n\"\n",
    "            f\"n_neighbors: {n_neighbors}\\n\"\n",
    "            f\"Selected Features: {n_features_to_select if n_features_to_select else 'All'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226a705f",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f106f",
   "metadata": {},
   "source": [
    "## 3.5.10 MRMR (Minimum Redundancy Maximum Relevance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dccb18",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding MRMR (Minimum Redundancy Maximum Relevance) } </h2>\n",
    "</summary>\n",
    "<h3> What is MRMR? </h3>\n",
    "<p> MRMR selects features that are highly relevant to the target and minimally redundant with each other.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Balances relevance and diversity among features.</li>\n",
    "    <li> Often used in high-dimensional datasets like genomics.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://elliot-weissberg.medium.com/another-feature-selection-algorithm-mrmr-3827b6b19e33\" target=\"_blank\">MRMR</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19146d93",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target values (Series or array)\n",
    "- n_features_to_select: Number of top features to select\n",
    "- method: Scoring method for relevance ('mi' for mutual information, 'f' for F-statistic, etc.)\n",
    "- discrete_features: Whether features are discrete (important for mutual information)\n",
    "- show_plot: Whether to display feature importance plot\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- List of selected feature names (ranked by MRMR)\n",
    "- DataFrame of feature scores (relevance, redundancy, combined score)\n",
    "- Displays a feature importance plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cfd708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, f_classif, f_regression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def mrmr_feature_selection(X,\n",
    "                            y,\n",
    "                            n_features_to_select=10,\n",
    "                            method='mi',\n",
    "                            discrete_features='auto',\n",
    "                            show_plot=True,\n",
    "                            plot_size=(12, 8),\n",
    "                            random_state=42):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    if method == 'mi':\n",
    "        if len(np.unique(y)) <= 10:\n",
    "            relevance = mutual_info_classif(X, y, discrete_features=discrete_features, random_state=random_state)\n",
    "        else:\n",
    "            relevance = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=random_state)\n",
    "    elif method == 'f':\n",
    "        if len(np.unique(y)) <= 10:\n",
    "            relevance, _ = f_classif(X, y)\n",
    "        else:\n",
    "            relevance, _ = f_regression(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'mi' or 'f'.\")\n",
    "\n",
    "    relevance_scores = pd.Series(relevance, index=X.columns)\n",
    "\n",
    "    selected_features = []\n",
    "    candidate_features = list(X.columns)\n",
    "\n",
    "    redundancy_matrix = X.corr().abs()\n",
    "\n",
    "    while len(selected_features) < n_features_to_select and candidate_features:\n",
    "        scores = {}\n",
    "        for feature in candidate_features:\n",
    "            if selected_features:\n",
    "                redundancy = redundancy_matrix.loc[feature, selected_features].mean()\n",
    "            else:\n",
    "                redundancy = 0\n",
    "\n",
    "            score = relevance_scores[feature] - redundancy\n",
    "            scores[feature] = score\n",
    "\n",
    "        best_feature = max(scores, key=scores.get)\n",
    "        selected_features.append(best_feature)\n",
    "        candidate_features.remove(best_feature)\n",
    "\n",
    "    final_scores = pd.DataFrame({\n",
    "        'Feature': selected_features,\n",
    "        'Relevance': relevance_scores[selected_features].values,\n",
    "        'Redundancy': [redundancy_matrix.loc[f, selected_features].mean() for f in selected_features],\n",
    "        'MRMR_Score': [relevance_scores[f] - redundancy_matrix.loc[f, selected_features].mean() for f in selected_features]\n",
    "    }).sort_values('MRMR_Score', ascending=True)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.plasma(np.linspace(0.3, 0.9, len(final_scores)))\n",
    "\n",
    "        bars = plt.barh(\n",
    "            final_scores['Feature'],\n",
    "            final_scores['MRMR_Score'],\n",
    "            color=colors,\n",
    "            alpha=0.9\n",
    "        )\n",
    "\n",
    "        plt.xlabel('MRMR Score (Relevance - Redundancy)', fontsize=12)\n",
    "        plt.title('MRMR Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + np.sign(width) * 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: MRMR ({method.upper()})\\n\"\n",
    "            f\"Features Selected: {n_features_to_select}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return selected_features, final_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d082b2",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7cc32",
   "metadata": {},
   "source": [
    "## 3.5.11 Leave-One-Covariate-Out (LOCO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890b4f46",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding LOCO (Leave-One-Covariate-Out) } </h2>\n",
    "</summary>\n",
    "<h3> What is LOCO? </h3>\n",
    "<p> LOCO measures a feature's importance by training a model without it and checking how much the performance drops.</p>\n",
    "<h3> Its role in Feature Importance: </h3>\n",
    "<ul>\n",
    "    <li> Similar to drop-column but framed statistically.</li>\n",
    "    <li> Useful for creating confidence intervals for importance.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://arxiv.org/abs/1806.03827\" target=\"_blank\">LOCO Paper</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7984cb80",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A fitted predictive model (must implement predict or predict_proba)\n",
    "- X: Features (DataFrame)\n",
    "- y: Target values (Series or array)\n",
    "- scoring: Scoring function (e.g., accuracy_score, r2_score) depending on task\n",
    "- task_type: 'classification' or 'regression'\n",
    "- higher_is_better: Whether a higher score indicates better model performance\n",
    "- show_plot: Whether to plot LOCO feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- sample_weight: Optional sample weights for scoring\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame with feature importance scores (performance drop after removing feature)\n",
    "- Displays LOCO feature importance plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92c84c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import check_random_state\n",
    "from copy import deepcopy\n",
    "\n",
    "def loco_feature_importance(model,\n",
    "                             X,\n",
    "                             y,\n",
    "                             scoring,\n",
    "                             task_type='classification',\n",
    "                             higher_is_better=True,\n",
    "                             show_plot=True,\n",
    "                             plot_size=(12, 8),\n",
    "                             sample_weight=None,\n",
    "                             random_state=42):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "\n",
    "    model = deepcopy(model)\n",
    "\n",
    "    # Baseline Score (full feature set)\n",
    "    if task_type == 'classification' and hasattr(model, \"predict_proba\"):\n",
    "        y_pred = model.predict_proba(X)\n",
    "        y_true = y\n",
    "    else:\n",
    "        y_pred = model.predict(X)\n",
    "        y_true = y\n",
    "\n",
    "    baseline_score = scoring(y_true, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "    importance_dict = {}\n",
    "\n",
    "    for feature in X.columns:\n",
    "        X_temp = X.drop(columns=[feature])\n",
    "\n",
    "        try:\n",
    "            y_pred_temp = model.predict(X_temp) if task_type == 'regression' else model.predict_proba(X_temp)\n",
    "            score_temp = scoring(y_true, y_pred_temp, sample_weight=sample_weight)\n",
    "        except Exception:\n",
    "            # Model might depend on fixed input shapes; skip feature if incompatible\n",
    "            score_temp = np.nan\n",
    "\n",
    "        importance = (baseline_score - score_temp) if higher_is_better else (score_temp - baseline_score)\n",
    "        importance_dict[feature] = importance\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': list(importance_dict.keys()),\n",
    "        'LOCO_Importance': list(importance_dict.values())\n",
    "    }).sort_values('LOCO_Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.inferno(np.linspace(0.3, 0.9, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(\n",
    "            importance_df['Feature'],\n",
    "            importance_df['LOCO_Importance'],\n",
    "            color=colors,\n",
    "            alpha=0.9\n",
    "        )\n",
    "\n",
    "        plt.xlabel('Decrease in Score After Removing Feature', fontsize=12)\n",
    "        plt.title('LOCO Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + np.sign(width) * 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Leave-One-Covariate-Out (LOCO)\\n\"\n",
    "            f\"Task: {task_type.title()}\\n\"\n",
    "            f\"Higher Score Better: {higher_is_better}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691caac9",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e305c",
   "metadata": {},
   "source": [
    "## 3.5.12 Forward/Backward Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3359a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Forward and Backward Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is Forward/Backward Selection? </h3>\n",
    "<p> Forward Selection starts with no features and adds the best one at each step. Backward starts with all and removes the least useful.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Simple and greedy heuristic for selecting a subset of features.</li>\n",
    "    <li> Often used with cross-validation for scoring.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/whats_new/v0.24.html#id13\" target=\"_blank\">sklearn Sequential Feature Selector</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb132825",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A fitted predictive model (must implement fit and predict/predict_proba)\n",
    "- X: Features (DataFrame)\n",
    "- y: Target values (Series or array)\n",
    "- direction: 'forward' or 'backward' selection\n",
    "- scoring: Scoring function (e.g., accuracy_score, r2_score) depending on task\n",
    "- n_features_to_select: Number of features to select (optional, defaults to half)\n",
    "- task_type: 'classification' or 'regression'\n",
    "- higher_is_better: Whether a higher score indicates better model performance\n",
    "- show_plot: Whether to plot feature selection process\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- sample_weight: Optional sample weights for scoring\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- List of selected feature names\n",
    "- DataFrame of feature selection progression (feature, score)\n",
    "- Displays the feature selection plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68e012a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "def sequential_feature_selection(model,\n",
    "                                  X,\n",
    "                                  y,\n",
    "                                  direction='forward',\n",
    "                                  scoring=None,\n",
    "                                  n_features_to_select=None,\n",
    "                                  task_type='classification',\n",
    "                                  higher_is_better=True,\n",
    "                                  show_plot=True,\n",
    "                                  plot_size=(12, 8),\n",
    "                                  sample_weight=None,\n",
    "                                  random_state=42):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    random_state = check_random_state(random_state)\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "\n",
    "    all_features = list(X.columns)\n",
    "    selected_features = [] if direction == 'forward' else all_features.copy()\n",
    "    remaining_features = all_features.copy()\n",
    "\n",
    "    if n_features_to_select is None:\n",
    "        n_features_to_select = max(1, int(len(all_features) / 2))\n",
    "\n",
    "    selection_progress = []\n",
    "\n",
    "    while (len(selected_features) < n_features_to_select) if direction == 'forward' else (len(selected_features) > n_features_to_select):\n",
    "        best_score = -np.inf if higher_is_better else np.inf\n",
    "        best_feature = None\n",
    "\n",
    "        search_features = remaining_features if direction == 'forward' else selected_features\n",
    "\n",
    "        for feature in search_features:\n",
    "            if direction == 'forward':\n",
    "                trial_features = selected_features + [feature]\n",
    "            else:\n",
    "                trial_features = [f for f in selected_features if f != feature]\n",
    "\n",
    "            X_trial = X[trial_features]\n",
    "\n",
    "            temp_model = clone(model)\n",
    "            temp_model.fit(X_trial, y, sample_weight=sample_weight)\n",
    "            \n",
    "            if task_type == 'classification' and hasattr(temp_model, \"predict_proba\"):\n",
    "                y_pred = temp_model.predict_proba(X_trial)\n",
    "            else:\n",
    "                y_pred = temp_model.predict(X_trial)\n",
    "\n",
    "            trial_score = scoring(y, y_pred, sample_weight=sample_weight)\n",
    "\n",
    "            is_better = (trial_score > best_score) if higher_is_better else (trial_score < best_score)\n",
    "\n",
    "            if is_better:\n",
    "                best_score = trial_score\n",
    "                best_feature = feature\n",
    "\n",
    "        if best_feature is not None:\n",
    "            if direction == 'forward':\n",
    "                selected_features.append(best_feature)\n",
    "                remaining_features.remove(best_feature)\n",
    "            else:\n",
    "                selected_features.remove(best_feature)\n",
    "\n",
    "            selection_progress.append((best_feature, best_score))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    progression_df = pd.DataFrame(selection_progress, columns=['Feature', 'Score'])\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        plt.plot(range(1, len(progression_df) + 1),\n",
    "                 progression_df['Score'],\n",
    "                 marker='o', linestyle='-',\n",
    "                 color='teal')\n",
    "        plt.xticks(range(1, len(progression_df) + 1), progression_df['Feature'], rotation=90)\n",
    "        plt.xlabel('Features Selected', fontsize=12)\n",
    "        plt.ylabel('Score', fontsize=12)\n",
    "        plt.title(f\"Sequential Feature Selection ({direction.title()})\", fontsize=14, pad=20)\n",
    "        plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Sequential Feature Selection\\n\"\n",
    "            f\"Direction: {direction.title()}\\n\"\n",
    "            f\"Selected Features: {len(selected_features)}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return selected_features, progression_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a5788",
   "metadata": {},
   "source": [
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

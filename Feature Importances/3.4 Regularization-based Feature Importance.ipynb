{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850266ae",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click the markdown to see the code instead of this </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e0cf51",
   "metadata": {},
   "source": [
    "# 3.4 Regularization-based Feature Importance\n",
    "-- Penalty-induced sparsity methods\n",
    "\n",
    "- Lasso Regression (L1 penalty)\n",
    "- Ridge Regression (L2 - not for selection but for shrinkage)\n",
    "- Elastic Net\n",
    "- Group Lasso (if features are grouped)\n",
    "- LARS (Least Angle Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b372fe",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f06c4",
   "metadata": {},
   "source": [
    "## 3.4.1 Lasso Regression (L1 penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b729499",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Lasso Regression (L1) for Feature Selection } </h2>\n",
    "</summary>\n",
    "<h3> What is Lasso Regression? </h3>\n",
    "<p> Lasso (Least Absolute Shrinkage and Selection Operator) is a linear regression technique with L1 regularization that adds a penalty equal to the absolute value of the magnitude of coefficients.</p>\n",
    "<h3> It's role in Feature Importance / Selection: </h3>\n",
    "<ul>\n",
    "    <li> L1 penalty forces some coefficients to exactly zeroâ€”hence performing feature selection.</li>\n",
    "    <li> Effective when there are many irrelevant features.</li>\n",
    "    <li> Helps reduce overfitting and interpret models better.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=NGf0voTMlcs\" target=\"_blank\">Lasso Regression Explained</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a24122",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- alpha: Regularization strength (higher = more shrinkage, default=0.01)\n",
    "- scale_data: Whether to standardize features before fitting\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- Fitted Lasso model (for inspection or reuse)\n",
    "- DataFrame of selected features with their corresponding coefficients\n",
    "- Displays a plot of feature importance (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7da240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "def lasso_feature_importance(X,\n",
    "                              y,\n",
    "                              alpha=0.01,\n",
    "                              scale_data=True,\n",
    "                              random_state=42,\n",
    "                              show_plot=True,\n",
    "                              plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    # Fit Lasso model\n",
    "    lasso = Lasso(alpha=alpha, random_state=random_state, max_iter=10000)\n",
    "    lasso.fit(X_processed, y)\n",
    "\n",
    "    coefs = lasso.coef_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefs,\n",
    "        'Absolute_Coefficient': np.abs(coefs)\n",
    "    }).sort_values('Absolute_Coefficient', ascending=False)\n",
    "\n",
    "    selected_features_df = importance_df[importance_df['Absolute_Coefficient'] > 0].copy()\n",
    "\n",
    "    if show_plot and not selected_features_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        selected_features_sorted = selected_features_df.sort_values('Absolute_Coefficient', ascending=True)\n",
    "\n",
    "        colors = plt.cm.inferno(np.linspace(0.2, 1, len(selected_features_sorted)))\n",
    "        bars = plt.barh(selected_features_sorted['Feature'],\n",
    "                        selected_features_sorted['Absolute_Coefficient'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Absolute Lasso Coefficient', fontsize=12)\n",
    "        plt.title('LASSO Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.001,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: LASSO (L1) Regression\\n\"\n",
    "            f\"Alpha: {alpha}\\n\"\n",
    "            f\"Scale Data: {scale_data}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return lasso, selected_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be074d4",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de59a3",
   "metadata": {},
   "source": [
    "## 3.4.2 Ridge Regression (L2-not for selection but for shrinkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce83e83",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Ridge Regression (L2 Penalty) } </h2>\n",
    "</summary>\n",
    "<h3> What is Ridge Regression? </h3>\n",
    "<p> Ridge regression is a linear model that adds an L2 penalty (squared magnitude of coefficients) to reduce overfitting.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Does not perform feature selection, but shrinks all coefficients to reduce model complexity.</li>\n",
    "    <li> Useful for multicollinearity and improving generalization.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=Q81RR3yKn30\" target=\"_blank\">Ridge Regression (StatQuest)</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ad10d1",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- alpha: Regularization strength (default=1.0; higher = more shrinkage)\n",
    "- scale_data: Whether to standardize features before fitting\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- Fitted Ridge model (for inspection or reuse)\n",
    "- DataFrame of features with their corresponding coefficients\n",
    "- Displays a plot of feature importance (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8301777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "def ridge_feature_importance(X,\n",
    "                              y,\n",
    "                              alpha=1.0,\n",
    "                              scale_data=True,\n",
    "                              random_state=42,\n",
    "                              show_plot=True,\n",
    "                              plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    # Fit Ridge model\n",
    "    ridge = Ridge(alpha=alpha, random_state=random_state, max_iter=10000)\n",
    "    ridge.fit(X_processed, y)\n",
    "\n",
    "    coefs = ridge.coef_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefs,\n",
    "        'Absolute_Coefficient': np.abs(coefs)\n",
    "    }).sort_values('Absolute_Coefficient', ascending=False)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Absolute_Coefficient', ascending=True)\n",
    "\n",
    "        colors = plt.cm.cividis(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Absolute_Coefficient'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Absolute Ridge Coefficient', fontsize=12)\n",
    "        plt.title('Ridge Feature Importance (Shrinkage View)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.001,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Ridge (L2) Regression\\n\"\n",
    "            f\"Alpha: {alpha}\\n\"\n",
    "            f\"Scale Data: {scale_data}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return ridge, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b8b38",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dcc456",
   "metadata": {},
   "source": [
    "## 3.4.3 Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5687b460",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Elastic Net Regression } </h2>\n",
    "</summary>\n",
    "<h3> What is Elastic Net? </h3>\n",
    "<p> Elastic Net is a linear regression that combines L1 and L2 penalties, allowing for both variable selection and regularization.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Overcomes limitations of Lasso when features are correlated.</li>\n",
    "    <li> Selects groups of related variables and improves prediction accuracy.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=2IAfQdAPdLY\" target=\"_blank\">Elastic Net Explanation</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885ad9bd",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- alpha: Overall regularization strength (default=1.0)\n",
    "- l1_ratio: Mix ratio (0 = pure Ridge, 1 = pure Lasso; default=0.5)\n",
    "- scale_data: Whether to standardize features before fitting\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- Fitted ElasticNet model (for inspection or reuse)\n",
    "- DataFrame of features with their corresponding coefficients\n",
    "- Displays a plot of feature importance (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0010378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "def elasticnet_feature_importance(X,\n",
    "                                   y,\n",
    "                                   alpha=1.0,\n",
    "                                   l1_ratio=0.5,\n",
    "                                   scale_data=True,\n",
    "                                   random_state=42,\n",
    "                                   show_plot=True,\n",
    "                                   plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    # Fit ElasticNet model\n",
    "    elastic = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=random_state, max_iter=10000)\n",
    "    elastic.fit(X_processed, y)\n",
    "\n",
    "    coefs = elastic.coef_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Coefficient': coefs,\n",
    "        'Absolute_Coefficient': np.abs(coefs)\n",
    "    }).sort_values('Absolute_Coefficient', ascending=False)\n",
    "\n",
    "    selected_features_df = importance_df[importance_df['Absolute_Coefficient'] > 0].copy()\n",
    "\n",
    "    if show_plot and not selected_features_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        selected_features_sorted = selected_features_df.sort_values('Absolute_Coefficient', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(selected_features_sorted)))\n",
    "        bars = plt.barh(selected_features_sorted['Feature'],\n",
    "                        selected_features_sorted['Absolute_Coefficient'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Absolute ElasticNet Coefficient', fontsize=12)\n",
    "        plt.title('ElasticNet Feature Importance (Shrinkage + Selection)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.001,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: ElasticNet Regression\\n\"\n",
    "            f\"Alpha: {alpha} | L1 Ratio: {l1_ratio}\\n\"\n",
    "            f\"Scale Data: {scale_data}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return elastic, selected_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e33d07",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f851a8f",
   "metadata": {},
   "source": [
    "## 3.4.4 Group Lasso (if features are grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b39788d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Group Lasso } </h2>\n",
    "</summary>\n",
    "<h3> What is Group Lasso? </h3>\n",
    "<p> Group Lasso extends Lasso by allowing selection of entire groups of features, rather than individual ones.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Useful when features are naturally grouped (e.g., polynomial terms or embeddings).</li>\n",
    "    <li> Selects or discards entire groups, promoting interpretability.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://statisticseasily.com/glossario/what-is-group-lasso-a-comprehensive-guide/\" target=\"_blank\">Group Lasso Article</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d7aad",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- groups: List/array defining group memberships (same length as number of features)\n",
    "- group_reg: Regularization strength for group penalty (default=0.05)\n",
    "- l1_reg: Regularization strength for individual features inside groups (default=0.01)\n",
    "- scale_data: Whether to standardize features before fitting\n",
    "- max_iter: Maximum number of iterations\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- Fitted GroupLasso model (for inspection or reuse)\n",
    "- DataFrame of group-level feature importances\n",
    "- Displays a plot of group importance (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7710ff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting group_lasso\n",
      "  Downloading group_lasso-1.5.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from group_lasso) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from group_lasso) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->group_lasso) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->group_lasso) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->group_lasso) (1.10.0)\n",
      "Installing collected packages: group_lasso\n",
      "Successfully installed group_lasso-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install group_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e5d961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from group_lasso import GroupLasso\n",
    "\n",
    "def group_lasso_feature_importance(X,\n",
    "                                    y,\n",
    "                                    groups,\n",
    "                                    group_reg=0.05,\n",
    "                                    l1_reg=0.01,\n",
    "                                    scale_data=True,\n",
    "                                    random_state=42,\n",
    "                                    max_iter=1000,\n",
    "                                    show_plot=True,\n",
    "                                    plot_size=(12, 8)):\n",
    "    \n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    groups_array = np.array(groups)\n",
    "\n",
    "    # Fit Group Lasso\n",
    "    model = GroupLasso(groups=groups_array,\n",
    "                       group_reg=group_reg,\n",
    "                       l1_reg=l1_reg,\n",
    "                       n_iter=max_iter,\n",
    "                       scale_reg='group_size',\n",
    "                       random_state=random_state,\n",
    "                       supress_warning=True)\n",
    "    model.fit(X_processed, y)\n",
    "\n",
    "    coefs = model.coef_\n",
    "\n",
    "    group_importances = {}\n",
    "    for group_id in np.unique(groups_array):\n",
    "        if group_id == -1:\n",
    "            continue  # Ignore unassigned features\n",
    "        group_indices = np.where(groups_array == group_id)[0]\n",
    "        group_coef_norm = np.linalg.norm(coefs[group_indices], ord=2)  # L2 norm of group coefficients\n",
    "        group_importances[group_id] = group_coef_norm\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Group': list(group_importances.keys()),\n",
    "        'Importance': list(group_importances.values())\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.inferno(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Group'].astype(str),\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Group Coefficient L2 Norm', fontsize=12)\n",
    "        plt.title('Group Lasso Feature Group Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.001,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Group Lasso\\n\"\n",
    "            f\"Group Reg: {group_reg} | L1 Reg: {l1_reg}\\n\"\n",
    "            f\"Scale Data: {scale_data}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return model, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab22d48",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35ba867",
   "metadata": {},
   "source": [
    "## 3.4.5 LARS (Least Angle Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade20f46",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Least Angle Regression (LARS) } </h2>\n",
    "</summary>\n",
    "<h3> What is LARS? </h3>\n",
    "<p> LARS is a regression algorithm particularly useful when the number of features is greater than the number of samples.</p>\n",
    "<h3> Its role in Feature Selection: </h3>\n",
    "<ul>\n",
    "    <li> Efficiently finds the most influential features in a stepwise fashion.</li>\n",
    "    <li> Closely related to Lasso and can approximate Lasso paths.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression\" target=\"_blank\">scikit-learn LARS Docs</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db2f17d",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- X: Features (DataFrame)\n",
    "- y: Target variable (array-like)\n",
    "- normalize: Whether to standardize features before fitting\n",
    "- n_nonzero_coefs: Target number of non-zero coefficients (optional, controls sparsity)\n",
    "- random_state: Random seed for reproducibility\n",
    "- show_plot: Whether to plot feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "\n",
    "##### Returns:\n",
    "- Fitted LARS model\n",
    "- DataFrame of feature importances (absolute coefficient values)\n",
    "- Displays a plot of feature importances if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "621d7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lars\n",
    "\n",
    "def lars_feature_importance(X,\n",
    "                             y,\n",
    "                             normalize=True,\n",
    "                             n_nonzero_coefs=None,\n",
    "                             random_state=42,\n",
    "                             show_plot=True,\n",
    "                             plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    feature_names = X.columns.tolist()\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "    else:\n",
    "        X_processed = X_processed.values\n",
    "\n",
    "    # Fit LARS\n",
    "    model = Lars(n_nonzero_coefs=n_nonzero_coefs)\n",
    "    model.fit(X_processed, y)\n",
    "\n",
    "    coefs = model.coef_\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(coefs)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if show_plot and not importance_df.empty:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Absolute Coefficient Value', fontsize=12)\n",
    "        plt.title('LARS Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: LARS Regression\\n\"\n",
    "            f\"Normalize: {normalize}\\n\"\n",
    "            f\"Target Non-Zero Coefs: {n_nonzero_coefs}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return model, importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3737a26",
   "metadata": {},
   "source": [
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

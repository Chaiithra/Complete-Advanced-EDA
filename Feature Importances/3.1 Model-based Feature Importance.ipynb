{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88a05b4",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click this markdown to see the code instead of this </b>\n",
    "</summary>\n",
    "    <p> Collapsable content is marked with {} </p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8740b8",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0818474",
   "metadata": {},
   "source": [
    "# 3.1 Model-based Feature Importance\n",
    "-- Importance score derived from model internals (weights, impurity, gain, etc.)\n",
    "- Tree-based models (Decision Tree/ Random Forest/ Extra Trees Importance)\n",
    "-  XGBoost/LightGBM/CatBoost Feature Importance\n",
    "- Logistic regression (coefficients)\n",
    "- SVM (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb6e8f3",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f065f",
   "metadata": {},
   "source": [
    "## 3.1.1 Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eceb2c",
   "metadata": {},
   "source": [
    "#### Best used when:\n",
    "\n",
    "-  Your data has nonlinear relationships and interactions between variables.\n",
    "- You have mixed data types (categorical + numerical).\n",
    "- You're looking for quick, intuitive feature rankings.\n",
    "- Not ideal for high cardinality categorical features without proper encoding (e.g., too many labels in a single feature).\n",
    "- May be biased toward features with more levels or higher variance.\n",
    "\n",
    "#### Use for:\n",
    "\n",
    "- Feature ranking.\n",
    "- Pre-filtering before final model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06022cc8",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c77695",
   "metadata": {},
   "source": [
    "### 3.1.1.1 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116d880",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Decision Trees and it's role in Feature Importance } </h2>\n",
    "    </summary>\n",
    "    <h3> What are Decision trees? </h3>\n",
    "    <p> Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by recursively splitting the dataset inyo subsets based on attribute values.</p>\n",
    "    <h3> It's role in Feature Importance:</h3>\n",
    "    <p> Decision trees evaluate feature importance by assessing how much each feature contributes to the reduction of impurity or disorder in the dataset.</p>\n",
    "    <ul>\n",
    "        <li> This is often measured using metrics like Gini impurity or entropy </li>\n",
    "        <li> Feature importance scores can be derived from the decision tree by summing up the weighted impurity reductions for each feature across all the nodes where that feature is used. </li>\n",
    "        <li> They choose the most informative features for splitting.</li>\n",
    "    </ul>\n",
    "    <h3> Resources: </h3>\n",
    "    <ol>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=7VeUPuFGJHk\" target=\"_blank\">Decision Trees Explained (StatQuest)</a></li>\n",
    "  <li><a href=\"https://www.youtube.com/watch?v=ZVR2Way4nwQ\" target=\"_blank\">Feature Importance in Decision Trees (StatQuest)</a></li>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2b211",
   "metadata": {},
   "source": [
    "#### Decision Tree-specific feature importance with unique tree parameters.\n",
    "    \n",
    "##### Parameters:\n",
    "    \n",
    "- max_depth: Maximum tree depth (None for unlimited)\n",
    "- min_samples_split: Minimum samples required to split a node\n",
    "- min_samples_leaf: Minimum samples required at each leaf node\n",
    "- criterion: 'gini' or 'entropy' (classification), 'mse' or 'friedman_mse' (regression)\n",
    "- plot_tree_visualization: Whether to plot the full decision tree\n",
    "    \n",
    "##### Returns:\n",
    "\n",
    "- DataFrame with feature importances\n",
    "- Optionally displays the decision tree structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2d8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.tree import plot_tree  # For tree visualization\n",
    "\n",
    "def decision_tree_feature_importance(x, y, task_type='classification',\n",
    "                                    max_depth=None, min_samples_split=2,\n",
    "                                    min_samples_leaf=1, criterion='gini',\n",
    "                                    random_state=None, figsize=(15, 8),\n",
    "                                    plot_tree_visualization=False):\n",
    "    \n",
    "    # Convert x to DataFrame if it isn't already\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        x = pd.DataFrame(X)\n",
    "    \n",
    "    # Initialize model with decision tree-specific parameters\n",
    "    if task_type == 'classification':\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            criterion=criterion,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif task_type == 'regression':\n",
    "        model = DecisionTreeRegressor(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            criterion=criterion,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be 'classification' or 'regression'\")\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(x, y)\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance = model.feature_importances_\n",
    "    feature_names = x.columns\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(figsize[0], figsize[1]//2))\n",
    "    bars = plt.barh(importance_df['Feature'], importance_df['Importance'], color='darkcyan')\n",
    "    plt.xlabel('Mean Decrease in Impurity')\n",
    "    plt.title('Decision Tree Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # Add importance values on bars   # bars contain all the rectangle objects representing the bars\n",
    "    for bar in bars:                  # Loops through each bar in the horizontal bar plot\n",
    "                                      # Gets the width of the current bar i.e the feature importance score\n",
    "        width = bar.get_width()       # For horizontal bars, width = the x-axis value\n",
    "        plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.4f}',\n",
    "                va='center')\n",
    "        # width+0.01 : places text just to the right of the bar\n",
    "        # bar.get_y() + bar.get_height()/2: \n",
    "        # - Gets the bottom position of the bar(where the bar touches y/x axis)\n",
    "        # Texts starts at the position (base+h/2)\n",
    "        # f'{width:.4f}': Feature Importance value has 4 decimal places\n",
    "        # va: vertical alignment: ensures numbers stay aligned with the middlle of each bar\n",
    "    \n",
    "    # plt.savefig(\"plot.png\", bbox_inches=\"tight\")  # Alternative for saving\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Optional tree visualization\n",
    "    if plot_tree_visualization:\n",
    "        plt.figure(figsize=figsize)\n",
    "        plot_tree(model, \n",
    "                 feature_names=feature_names,  \n",
    "                 filled=True, \n",
    "                 rounded=True,\n",
    "                 proportion=True,\n",
    "                 impurity=True,\n",
    "                 class_names=model.classes_ if task_type == 'classification' else None)\n",
    "        plt.title('Decision Tree Structure')\n",
    "        plt.show()\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33e0a2",
   "metadata": {},
   "source": [
    "### 3.1.1.2 Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54892f3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Random Forests and it's role in Feature Importance } </h2>\n",
    "</summary>\n",
    "    <h3> What are Random Forests? </h3>\n",
    "    <p> Random Forests are a type of supervised learning algorithm used for both classification and regression tasks. Random forest produces multiple decision trees, randomly choosing features to make decisions when splitting nodes to create each tree. It then takes these randomized observations from each tree and averages them out to build a final model.</p>\n",
    "    <h3> It's role in Feature Importance:</h3>\n",
    "    <p>Random Forests excel at feature selection by providing robust importance rankings that can help identify the most relevant variables in a dataset.</p>\n",
    "    <ul>\n",
    "        <li>They calculate feature importance by averaging the importance scores across all trees in the forest, making the rankings more stable than single decision trees.</li>\n",
    "        <li>Random Forests use Out-of-Bag (OOB) samples to compute permutation importance, which measures how much prediction error increases when a feature is randomly shuffled.</li>\n",
    "        <li>For a dataset with many features, Random Forests can identify the top variables that contribute most to predictive power, allowing you to reduce dimensionality while preserving model performance.</li>\n",
    "        <li>They naturally handle feature interactions, making them effective at identifying variables that may not be important individually but are valuable in combination with others.</li>\n",
    "        <li>When working with high-dimensional data, you can use Random Forest importance scores to create a reduced feature set before training your final model.</li>\n",
    "    </ul>\n",
    "    <h3> Resources: </h3>\n",
    "    <ol>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\" target=\"_blank\">Random Forest Explained (StatQuest)</a></li>\n",
    "  <li><a href=\"https://www.youtube.com/watch?v=XLFeN8GfD74\" target=\"_blank\">Feature Importance in Random Forests (StatQuest)</a></li>\n",
    "  <li><a href=\"https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\" target=\"_blank\">Scikit-learn: Feature Importance using Random Forest</a></li>\n",
    "  <li><a href=\"https://towardsdatascience.com/random-forest-feature-importance-how-to-interpret-it-and-why-it-matters-cc6c2f4a9f42\" target=\"_blank\">Random Forest Feature Importance – How to Interpret It (TDS article)</a></li>\n",
    "  <li><a href=\"https://explained.ai/rf-importance/index.html\" target=\"_blank\">Explained.ai – Visual Introduction to Random Forest Feature Importance</a></li>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd66d62",
   "metadata": {},
   "source": [
    "#### Random Forest-specific feature importance with key RF hyperparameters.\n",
    "    \n",
    "##### Parameters:\n",
    "\n",
    "- n_estimators: Number of trees in the forest\n",
    "- max_depth: Maximum tree depth\n",
    "- min_samples_split: Minimum samples to split a node\n",
    "- min_samples_leaf: Minimum samples at leaf nodes\n",
    "- max_features: Features considered at each split ('auto', 'sqrt', log2', or int/float)\n",
    "- bootstrap: Whether bootstrap samples are used\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame with feature importances\n",
    "- Displays a styled importance plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fed9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def random_forest_feature_importance(X, y, task_type='classification',\n",
    "                                    n_estimators=100, max_depth=None,\n",
    "                                    min_samples_split=2, min_samples_leaf=1,\n",
    "                                    max_features='auto', bootstrap=True,\n",
    "                                    random_state=42, top_n_features=20,\n",
    "                                    show_plot=True, plot_size=(12, 8)):\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, x_test, y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    # Initialize Random Forest with RF-specific parameters\n",
    "    if task_type == 'classification':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1  # Use all cores\n",
    "        )\n",
    "    elif task_type == 'regression':\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be 'classification' or 'regression'\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get importance scores\n",
    "    importances = model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances,\n",
    "        'Std_Deviation': std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Select top N features\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "    \n",
    "    # Enhanced visualization\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=True)\n",
    "        \n",
    "        # Create colored bars (gradient from low to high importance)\n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(importance_df)))\n",
    "        \n",
    "        bars = plt.barh(importance_df['Feature'], \n",
    "                       importance_df['Importance'], \n",
    "                       color=colors,\n",
    "                       xerr=importance_df['Std_Deviation'],\n",
    "                       capsize=3)\n",
    "        \n",
    "        # Style the plot\n",
    "        plt.xlabel('Mean Decrease in Impurity (MDI)', fontsize=12)\n",
    "        plt.title('Random Forest Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add importance values on bars\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005, \n",
    "                    bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.4f}',\n",
    "                    va='center',\n",
    "                    fontsize=10)\n",
    "        \n",
    "        # Add RF hyperparameters as annotation\n",
    "        params_text = (\n",
    "            f\"RF Hyperparameters:\\n\"\n",
    "            f\"n_estimators={n_estimators}, max_features={max_features}\\n\"\n",
    "            f\"max_depth={max_depth}, min_samples_split={min_samples_split}\"\n",
    "        )\n",
    "        plt.annotate(params_text,\n",
    "                    xy=(0.98, 0.02),\n",
    "                    xycoords='axes fraction',\n",
    "                    ha='right',\n",
    "                    va='bottom',\n",
    "                    bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2487730c",
   "metadata": {},
   "source": [
    "### 3.1.1.3 Extra Trees (Extremely Randomized Trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02391983",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2>{ Understanding Extra Trees and their role in Feature Importance }</h2>\n",
    "</summary>\n",
    "    <h3>What are Extra Trees?</h3>\n",
    "    <p>Extra Trees (Extremely Randomized Trees) are an ensemble learning method that builds upon the Random Forest concept but introduces additional randomization. Unlike Random Forests, Extra Trees randomly select splitting points for each feature rather than searching for the optimal split, and they often use the entire original dataset instead of bootstrap samples.</p>\n",
    "    <h3>Its role in Feature Importance:</h3>\n",
    "    <p>Extra Trees provide a unique approach to feature selection by introducing more randomization, which can help identify consistently important features in noisy datasets.</p>\n",
    "    <ul>\n",
    "        <li>They calculate feature importance through Mean Decrease in Impurity (MDI), averaging importance scores across all trees with less bias towards high-cardinality features compared to standard decision trees.</li>\n",
    "        <li>Due to their extreme randomization in splitting, features that consistently perform well across many random splits are truly informative, making the importance rankings more robust against overfitting.</li>\n",
    "        <li>Unlike Random Forests, Extra Trees typically use bootstrap=False by default, which means they use the entire dataset for each tree, potentially capturing more subtle feature relationships.</li>\n",
    "        <li>They're particularly effective at feature selection when dealing with datasets containing many correlated features, as the randomization helps reduce the preference for any particular feature among correlated groups.</li>\n",
    "        <li>The standard deviation of feature importance across trees provides valuable insight into the stability of feature rankings, helping identify which features are consistently important across model variations.</li>\n",
    "    </ul>\n",
    "    <h3> Resources: </h3>\n",
    "    <ol>\n",
    "        <li><a href=\"https://people.montefiore.uliege.be/ernst/uploads/news/id63/extremely-randomized-trees.pdf\" target=\"_blank\">Extremely Randomized Trees Paper (PDF)</a></li>\n",
    "<li><a href=\"https://www.youtube.com/watch?v=Fh4aqbdfFvs\" target=\"_blank\">Extremely Randomized Trees Explained (YouTube)</a></li>\n",
    "    </ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b05356",
   "metadata": {},
   "source": [
    "#### Extra Trees-specific feature importance with unique ET parameters.\n",
    "    \n",
    "##### Parameters:\n",
    "\n",
    "- n_estimators: Number of trees\n",
    "- max_depth: Maximum tree depth\n",
    "- min_samples_split: Minimum samples to split a node\n",
    "- min_samples_leaf: Minimum samples at leaf nodes\n",
    "- max_features: Features considered at each split ('auto', 'sqrt', etc.)\n",
    "- bootstrap: Whether bootstrap samples are used (typically False for Extra Trees)\n",
    "\n",
    "##### Returns:\n",
    "\n",
    "- DataFrame with feature importances and variability\n",
    "- Displays a styled importance plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35968e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def extra_trees_feature_importance(X, y, task_type='classification',\n",
    "                                  n_estimators=100, max_depth=None,\n",
    "                                  min_samples_split=2, min_samples_leaf=1,\n",
    "                                  max_features='auto', bootstrap=False,  # Key difference: often bootstrap=False\n",
    "                                  random_state=42, top_n_features=None,\n",
    "                                  show_plot=True, plot_size=(12, 8)):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    # Initialize Extra Trees model with ET-specific defaults\n",
    "    if task_type == 'classification':\n",
    "        model = ExtraTreesClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,  # Often False for Extra Trees\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    elif task_type == 'regression':\n",
    "        model = ExtraTreesRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            bootstrap=bootstrap,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be 'classification' or 'regression'\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get importance scores and variability\n",
    "    importances = model.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances,\n",
    "        'Std_Deviation': std\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Select top N features\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "    \n",
    "    # Enhanced visualization\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=True)\n",
    "        \n",
    "        # Create colored bars with error bars\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df)))\n",
    "        bars = plt.barh(importance_df['Feature'], \n",
    "                       importance_df['Importance'], \n",
    "                       color=colors,\n",
    "                       xerr=importance_df['Std_Deviation'],\n",
    "                       capsize=4,\n",
    "                       alpha=0.7)\n",
    "        \n",
    "        # Style the plot\n",
    "        plt.xlabel('Mean Decrease in Impurity (MDI)', fontsize=12)\n",
    "        plt.title('Extra Trees Feature Importance\\n(More Randomized Splits Than Random Forest)', \n",
    "                fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "        \n",
    "        # Add importance values\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005, \n",
    "                    bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.4f}',\n",
    "                    va='center',\n",
    "                    fontsize=10)\n",
    "        \n",
    "        # Add ET-specific parameters as annotation\n",
    "        params_text = (\n",
    "            f\"Extra Trees Parameters:\\n\"\n",
    "            f\"n_estimators={n_estimators}, max_features={max_features}\\n\"\n",
    "            f\"bootstrap={bootstrap}, min_samples_split={min_samples_split}\"\n",
    "        )\n",
    "        plt.annotate(params_text,\n",
    "                   xy=(0.98, 0.02),\n",
    "                   xycoords='axes fraction',\n",
    "                   ha='right',\n",
    "                   va='bottom',\n",
    "                   bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffd423",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5a618",
   "metadata": {},
   "source": [
    "## 3.1.2 XGBoost/LightGBM/CatBoost Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8ce32",
   "metadata": {},
   "source": [
    "#### Best used when:\n",
    "\n",
    "- You want high-performance models with built-in interpretability.\n",
    "- Your data has missing values, imbalanced classes, or is sparse.\n",
    "- You want multiple importance views (gain vs. cover vs. permutation).\n",
    "- May be sensitive to model tuning; different hyperparameters may shift feature rankings.\n",
    "- May not reflect causal impact, only predictive contribution.\n",
    "\n",
    "#### Use for:\n",
    "\n",
    "- Feature selection in high-dimensional, nonlinear tasks.\n",
    "\n",
    "- Detecting redundant or low-contribution variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ec42b",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc014a",
   "metadata": {},
   "source": [
    "### 3.1.2.1 XG Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24899644",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding XGBoost and its Role in Feature Importance } </h2>\n",
    "</summary>\n",
    "<div>\n",
    "    <h3> What is XGBoost? </h3>\n",
    "    <p> XGBoost (Extreme Gradient Boosting) is an optimized gradient boosting framework that uses decision trees in an ensemble method. It is designed to be highly efficient, flexible, and portable, delivering state-of-the-art results on many machine learning tasks with speed and performance optimizations.</p>\n",
    "    <h3> Its Role in Feature Importance:</h3>\n",
    "    <p>XGBoost provides several built-in techniques to assess the importance of features in your data, helping practitioners understand which variables are driving model predictions.</p>\n",
    "    <ul>\n",
    "        <li>XGBoost calculates feature importance using three main metrics: weight (number of times a feature is used to split), gain (average improvement in accuracy), and cover (relative number of observations affected).</li>\n",
    "        <li>The <code>plot_importance()</code> function in XGBoost helps visualize the most relevant features ranked by your chosen metric.</li>\n",
    "        <li>It supports both gain-based and permutation-based importance, allowing for deeper insights into feature relevance and stability.</li>\n",
    "        <li>Regularization in XGBoost helps prevent overfitting, which in turn leads to more reliable feature importance scores compared to unregularized tree methods.</li>\n",
    "        <li>In feature selection workflows, XGBoost can serve as a first-pass filter to prune irrelevant or redundant features before model refinement.</li>\n",
    "    </ul>\n",
    "    <h3> Resources: </h3>\n",
    "    <ol>\n",
    "        <li><a href=\"https://arxiv.org/abs/1706.06060\" target=\"_blank\">Consistent Feature Attribution for Tree Ensembles (arXiv)</a></li>\n",
    "        <li><a href=\"https://arxiv.org/abs/2105.05328\" target=\"_blank\">Comparing Interpretability and Explainability for Feature Selection (arXiv)</a></li>\n",
    "        <li><a href=\"https://arxiv.org/abs/1901.08433\" target=\"_blank\">A XGBoost Risk Model via Feature Selection and Bayesian Hyper-parameter Optimization (arXiv)</a></li>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=OtD8wVaFm6E\" target=\"_blank\">XGBoost Tutorial for Beginners (YouTube)</a></li>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\" target=\"_blank\">XGBoost Feature Importance Explained (YouTube)</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab91111",
   "metadata": {},
   "source": [
    "XGBoost-specific feature importance with multiple importance types.\n",
    "    \n",
    "##### Parameters:\n",
    "    \n",
    "- n_estimators: Number of boosting rounds\n",
    "- learning_rate: Boosting learning rate\n",
    "- max_depth: Maximum tree depth\n",
    "- gamma: Minimum loss reduction to make a split\n",
    "- subsample: Subsample ratio of training instances\n",
    "- colsample_bytree: Subsample ratio of features\n",
    "- reg_alpha: L1 regularization (alpha)\n",
    "- reg_lambda: L2 regularization (lambda)\n",
    "- random_state: Random seed\n",
    "- importance_type: 'weight', 'gain', 'cover', 'total_gain', 'total_cover\n",
    "    \n",
    "##### Returns:\n",
    "\n",
    "- Dictionary of DataFrames for all importance types\n",
    "- Displays a styled importance plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f9a26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def xgboost_feature_importance(X, y, task_type='classification',\n",
    "                             n_estimators=100, learning_rate=0.1,\n",
    "                             max_depth=3, gamma=0, subsample=0.8,\n",
    "                             colsample_bytree=0.8, reg_alpha=0, reg_lambda=1,\n",
    "                             random_state=42, importance_type='weight',\n",
    "                             top_n_features=20, show_plot=True,\n",
    "                             plot_size=(12, 8)):\n",
    "   \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    # Initialize XGBoost model with XGB-specific parameters\n",
    "    if task_type == 'classification':\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='logloss' if len(set(y)) == 2 else 'mlogloss'\n",
    "        )\n",
    "    elif task_type == 'regression':\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            max_depth=max_depth,\n",
    "            gamma=gamma,\n",
    "            subsample=subsample,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            eval_metric='rmse'\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be 'classification' or 'regression'\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Get all importance types\n",
    "    importance_types = ['weight', 'gain', 'cover', 'total_gain', 'total_cover']\n",
    "    importance_dfs = {}\n",
    "    \n",
    "    for imp_type in importance_types:\n",
    "        importance_scores = model.get_booster().get_score(importance_type=imp_type)\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': list(importance_scores.keys()),\n",
    "            imp_type: list(importance_scores.values())\n",
    "        }).sort_values(imp_type, ascending=False)\n",
    "        \n",
    "        if top_n_features:\n",
    "            importance_df = importance_df.head(top_n_features)\n",
    "        \n",
    "        importance_dfs[imp_type] = importance_df\n",
    "    \n",
    "    # Plot specified importance type\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df = importance_dfs[importance_type].sort_values(importance_type, ascending=True)\n",
    "        \n",
    "        # Create gradient-colored bars\n",
    "        colors = plt.cm.viridis(np.linspace(0.2, 1, len(importance_df)))\n",
    "        bars = plt.barh(importance_df['Feature'], \n",
    "                       importance_df[importance_type], \n",
    "                       color=colors,\n",
    "                       alpha=0.7)\n",
    "        \n",
    "        # Style the plot\n",
    "        plt.xlabel(f'XGBoost Importance ({importance_type})', fontsize=12)\n",
    "        plt.title(f'XGBoost Feature Importance\\n(Importance Type: {importance_type})', \n",
    "                 fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Add values on bars\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005, \n",
    "                    bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.2f}',\n",
    "                    va='center',\n",
    "                    fontsize=10)\n",
    "        \n",
    "        # Add XGB-specific parameters\n",
    "        params_text = (\n",
    "            f\"XGBoost Parameters:\\n\"\n",
    "            f\"learning_rate={learning_rate}, max_depth={max_depth}\\n\"\n",
    "            f\"subsample={subsample}, colsample_bytree={colsample_bytree}\\n\"\n",
    "            f\"reg_alpha={reg_alpha}, reg_lambda={reg_lambda}\"\n",
    "        )\n",
    "        plt.annotate(params_text,\n",
    "                   xy=(0.98, 0.02),\n",
    "                   xycoords='axes fraction',\n",
    "                   ha='right',\n",
    "                   va='bottom',\n",
    "                   fontsize=9,\n",
    "                   bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return importance_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e7f82",
   "metadata": {},
   "source": [
    "### 3.1.2.2 Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452a2c2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2>{ Understanding LightGBM and its role in Feature Importance }</h2>\n",
    "</summary>\n",
    "<h3>What is LightGBM?</h3>\n",
    "<p>LightGBM (Light Gradient Boosting Machine) is a gradient boosting framework that uses tree-based learning algorithms. It's designed for efficiency and speed, particularly with large datasets, through its unique histogram-based approach and leaf-wise tree growth strategy rather than the level-wise approach used by other boosting algorithms.</p>\n",
    "\n",
    "<h3>Its role in Feature Importance:</h3>\n",
    "<p>LightGBM offers powerful built-in capabilities for feature selection that can significantly improve model performance while reducing dimensionality.</p>\n",
    "<ul>\n",
    "    <li>It provides multiple feature importance metrics: 'split' counts how many times a feature is used in splits; 'gain' measures the total reduction of loss contributed by splits on a feature.</li>\n",
    "    <li>LightGBM's exclusive leaf-wise growth strategy focuses on the most promising leaves rather than expanding all nodes at the same level, naturally prioritizing the most informative features.</li>\n",
    "    <li>It handles categorical features natively through efficient binning, providing more accurate importance measurements for categorical variables without requiring preprocessing like one-hot encoding.</li>\n",
    "    <li>For high-dimensional datasets, LightGBM includes built-in feature selection capabilities via its 'feature_fraction' parameter, which randomly selects a subset of features for each tree iteration.</li>\n",
    "    <li>It offers Gradient-based One-Side Sampling (GOSS) that focuses on instances with larger gradients while randomly sampling instances with smaller gradients, implicitly highlighting features that are most relevant for difficult-to-predict samples.</li>\n",
    "</ul>\n",
    "<h3>Resources:</h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://lightgbm.readthedocs.io/en/latest/Features.html\" target=\"_blank\">LightGBM Official Documentation</a></li>\n",
    "    <li><a href=\"https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf\" target=\"_blank\">LightGBM: A Highly Efficient Gradient Boosting Decision Tree (Original Paper)</a></li>\n",
    "    <li><a href=\"https://www.youtube.com/watch?v=n_ZMQj09S6w\" target=\"_blank\">Understanding LightGBM Parameters (Video Tutorial)</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f0d06",
   "metadata": {},
   "source": [
    "#### LightGBM-specific feature importance with unique boosting parameters.\n",
    "    \n",
    "##### Parameters:\n",
    "   \n",
    "- num_leaves: Maximum number of leaves in one tree\n",
    "- max_depth: Limit tree depth (-1 for no limit)\n",
    "- learning_rate: Shrinkage rate\n",
    "- n_estimators: Number of boosting iterations\n",
    "- min_child_samples: Minimum data in one leaf\n",
    "- subsample: Row subsample ratio\n",
    "- colsample_bytree: Column subsample ratio\n",
    "- reg_alpha: L1 regularization\n",
    "- reg_lambda: L2 regularization\n",
    "- importance_type: 'split' (count) or 'gain' (average gain)  \n",
    "    \n",
    "##### Returns:\n",
    "\n",
    "- DataFrame with feature importances\n",
    "- Displays a styled importance plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "260fae52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lightgbm) (1.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from lightgbm) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0003f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def lightgbm_feature_importance(X, y, task_type='classification',\n",
    "                              num_leaves=31, max_depth=-1,\n",
    "                              learning_rate=0.1, n_estimators=100,\n",
    "                              min_child_samples=20, subsample=1.0,\n",
    "                              colsample_bytree=1.0, reg_alpha=0.0,\n",
    "                              reg_lambda=0.0, random_state=42,\n",
    "                              importance_type='split', top_n_features=20,\n",
    "                              show_plot=True, plot_size=(12, 8)):\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    # LightGBM dataset format\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # LightGBM-specific parameters\n",
    "    params = {\n",
    "        'objective': 'binary' if task_type == 'classification' and len(set(y)) == 2 \n",
    "                    else 'multiclass' if task_type == 'classification' \n",
    "                    else 'regression',\n",
    "        'num_leaves': num_leaves,\n",
    "        'max_depth': max_depth,\n",
    "        'learning_rate': learning_rate,\n",
    "        'feature_fraction': colsample_bytree,\n",
    "        'bagging_fraction': subsample,\n",
    "        'min_child_samples': min_child_samples,\n",
    "        'lambda_l1': reg_alpha,\n",
    "        'lambda_l2': reg_lambda,\n",
    "        'verbosity': -1,\n",
    "        'seed': random_state,\n",
    "        'num_threads': -1\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=n_estimators,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20, verbose=False)]\n",
    "    )\n",
    "    \n",
    "    # Get importance scores\n",
    "    importance = model.feature_importance(importance_type=importance_type)\n",
    "    feature_names = model.feature_name()\n",
    "    \n",
    "    # Create importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Select top N features\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "    \n",
    "    # Enhanced visualization\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df = importance_df.sort_values('Importance', ascending=True)\n",
    "        \n",
    "        # Gradient coloring\n",
    "        colors = plt.cm.coolwarm(np.linspace(0, 1, len(importance_df)))\n",
    "        bars = plt.barh(importance_df['Feature'], \n",
    "                       importance_df['Importance'], \n",
    "                       color=colors,\n",
    "                       alpha=0.7)\n",
    "        \n",
    "        # Style the plot\n",
    "        plt.xlabel(f'Importance ({importance_type})', fontsize=12)\n",
    "        title_type = 'Split Count' if importance_type == 'split' else 'Average Gain'\n",
    "        plt.title(f'LightGBM Feature Importance\\n({title_type})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Add values on bars\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005, \n",
    "                    bar.get_y() + bar.get_height()/2, \n",
    "                    f'{width:.0f}' if importance_type == 'split' else f'{width:.2f}',\n",
    "                    va='center',\n",
    "                    fontsize=10)\n",
    "        \n",
    "        # Add LightGBM-specific parameters\n",
    "        params_text = (\n",
    "            f\"LightGBM Parameters:\\n\"\n",
    "            f\"num_leaves={num_leaves}, max_depth={max_depth}\\n\"\n",
    "            f\"learning_rate={learning_rate}, min_child_samples={min_child_samples}\\n\"\n",
    "            f\"subsample={subsample}, colsample_bytree={colsample_bytree}\"\n",
    "        )\n",
    "        plt.annotate(params_text,\n",
    "                    xy=(0.98, 0.02),\n",
    "                    xycoords='axes fraction',\n",
    "                    ha='right',\n",
    "                    va='bottom',\n",
    "                    fontsize=9,\n",
    "                    bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e657b2",
   "metadata": {},
   "source": [
    "### 3.1.2.3 Cat Boost "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdddf2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding CatBoost and its Role in Feature Importance }</h2>\n",
    "</summary>\n",
    "<div>\n",
    "    <h3> What is CatBoost? </h3>\n",
    "    <p> CatBoost is an open-source gradient boosting library developed by Yandex, designed to handle categorical features efficiently without extensive preprocessing. It employs techniques like ordered boosting and symmetric trees to reduce overfitting and enhance performance across various machine learning tasks, including classification, regression, and ranking. </p>\n",
    "    <h3> Its Role in Feature Importance:</h3>\n",
    "    <p>CatBoost offers multiple methods to evaluate feature importance, aiding in understanding model behavior and refining feature selection:</p>\n",
    "    <ul>\n",
    "        <li><b>PredictionValuesChange</b>: Measures the average change in predictions when a feature's value changes, indicating its impact on the model's output.</li>\n",
    "        <li><b>LossFunctionChange</b>: Assesses the change in the loss function when a feature is excluded, providing insight into its contribution to model performance.</li>\n",
    "        <li><b>ShapValues</b>: Utilizes SHAP (SHapley Additive exPlanations) to attribute contributions of each feature to individual predictions, offering a detailed interpretability framework.</li>\n",
    "        <li><b>Interaction</b>: Evaluates how combinations of features contribute to the model's predictions, identifying synergistic effects between features.</li>\n",
    "        <li>Feature importance values can be accessed using the <code>get_feature_importance()</code> method, allowing for analysis and visualization of feature contributions.</li>\n",
    "    </ul>\n",
    "    <h3> Resources: </h3>\n",
    "    <ol>\n",
    "        <li><a href=\"https://catboost.ai/docs/en/features/feature-importances-calculation\" target=\"_blank\">CatBoost Feature Importance Documentation</a></li>\n",
    "        <li><a href=\"https://www.geeksforgeeks.org/catboost-feature-importance/\" target=\"_blank\">CatBoost Feature Importance | GeeksforGeeks</a></li>\n",
    "        <li><a href=\"https://www.rasgoml.com/feature-engineering-tutorials/how-to-generate-feature-importance-plots-using-catboost\" target=\"_blank\">How To Generate Feature Importance Plots Using CatBoost - Rasgo</a></li>\n",
    "        <li><a href=\"https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Catboost%20tutorial.html\" target=\"_blank\">CatBoost Tutorial — SHAP Documentation</a></li>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=9rXW1uHhZxI\" target=\"_blank\">CatBoost Tutorial for Beginners (YouTube)</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851bba6",
   "metadata": {},
   "source": [
    "#### CatBoost-specific feature importance with multiple importance types.\n",
    "    \n",
    "##### Parameters:\n",
    "\n",
    "- iterations: Number of boosting rounds\n",
    "- learning_rate: Boosting learning rate\n",
    "- depth: Depth of the trees\n",
    "- l2_leaf_reg: L2 regularization term\n",
    "- random_strength: Randomness strength to use at score calculation\n",
    "- bagging_temperature: Controls intensity of Bayesian bagging\n",
    "- border_count: Number of splits for numerical features\n",
    "- random_state: Random seed\n",
    "- importance_type: 'PredictionValuesChange', 'LossFunctionChange', 'ShapValues'\n",
    "\n",
    "    \n",
    "##### Returns:\n",
    "\n",
    "- Dictionary of DataFrames for all importance types\n",
    "- Displays a styled importance plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aa3a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def catboost_feature_importance(X, y, task_type='classification',\n",
    "                                 iterations=500, learning_rate=0.03,\n",
    "                                 depth=6, l2_leaf_reg=3,\n",
    "                                 random_strength=1,\n",
    "                                 bagging_temperature=1,\n",
    "                                 border_count=128,\n",
    "                                 random_state=42,\n",
    "                                 importance_type='PredictionValuesChange',\n",
    "                                 top_n_features=20,\n",
    "                                 show_plot=True,\n",
    "                                 plot_size=(12, 8)):\n",
    "\n",
    "    \n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    # Initialize CatBoost model with CatBoost-specific parameters\n",
    "    model_params = {\n",
    "        'iterations': iterations,\n",
    "        'learning_rate': learning_rate,\n",
    "        'depth': depth,\n",
    "        'l2_leaf_reg': l2_leaf_reg,\n",
    "        'random_strength': random_strength,\n",
    "        'bagging_temperature': bagging_temperature,\n",
    "        'border_count': border_count,\n",
    "        'random_state': random_state,\n",
    "        'verbose': 0\n",
    "    }\n",
    "    \n",
    "    if task_type == 'classification':\n",
    "        model = CatBoostClassifier(**model_params, loss_function='Logloss')\n",
    "    elif task_type == 'regression':\n",
    "        model = CatBoostRegressor(**model_params, loss_function='RMSE')\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be 'classification' or 'regression'\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Importance Types\n",
    "    importance_types = ['PredictionValuesChange', 'LossFunctionChange', 'ShapValues']\n",
    "    importance_dfs = {}\n",
    "    \n",
    "    for imp_type in importance_types:\n",
    "        if imp_type == 'ShapValues':\n",
    "            shap_values = model.get_feature_importance(Pool(X, label=y), type=imp_type)\n",
    "            feature_importance = np.abs(shap_values).mean(axis=0)[:-1]  # exclude last element (bias)\n",
    "        else:\n",
    "            feature_importance = model.get_feature_importance(Pool(X, label=y), type=imp_type)\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            imp_type: feature_importance\n",
    "        }).sort_values(imp_type, ascending=False)\n",
    "        \n",
    "        if top_n_features:\n",
    "            importance_df = importance_df.head(top_n_features)\n",
    "        \n",
    "        importance_dfs[imp_type] = importance_df\n",
    "    \n",
    "    # Plot specified importance type\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df = importance_dfs[importance_type].sort_values(importance_type, ascending=True)\n",
    "        \n",
    "        # Gradient color bars\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df)))\n",
    "        bars = plt.barh(importance_df['Feature'],\n",
    "                        importance_df[importance_type],\n",
    "                        color=colors,\n",
    "                        alpha=0.8)\n",
    "        \n",
    "        # Style\n",
    "        plt.xlabel(f'CatBoost Importance ({importance_type})', fontsize=12)\n",
    "        plt.title(f'CatBoost Feature Importance\\n(Importance Type: {importance_type})',\n",
    "                  fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Annotate values\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.005,\n",
    "                     bar.get_y() + bar.get_height()/2,\n",
    "                     f'{width:.2f}',\n",
    "                     va='center',\n",
    "                     fontsize=10)\n",
    "        \n",
    "        # Add CatBoost-specific hyperparameters\n",
    "        params_text = (\n",
    "            f\"CatBoost Parameters:\\n\"\n",
    "            f\"learning_rate={learning_rate}, depth={depth},\\n\"\n",
    "            f\"l2_leaf_reg={l2_leaf_reg}, random_strength={random_strength}\\n\"\n",
    "            f\"bagging_temperature={bagging_temperature}, border_count={border_count}\"\n",
    "        )\n",
    "        plt.annotate(params_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return importance_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5c9cd",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f456d8e",
   "metadata": {},
   "source": [
    "## 3.1.3 Logistic Regression (Coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbed7f8",
   "metadata": {},
   "source": [
    "#### Best used when:\n",
    "\n",
    "- You have linearly separable data.\n",
    "- Your features are standardized or scaled.\n",
    "- You need interpretable models (e.g., healthcare, finance).\n",
    "- You want to understand feature direction (positive/negative influence).\n",
    "- Not ideal for capturing nonlinear interactions.\n",
    "- Correlated features can distort the interpretation (use L1 regularization for selection).\n",
    "\n",
    "#### Use for:\n",
    "\n",
    "- Feature selection for interpretable and simple models.\n",
    "- Feature ranking when the direction of impact matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f21d2",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62145c4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2>{ Understanding Logistic Regression and its Role in Feature Importance }</h2>\n",
    "</summary>\n",
    "<div>\n",
    "    <h3> What is Logistic Regression? </h3>\n",
    "    <p> Logistic Regression is a linear model used for binary (and multiclass) classification tasks. It estimates the probability that a given input belongs to a particular class using the logistic (sigmoid) function and models the relationship between features and the log-odds of the outcome. </p>\n",
    "    <h3> Its Role in Feature Importance:</h3>\n",
    "    <p>Logistic Regression directly provides feature importance through its learned coefficients, which reflect the strength and direction of each feature's impact on the target prediction:</p>\n",
    "    <ul>\n",
    "        <li>Each feature is assigned a coefficient representing the change in the log-odds of the outcome for a one-unit increase in that feature, holding others constant.</li>\n",
    "        <li>Positive coefficients indicate that increasing the feature value increases the predicted probability, while negative coefficients indicate the opposite.</li>\n",
    "        <li>By examining the magnitude of standardized coefficients, one can assess which features have the most influence on predictions.</li>\n",
    "        <li>Regularization techniques like L1 (Lasso) can be applied to perform automatic feature selection by shrinking less important coefficients to zero.</li>\n",
    "        <li>Logistic Regression is especially valuable when interpretability is critical, as it provides a transparent mapping between inputs and outputs.</li>\n",
    "    </ul>\n",
    "    <h3> Resources: </h3>\n",
    "    <ol>\n",
    "        <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\" target=\"_blank\">LogisticRegression — scikit-learn Documentation</a></li>\n",
    "        <li><a href=\"https://www.statsmodels.org/stable/examples/notebooks/generated/logit.html\" target=\"_blank\">Logistic Regression Example — Statsmodels</a></li>\n",
    "        <li><a href=\"https://towardsdatascience.com/interpreting-coefficients-in-logistic-regression-ebd6d4a6f2b0\" target=\"_blank\">Interpreting Coefficients in Logistic Regression (Towards Data Science)</a></li>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=yIYKR4sgzI8\" target=\"_blank\">StatQuest: Logistic Regression Clearly Explained (YouTube)</a></li>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=zAULhNrnuL4\" target=\"_blank\">Feature Importance with Logistic Regression (YouTube)</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26f87a",
   "metadata": {},
   "source": [
    "#### Logistic Regression-specific feature importance using model coefficients.\n",
    "    \n",
    "##### Parameters:\n",
    "    \n",
    "- penalty: Regularization type ('l1', 'l2', 'elasticnet', or 'none')\n",
    "- C: Inverse of regularization strength (smaller -> stronger regularization)\n",
    "- solver: Optimization algorithm ('liblinear', 'saga', 'lbfgs', etc.)\n",
    "- max_iter: Maximum number of iterations\n",
    "- random_state: Random seed\n",
    "- scale_features: Whether to standardize features before fitting\n",
    "    \n",
    "    \n",
    "##### Returns:\n",
    "    \n",
    "- DataFrame with features and their absolute importance (coefficients)\n",
    "- Displays a sorted bar plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf18316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def logistic_regression_feature_importance(X, y,\n",
    "                                            penalty='l2', C=1.0,\n",
    "                                            solver='lbfgs', max_iter=1000,\n",
    "                                            random_state=42,\n",
    "                                            scale_features=True,\n",
    "                                            top_n_features=20,\n",
    "                                            show_plot=True,\n",
    "                                            plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    \n",
    "    feature_names = X.columns.tolist()\n",
    "\n",
    "    # Standardize features if required\n",
    "    if scale_features:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = X.values\n",
    "\n",
    "    # Initialize Logistic Regression model\n",
    "    model = LogisticRegression(\n",
    "        penalty=penalty,\n",
    "        C=C,\n",
    "        solver=solver,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # Get feature importance (absolute value of coefficients)\n",
    "    if len(np.array(model.coef_).shape) > 1 and model.coef_.shape[0] > 1:\n",
    "        # For multiclass, take mean absolute value across classes\n",
    "        coef_importance = np.mean(np.abs(model.coef_), axis=0)\n",
    "    else:\n",
    "        coef_importance = np.abs(model.coef_.flatten())\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': coef_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    # Keep top N features\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    # Plot feature importance\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        # Create gradient-colored bars\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'],\n",
    "                        importance_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.8)\n",
    "\n",
    "        plt.xlabel('Absolute Coefficient Value', fontsize=12)\n",
    "        plt.title('Logistic Regression Feature Importance\\n(Based on Coefficients)', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        # Annotate coefficient values\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                     f'{width:.4f}', va='center', fontsize=9)\n",
    "\n",
    "        # Annotate model parameters\n",
    "        params_text = (\n",
    "            f\"Logistic Regression Parameters:\\n\"\n",
    "            f\"penalty={penalty}, C={C}, solver={solver}\\n\"\n",
    "            f\"scale_features={scale_features}\"\n",
    "        )\n",
    "        plt.annotate(params_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e115fa3",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be6e221",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d164d0d",
   "metadata": {},
   "source": [
    "## 3.1.4 SVM (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2211496",
   "metadata": {},
   "source": [
    "#### Best used when:\n",
    "\n",
    "- You have high-dimensional data (e.g., text classification, genomics).\n",
    "- You need sparse and interpretable models (use L1 regularization).\n",
    "- You want to evaluate directional influence of features.\n",
    "- Doesn’t handle nonlinear interactions unless kernelized.\n",
    "- Can be sensitive to feature scaling—standardization is a must.\n",
    "\n",
    "#### Use for:\n",
    "\n",
    "- Feature ranking in high-dimensional linear problems.\n",
    "- Pre-filtering for downstream nonlinear models.-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac6beb",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241082a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2>{ Understanding Linear SVM and its Role in Feature Importance }</h2>\n",
    "</summary>\n",
    "<div>\n",
    "    <h3> What is Linear SVM? </h3>\n",
    "    <p> Linear Support Vector Machine (SVM) is a supervised learning algorithm used primarily for classification tasks. It finds the optimal hyperplane that separates data into different classes by maximizing the margin between the closest points (support vectors) of each class.</p>\n",
    "    <h3> Its Role in Feature Importance:</h3>\n",
    "    <p>Linear SVMs can provide insight into feature importance through the weights of the hyperplane coefficients:</p>\n",
    "    <ul>\n",
    "        <li>The model assigns a weight to each feature, which corresponds to the feature’s contribution to the separating hyperplane.</li>\n",
    "        <li>Larger absolute values of weights indicate features that have a stronger influence on the decision boundary.</li>\n",
    "        <li>Feature importance in Linear SVMs is direction-sensitive; positive weights push classification in one direction, negative in the other.</li>\n",
    "        <li>Regularization (e.g., L1 or L2 penalties) can be applied to induce sparsity or smoothness, helping identify relevant features and reduce overfitting.</li>\n",
    "        <li>Linear SVMs are particularly effective in high-dimensional settings like text classification, where feature interpretability is important.</li>\n",
    "    </ul>\n",
    "    <h3> Resources: </h3>\n",
    "    <ol>\n",
    "        <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\" target=\"_blank\">LinearSVC — scikit-learn Documentation</a></li>\n",
    "        <li><a href=\"https://www.csie.ntu.edu.tw/~cjlin/papers/linear.pdf\" target=\"_blank\">A Practical Guide to Support Vector Classification (PDF)</a></li>\n",
    "        <li><a href=\"https://towardsdatascience.com/svm-feature-importance-and-implementation-in-python-1b5b80291d80\" target=\"_blank\">SVM Feature Importance and Implementation in Python (Medium)</a></li>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=efR1C6CvhmE\" target=\"_blank\">StatQuest: Support Vector Machines Clearly Explained (YouTube)</a></li>\n",
    "        <li><a href=\"https://www.youtube.com/watch?v=Y6RRHw9uN9o\" target=\"_blank\">SVM Intuition and Feature Impact (YouTube)</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7323340",
   "metadata": {},
   "source": [
    "#### SVM (Linear Kernel) specific feature importance based on model coefficients.\n",
    "\n",
    "##### Parameters:\n",
    "    \n",
    "- penalty: Regularization ('l1', 'l2') [classification only]\n",
    "- loss: Loss function for classification\n",
    "- dual: Dual or primal formulation\n",
    "- tol: Tolerance for stopping criterion\n",
    "- C: Regularization parameter\n",
    "- fit_intercept: Whether to fit the intercept\n",
    "- scale_data: Whether to standardize features\n",
    "    \n",
    "##### Returns:\n",
    "    \n",
    "- DataFrame of features and absolute coefficient importance\n",
    "- Displays a styled bar plot (if show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f41462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC, LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def svm_linear_feature_importance(X, y, task_type='classification',\n",
    "                                   penalty='l2', loss='squared_hinge',\n",
    "                                   dual=True, tol=1e-4, C=1.0,\n",
    "                                   fit_intercept=True, max_iter=1000,\n",
    "                                   random_state=42,\n",
    "                                   top_n_features=20, show_plot=True,\n",
    "                                   scale_data=True, plot_size=(12, 8)):\n",
    "\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "\n",
    "    # Feature scaling (SVM sensitive to scale)\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "    # Initialize SVM model\n",
    "    if task_type == 'classification':\n",
    "        model = LinearSVC(\n",
    "            penalty=penalty,\n",
    "            loss=loss,\n",
    "            dual=dual,\n",
    "            tol=tol,\n",
    "            C=C,\n",
    "            fit_intercept=fit_intercept,\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    elif task_type == 'regression':\n",
    "        model = LinearSVR(\n",
    "            epsilon=0.0,\n",
    "            tol=tol,\n",
    "            C=C,\n",
    "            fit_intercept=fit_intercept,\n",
    "            max_iter=max_iter,\n",
    "            random_state=random_state\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"task_type must be 'classification' or 'regression'\")\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Get feature importance (coefficients)\n",
    "    if task_type == 'classification' and len(np.unique(y)) > 2:\n",
    "        # For multi-class: take mean of absolute coefficients across classes\n",
    "        coef = np.mean(np.abs(model.coef_), axis=0)\n",
    "    else:\n",
    "        coef = np.abs(model.coef_.ravel())\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': coef\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if top_n_features:\n",
    "        importance_df = importance_df.head(top_n_features)\n",
    "\n",
    "    # Plot\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        importance_df_sorted = importance_df.sort_values('Importance', ascending=True)\n",
    "        \n",
    "        # Gradient coloring\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(importance_df_sorted)))\n",
    "        bars = plt.barh(importance_df_sorted['Feature'], \n",
    "                        importance_df_sorted['Importance'], \n",
    "                        color=colors,\n",
    "                        alpha=0.8)\n",
    "\n",
    "        plt.xlabel('Absolute Coefficient Value', fontsize=12)\n",
    "        plt.title(f'SVM (Linear) Feature Importance\\n(Task: {task_type.capitalize()})', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        # Add values on bars\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.01, \n",
    "                     bar.get_y() + bar.get_height()/2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        # Annotate model hyperparameters\n",
    "        params_text = (\n",
    "            f\"SVM Parameters:\\n\"\n",
    "            f\"C={C}, penalty={penalty if task_type == 'classification' else 'N/A'}\\n\"\n",
    "            f\"loss={loss if task_type == 'classification' else 'N/A'}, tol={tol}\\n\"\n",
    "            f\"scale_data={scale_data}\"\n",
    "        )\n",
    "        plt.annotate(params_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0e6e9",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

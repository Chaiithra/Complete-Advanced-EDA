{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd846d76",
   "metadata": {},
   "source": [
    "--> To put content in collapsable format, we can use the following snippet in the markdowns:\n",
    "\n",
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<b> double click the markdown to see the code instead of this </b>\n",
    "</summary>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a670042",
   "metadata": {},
   "source": [
    "# 3.6 Neural Network Specific Methods\n",
    "-- Gradient & attention-based importance\n",
    "- Integrated Gradients\n",
    "- Saliency Maps / Grad-CAM\n",
    "- Attention Weights (NLP/Transformers)\n",
    "- DeepLIFT\n",
    "- Attention Weights (NLP/Transformers)\n",
    "- DeepSHAP\n",
    "- Layer-wise Relevance Propagation (LRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c4eea",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdc224a",
   "metadata": {},
   "source": [
    "## 3.6.1 Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3770ef",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Integrated Gradients } </h2>\n",
    "</summary>\n",
    "<h3> What is Integrated Gradients? </h3>\n",
    "<p> Integrated Gradients is a technique to attribute a model's prediction to its input features by integrating gradients along a path from a baseline input to the actual input.</p>\n",
    "<h3> Its role in Deep Learning Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Model-agnostic and applicable to any differentiable model.</li>\n",
    "    <li> Helps attribute importance scores to input features like image pixels or text tokens.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://arxiv.org/abs/1703.01365\" target=\"_blank\">Original Paper: Axiomatic Attribution for Deep Networks</a></li>\n",
    "    <li><a href=\"https://captum.ai/api/integrated_gradients.html\" target=\"_blank\">Captum (PyTorch) API</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55beaa7",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A trained deep learning model (TensorFlow/Keras or PyTorch style)\n",
    "- X: Features (DataFrame or numpy array) to explain\n",
    "- baseline: Baseline input to start integrating from (default: zero baseline)\n",
    "- target_label: Target output index/class for explanation (classification) or None (regression)\n",
    "- m_steps: Number of interpolation steps between baseline and input\n",
    "- batch_size: Batch size for gradient calculation\n",
    "- scale_data: Whether to standardize features before applying IG\n",
    "- show_plot: Whether to plot the integrated gradients per feature\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of feature attributions (importance scores)\n",
    "- Displays feature attribution bar plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce25440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def integrated_gradients(model,\n",
    "                          X,\n",
    "                          baseline=None,\n",
    "                          target_label=None,\n",
    "                          m_steps=50,\n",
    "                          batch_size=32,\n",
    "                          scale_data=True,\n",
    "                          show_plot=True,\n",
    "                          plot_size=(12, 8),\n",
    "                          random_state=42):\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        feature_names = X.columns.tolist()\n",
    "        X_processed = X.copy()\n",
    "    else:\n",
    "        X = np.array(X)\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "        X_processed = X.copy()\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "\n",
    "    X_processed = tf.convert_to_tensor(X_processed, dtype=tf.float32)\n",
    "\n",
    "    if baseline is None:\n",
    "        baseline = tf.zeros(shape=X_processed.shape, dtype=tf.float32)\n",
    "    else:\n",
    "        if isinstance(baseline, pd.DataFrame):\n",
    "            baseline = scaler.transform(baseline)\n",
    "        baseline = tf.convert_to_tensor(baseline, dtype=tf.float32)\n",
    "\n",
    "    def interpolate(baseline, input, steps):\n",
    "        alphas = tf.linspace(0.0, 1.0, steps + 1)\n",
    "        alphas_x = tf.expand_dims(alphas, axis=-1)\n",
    "        interpolated = baseline + alphas_x * (input - baseline)\n",
    "        return interpolated\n",
    "\n",
    "    def compute_gradients(inputs, target_label=None):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inputs)\n",
    "            predictions = model(inputs)\n",
    "\n",
    "            if target_label is not None:\n",
    "                outputs = predictions[:, target_label]\n",
    "            else:\n",
    "                outputs = predictions[:, 0]\n",
    "\n",
    "        grads = tape.gradient(outputs, inputs)\n",
    "        return grads\n",
    "\n",
    "    integrated_grads_list = []\n",
    "\n",
    "    for i in range(0, len(X_processed), batch_size):\n",
    "        input_batch = X_processed[i:i + batch_size]\n",
    "        baseline_batch = baseline[i:i + batch_size]\n",
    "\n",
    "        interpolated_batch = interpolate(baseline_batch, input_batch, m_steps)\n",
    "        interpolated_batch = tf.reshape(interpolated_batch, [-1, X_processed.shape[1]])\n",
    "\n",
    "        grads = compute_gradients(interpolated_batch, target_label=target_label)\n",
    "        grads = tf.reshape(grads, [batch_size, m_steps + 1, X_processed.shape[1]])\n",
    "\n",
    "        avg_grads = tf.reduce_mean(grads, axis=1)\n",
    "        delta = input_batch - baseline_batch\n",
    "        integrated_grads = delta * avg_grads\n",
    "\n",
    "        integrated_grads_list.append(integrated_grads.numpy())\n",
    "\n",
    "    integrated_grads_all = np.vstack(integrated_grads_list)\n",
    "    feature_importance = np.mean(np.abs(integrated_grads_all), axis=0)\n",
    "\n",
    "    attributions_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Attribution': feature_importance\n",
    "    }).sort_values('Attribution', ascending=False)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        attributions_df_sorted = attributions_df.sort_values('Attribution', ascending=True)\n",
    "\n",
    "        colors = plt.cm.plasma(np.linspace(0.2, 1, len(attributions_df_sorted)))\n",
    "        bars = plt.barh(attributions_df_sorted['Feature'],\n",
    "                        attributions_df_sorted['Attribution'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Attribution Magnitude', fontsize=12)\n",
    "        plt.title('Integrated Gradients Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Integrated Gradients\\n\"\n",
    "            f\"Steps: {m_steps}\\n\"\n",
    "            f\"Baseline: {'Zero' if baseline is None else 'Provided'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return attributions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf44557",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03483e40",
   "metadata": {},
   "source": [
    "## 3.6.2 Saliency Maps / Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825bcd2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Saliency Maps & Grad-CAM } </h2>\n",
    "</summary>\n",
    "<h3> What are Saliency Maps and Grad-CAM? </h3>\n",
    "<p> Saliency Maps use gradients to highlight input features (e.g., image pixels) that influence the output. Grad-CAM improves this by using gradients flowing into convolutional layers to produce heatmaps.</p>\n",
    "<h3> Their role in CNN Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Highlight important regions in an image for a specific class.</li>\n",
    "    <li> Grad-CAM is class-discriminative and visually intuitive.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://arxiv.org/abs/1610.02391\" target=\"_blank\">Grad-CAM Paper</a></li>\n",
    "    <li><a href=\"https://keras.io/examples/vision/grad_cam/\" target=\"_blank\">Keras Grad-CAM Example</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb68a751",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A trained deep learning model (TensorFlow/Keras)\n",
    "- X: Input features (DataFrame, numpy array, or Tensor) for which saliency is computed\n",
    "- target_layer_name: Name of the convolutional layer to compute Grad-CAM (for CNNs)\n",
    "- target_label: Index of the output/class to explain (classification), or None (regression)\n",
    "- method: 'saliency' for vanilla gradients, 'gradcam' for Grad-CAM\n",
    "- scale_data: Whether to standardize inputs\n",
    "- show_plot: Whether to visualize feature saliency\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of saliency scores (feature importance)\n",
    "- Displays a saliency heatmap/bar plot if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91078bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def saliency_feature_importance(model,\n",
    "                                 X,\n",
    "                                 target_layer_name=None,\n",
    "                                 target_label=None,\n",
    "                                 method='saliency',\n",
    "                                 scale_data=True,\n",
    "                                 show_plot=True,\n",
    "                                 plot_size=(12, 8),\n",
    "                                 random_state=42):\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        feature_names = X.columns.tolist()\n",
    "        X_processed = X.copy()\n",
    "    else:\n",
    "        X = np.array(X)\n",
    "        feature_names = [f\"Feature_{i}\" for i in range(X.shape[1])]\n",
    "        X_processed = X.copy()\n",
    "\n",
    "    tf.random.set_seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler()\n",
    "        X_processed = scaler.fit_transform(X_processed)\n",
    "\n",
    "    X_processed = tf.convert_to_tensor(X_processed, dtype=tf.float32)\n",
    "\n",
    "    @tf.function\n",
    "    def compute_saliency(inputs, target_label=None):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(inputs)\n",
    "            preds = model(inputs)\n",
    "            if target_label is not None:\n",
    "                output = preds[:, target_label]\n",
    "            else:\n",
    "                output = preds[:, 0]\n",
    "        grads = tape.gradient(output, inputs)\n",
    "        return grads\n",
    "\n",
    "    @tf.function\n",
    "    def compute_gradcam(inputs, model, target_layer_name, target_label=None):\n",
    "        grad_model = tf.keras.models.Model(\n",
    "            inputs=[model.inputs],\n",
    "            outputs=[model.get_layer(target_layer_name).output, model.output]\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            inputs = tf.cast(inputs, tf.float32)\n",
    "            conv_outputs, predictions = grad_model(inputs)\n",
    "            if target_label is not None:\n",
    "                loss = predictions[:, target_label]\n",
    "            else:\n",
    "                loss = predictions[:, 0]\n",
    "\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(1, 2))\n",
    "\n",
    "        conv_outputs = conv_outputs[0]\n",
    "        pooled_grads = pooled_grads[0]\n",
    "\n",
    "        heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "\n",
    "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "        return heatmap\n",
    "\n",
    "    if method == 'saliency':\n",
    "        grads = compute_saliency(X_processed, target_label=target_label)\n",
    "        saliency_scores = tf.reduce_mean(tf.abs(grads), axis=0).numpy()\n",
    "    elif method == 'gradcam':\n",
    "        if target_layer_name is None:\n",
    "            raise ValueError(\"For Grad-CAM, 'target_layer_name' must be specified.\")\n",
    "        heatmaps = []\n",
    "        for i in range(X_processed.shape[0]):\n",
    "            heatmap = compute_gradcam(X_processed[i:i+1], model, target_layer_name, target_label=target_label)\n",
    "            heatmaps.append(heatmap.numpy().flatten())\n",
    "\n",
    "        saliency_scores = np.mean(np.array(heatmaps), axis=0)\n",
    "        feature_names = [f\"Activation_{i}\" for i in range(len(saliency_scores))]\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'saliency' or 'gradcam'.\")\n",
    "\n",
    "    saliency_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': saliency_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        saliency_df_sorted = saliency_df.sort_values('Importance', ascending=True)\n",
    "\n",
    "        colors = plt.cm.inferno(np.linspace(0.2, 1, len(saliency_df_sorted)))\n",
    "        bars = plt.barh(saliency_df_sorted['Feature'],\n",
    "                        saliency_df_sorted['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Saliency Magnitude', fontsize=12)\n",
    "        plt.title(f'{method.upper()} Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=9)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: {method.upper()}\\n\"\n",
    "            f\"Target Layer: {target_layer_name if target_layer_name else 'Input Layer'}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return saliency_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e06dd",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c42bf",
   "metadata": {},
   "source": [
    "## 3.6.3 Attention Weights (NLP/Transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d547c1",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Attention Weights in Transformers } </h2>\n",
    "</summary>\n",
    "<h3> What are Attention Weights? </h3>\n",
    "<p> Attention weights determine how much each input token contributes to the representation of other tokens. In Transformers, they are central to how context is understood.</p>\n",
    "<h3> Their role in Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Show which parts of the input the model is focusing on at each layer/head.</li>\n",
    "    <li> Can be visualized as heatmaps to understand token-to-token influence.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://jalammar.github.io/illustrated-transformer/\" target=\"_blank\">The Illustrated Transformer (By Jay Alammar)</a></li>\n",
    "    <li><a href=\"https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions\" target=\"_blank\">Hugging Face Outputs</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898d70d7",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A trained Transformer/NLP model (e.g., HuggingFace, TensorFlow/Keras, or PyTorch model)\n",
    "- tokenizer: Tokenizer used for the model to process raw text\n",
    "- text_inputs: List of text inputs (or a single string) to analyze\n",
    "- layer: Transformer layer index to extract attention from (default last layer)\n",
    "- head: Attention head index to extract attention from (optional, defaults to averaging heads)\n",
    "- aggregation: How to aggregate token attention ('mean', 'max', 'first', etc.)\n",
    "- target_label: Optional, specific class index if classification is multi-output\n",
    "- show_plot: Whether to plot the attention importance\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of token-level attention scores (feature importance)\n",
    "- Displays a token attention heatmap if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b46ed9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def attention_feature_importance(model,\n",
    "                                  tokenizer,\n",
    "                                  text_inputs,\n",
    "                                  layer=-1,\n",
    "                                  head=None,\n",
    "                                  aggregation='mean',\n",
    "                                  target_label=None,\n",
    "                                  show_plot=True,\n",
    "                                  plot_size=(14, 6),\n",
    "                                  random_state=42):\n",
    "\n",
    "    if isinstance(text_inputs, str):\n",
    "        text_inputs = [text_inputs]\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    encoded = tokenizer(text_inputs,\n",
    "                        return_tensors='pt',\n",
    "                        padding=True,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded, output_attentions=True)\n",
    "\n",
    "    attentions = outputs.attentions  # List: (layers, batch_size, heads, tokens, tokens)\n",
    "    selected_attention = attentions[layer]  # Pick specified layer\n",
    "\n",
    "    # Shape: (batch_size, num_heads, tokens, tokens)\n",
    "    if head is not None:\n",
    "        attention_values = selected_attention[:, head, :, :]\n",
    "    else:\n",
    "        if aggregation == 'mean':\n",
    "            attention_values = selected_attention.mean(dim=1)\n",
    "        elif aggregation == 'max':\n",
    "            attention_values = selected_attention.max(dim=1).values\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported aggregation method.\")\n",
    "\n",
    "    attention_per_token = attention_values.mean(dim=1)  # Mean across attended tokens (axis=1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "\n",
    "    attention_scores = attention_per_token[0].cpu().numpy()\n",
    "\n",
    "    attention_df = pd.DataFrame({\n",
    "        'Token': tokens,\n",
    "        'Importance': attention_scores\n",
    "    })\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.plasma(np.linspace(0.3, 1, len(attention_df)))\n",
    "\n",
    "        bars = plt.barh(attention_df['Token'],\n",
    "                        attention_df['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Attention Weight', fontsize=12)\n",
    "        plt.title('Transformer Attention-Based Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=8)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: Attention Weights\\n\"\n",
    "            f\"Layer: {layer}\\n\"\n",
    "            f\"Aggregation: {aggregation.upper()}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return attention_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a492e07",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d33e72",
   "metadata": {},
   "source": [
    "## 3.6.4 DeepLIFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffe632",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding DeepLIFT } </h2>\n",
    "</summary>\n",
    "<h3> What is DeepLIFT? </h3>\n",
    "<p> DeepLIFT assigns contribution scores by comparing each neuron's activation to a reference activation and tracking differences back to input features.</p>\n",
    "<h3> Its role in Neural Network Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Tracks both positive and negative contributions of features.</li>\n",
    "    <li> More accurate than simple gradient methods in certain cases (e.g., ReLU saturation).</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://arxiv.org/abs/1704.02685\" target=\"_blank\">DeepLIFT Paper</a></li>\n",
    "    <li><a href=\"https://github.com/kundajelab/deeplift\" target=\"_blank\">DeepLIFT GitHub</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771d25e",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A trained deep learning model (TensorFlow/Keras or PyTorch)\n",
    "- inputs: Input data for which to compute feature importance (numpy array or tensor)\n",
    "- baseline: Baseline input for DeepLIFT comparisons (e.g., zeros or dataset mean; if None, uses zeros)\n",
    "- target_label: Optional, specific output neuron/class index for multi-output models\n",
    "- framework: Either 'tensorflow' or 'pytorch' depending on model type\n",
    "- show_plot: Whether to plot feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- feature_names: List of feature names (optional; otherwise indices will be used)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of feature-level importance scores\n",
    "- Displays a bar plot of feature importances if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7becbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: captum in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from captum) (22.0)\n",
      "Requirement already satisfied: numpy<2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from captum) (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from captum) (3.7.0)\n",
      "Requirement already satisfied: torch>=1.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from captum) (1.12.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from captum) (4.64.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\hp\\anaconda3\\lib\\site-packages (from torch>=1.10->captum) (4.12.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->captum) (9.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->captum) (4.25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->captum) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm->captum) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74dfe7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from captum.attr import DeepLift as DL_Pytorch\n",
    "import shap  # For TensorFlow DeepLIFT implementation\n",
    "\n",
    "def deep_lift_feature_importance(model,\n",
    "                                  inputs,\n",
    "                                  baseline=None,\n",
    "                                  target_label=None,\n",
    "                                  framework='tensorflow',\n",
    "                                  show_plot=True,\n",
    "                                  plot_size=(12, 6),\n",
    "                                  feature_names=None,\n",
    "                                  random_state=42):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    if framework == 'pytorch':\n",
    "        torch.manual_seed(random_state)\n",
    "    else:\n",
    "        tf.random.set_seed(random_state)\n",
    "\n",
    "    if isinstance(inputs, np.ndarray):\n",
    "        inputs_tensor = torch.tensor(inputs, dtype=torch.float32) if framework == 'pytorch' else tf.convert_to_tensor(inputs)\n",
    "    else:\n",
    "        inputs_tensor = inputs\n",
    "\n",
    "    if baseline is None:\n",
    "        baseline = np.zeros_like(inputs) if isinstance(inputs, np.ndarray) else torch.zeros_like(inputs)\n",
    "\n",
    "    if framework == 'pytorch':\n",
    "        model.eval()\n",
    "        deep_lift = DL_Pytorch(model)\n",
    "        attributions = deep_lift.attribute(inputs_tensor, baselines=torch.tensor(baseline, dtype=torch.float32), target=target_label)\n",
    "        importance_scores = attributions.detach().cpu().numpy().mean(axis=0)\n",
    "    else:  # TensorFlow\n",
    "        explainer = shap.DeepExplainer(model, baseline)\n",
    "        shap_values = explainer.shap_values(inputs)\n",
    "        if isinstance(shap_values, list):\n",
    "            if target_label is not None:\n",
    "                shap_values = shap_values[target_label]\n",
    "            else:\n",
    "                shap_values = shap_values[0]\n",
    "        importance_scores = np.mean(shap_values, axis=0)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(importance_scores.shape[-1])]\n",
    "\n",
    "    if importance_scores.ndim > 1:\n",
    "        importance_scores = importance_scores.mean(axis=0)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.inferno(np.linspace(0.3, 1, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(importance_df['Feature'],\n",
    "                        importance_df['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Attribution Score', fontsize=12)\n",
    "        plt.title('DeepLIFT Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=8)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: DeepLIFT\\n\"\n",
    "            f\"Framework: {framework.capitalize()}\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3adb48",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999dff3",
   "metadata": {},
   "source": [
    "## 3.6.5 DeepSHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67659307",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding DeepSHAP } </h2>\n",
    "</summary>\n",
    "<h3> What is DeepSHAP? </h3>\n",
    "<p> DeepSHAP is a SHAP variant for deep learning models. It combines DeepLIFT and SHAP theory to produce SHAP values for neural networks.</p>\n",
    "<h3> Its role in Deep Learning Feature Attribution: </h3>\n",
    "<ul>\n",
    "    <li> Captures nonlinear interactions and baseline references.</li>\n",
    "    <li> Suitable for DNNs, works via backward recursive rules.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://shap.readthedocs.io/en/latest/generated/shap.DeepExplainer.html\" target=\"_blank\">SHAP DeepExplainer Docs</a></li>\n",
    "    <li><a href=\"https://github.com/slundberg/shap\" target=\"_blank\">SHAP GitHub</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b88ccc",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A trained deep learning model (TensorFlow/Keras)\n",
    "- inputs: Input data to compute feature importance (numpy array or tensor)\n",
    "- background_data: Background dataset for baseline/reference distribution (e.g., a random sample of training data)\n",
    "- target_label: Optional, specific output neuron/class index for multi-output models\n",
    "- show_plot: Whether to plot feature importances\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- feature_names: List of feature names (optional; otherwise uses indices)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of feature-level importance scores\n",
    "- Displays a bar plot of feature importances if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d29455c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "\n",
    "def deepshap_feature_importance(model,\n",
    "                                 inputs,\n",
    "                                 background_data,\n",
    "                                 target_label=None,\n",
    "                                 show_plot=True,\n",
    "                                 plot_size=(12, 6),\n",
    "                                 feature_names=None,\n",
    "                                 random_state=42):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    tf.random.set_seed(random_state)\n",
    "\n",
    "    if not isinstance(inputs, np.ndarray):\n",
    "        inputs = np.array(inputs)\n",
    "    if not isinstance(background_data, np.ndarray):\n",
    "        background_data = np.array(background_data)\n",
    "\n",
    "    explainer = shap.DeepExplainer(model, background_data)\n",
    "    shap_values = explainer.shap_values(inputs)\n",
    "\n",
    "    if isinstance(shap_values, list):\n",
    "        if target_label is not None:\n",
    "            shap_values = shap_values[target_label]\n",
    "        else:\n",
    "            shap_values = shap_values[0]\n",
    "    else:\n",
    "        shap_values = shap_values\n",
    "\n",
    "    importance_scores = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "    if importance_scores.ndim > 1:\n",
    "        importance_scores = importance_scores.mean(axis=0)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(importance_scores.shape[-1])]\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.plasma(np.linspace(0.3, 1, len(importance_df)))\n",
    "\n",
    "        bars = plt.barh(importance_df['Feature'],\n",
    "                        importance_df['Importance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Mean |SHAP Value|', fontsize=12)\n",
    "        plt.title('DeepSHAP Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=8)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: DeepSHAP\\n\"\n",
    "            f\"Model: Deep Neural Network\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136820b",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e79c6",
   "metadata": {},
   "source": [
    "## 3.6.6 Layer-wise Relevance Propagation (LRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852f21a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"cursor: pointer\">\n",
    "<h2> { Understanding Layer-wise Relevance Propagation (LRP) } </h2>\n",
    "</summary>\n",
    "<h3> What is LRP? </h3>\n",
    "<p> LRP is a technique to explain predictions by propagating relevance scores backward from the output to the input features.</p>\n",
    "<h3> Its role in Model Interpretability: </h3>\n",
    "<ul>\n",
    "    <li> Particularly useful for visualizing decisions in image classification.</li>\n",
    "    <li> It redistributes the prediction score layer by layer in a conservative way.</li>\n",
    "</ul>\n",
    "<h3> Resources: </h3>\n",
    "<ol>\n",
    "    <li><a href=\"https://www.sciencedirect.com/science/article/pii/S0031320320303703\" target=\"_blank\">Comprehensive LRP Review</a></li>\n",
    "    <li><a href=\"http://heatmapping.org/tutorial/\" target=\"_blank\">LRP Tutorial Site</a></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a1f5b",
   "metadata": {},
   "source": [
    "##### Parameters:\n",
    "- model: A trained neural network model (preferably TensorFlow/Keras or PyTorch with LRP support)\n",
    "- inputs: Input data to compute feature importance (numpy array or tensor)\n",
    "- target_label: Optional, specific output neuron/class index for relevance focusing\n",
    "- show_plot: Whether to plot feature relevance scores\n",
    "- plot_size: Tuple indicating plot size (width, height)\n",
    "- feature_names: List of feature names (optional; otherwise uses indices)\n",
    "- random_state: Random seed for reproducibility\n",
    "\n",
    "##### Returns:\n",
    "- DataFrame of feature-level relevance scores\n",
    "- Displays a bar plot of feature relevance if show_plot=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9a03110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting innvestigate\n",
      "  Downloading innvestigate-2.1.2-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 66.8/66.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: future<0.19,>=0.18 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from innvestigate) (0.18.3)\n",
      "Requirement already satisfied: tensorflow<2.15,>=2.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from innvestigate) (2.10.1)\n",
      "Requirement already satisfied: matplotlib<4,>=3.5 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from innvestigate) (3.10.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from innvestigate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.5->innvestigate) (22.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.5->innvestigate) (1.4.8)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.5->innvestigate) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.5->innvestigate) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.5->innvestigate) (3.2.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.5->innvestigate) (4.57.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib<4,>=3.5->innvestigate) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib<4,>=3.5->innvestigate) (1.3.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (1.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (4.12.2)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (0.31.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (24.3.25)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (18.1.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (2.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (2.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (3.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (3.19.6)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (0.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (65.6.3)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (2.10.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (2.1.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (3.11.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorflow<2.15,>=2.6->innvestigate) (0.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.15,>=2.6->innvestigate) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (2.39.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (2.28.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (5.5.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<2.15,>=2.6->innvestigate) (3.2.2)\n",
      "Installing collected packages: innvestigate\n",
      "Successfully installed innvestigate-2.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\hp\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install innvestigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651f5a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import innvestigate\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install 'innvestigate' package: pip install innvestigate\")\n",
    "\n",
    "def lrp_feature_importance(model,\n",
    "                            inputs,\n",
    "                            target_label=None,\n",
    "                            show_plot=True,\n",
    "                            plot_size=(12, 6),\n",
    "                            feature_names=None,\n",
    "                            random_state=42):\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    tf.random.set_seed(random_state)\n",
    "\n",
    "    if not isinstance(inputs, np.ndarray):\n",
    "        inputs = np.array(inputs)\n",
    "\n",
    "    analyzer = innvestigate.create_analyzer(\"lrp.z\", model)\n",
    "\n",
    "    if target_label is not None:\n",
    "        analysis = analyzer.analyze(inputs, neuron_selection=target_label)\n",
    "    else:\n",
    "        analysis = analyzer.analyze(inputs)\n",
    "\n",
    "    relevance_scores = np.mean(np.abs(analysis), axis=0)\n",
    "\n",
    "    if relevance_scores.ndim > 1:\n",
    "        relevance_scores = relevance_scores.mean(axis=0)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = [f'Feature_{i}' for i in range(relevance_scores.shape[-1])]\n",
    "\n",
    "    relevance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Relevance': relevance_scores\n",
    "    }).sort_values('Relevance', ascending=False)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=plot_size)\n",
    "        colors = plt.cm.magma(np.linspace(0.3, 1, len(relevance_df)))\n",
    "\n",
    "        bars = plt.barh(relevance_df['Feature'],\n",
    "                        relevance_df['Relevance'],\n",
    "                        color=colors,\n",
    "                        alpha=0.9)\n",
    "\n",
    "        plt.xlabel('Mean Relevance Score', fontsize=12)\n",
    "        plt.title('Layer-wise Relevance Propagation (LRP) Feature Importance', fontsize=14, pad=20)\n",
    "        plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "\n",
    "        for bar in bars:\n",
    "            width = bar.get_width()\n",
    "            plt.text(width + 0.0005,\n",
    "                     bar.get_y() + bar.get_height() / 2,\n",
    "                     f'{width:.4f}',\n",
    "                     va='center',\n",
    "                     fontsize=8)\n",
    "\n",
    "        method_text = (\n",
    "            f\"Method: LRP\\n\"\n",
    "            f\"Model: Deep Neural Network\"\n",
    "        )\n",
    "        plt.annotate(method_text,\n",
    "                     xy=(0.98, 0.02),\n",
    "                     xycoords='axes fraction',\n",
    "                     ha='right',\n",
    "                     va='bottom',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle='round', alpha=0.1))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return relevance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d7f43e",
   "metadata": {},
   "source": [
    "----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
